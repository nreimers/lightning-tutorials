


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-PQBQ3CV');
  </script>
  <!-- End Google Tag Manager -->

  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="google-site-verification" content="okUst94cAlWSsUsGZTB4xSS4UKTtRV8Nu5XZ9pdd3Aw" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Multi-agent Reinforcement Learning With WarpDrive &mdash; lightning-tutorials  documentation</title>
  

  
  
    <link rel="shortcut icon" href="../../_static/icon.svg"/>
  
  
  
    <link rel="canonical" href="https://www.pytorchlightning.ai/notebooks/lightning_examples/warp-drive.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sphinx_paramlinks.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Simple image classification with Lightning Flash" href="../templates/img-classify.html" />
    <link rel="prev" title="Finetune Transformers Models with PyTorch Lightning" href="text-transformers.html" />
  <!-- Google Analytics -->
  
  <!-- End Google Analytics -->
  

  
  <script src="../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/UCity/UCity-Light.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/UCity/UCity-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/UCity/UCity-Semibold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/Inconsolata/Inconsolata.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <script defer src="https://use.fontawesome.com/releases/v6.1.1/js/all.js" integrity="sha384-xBXmu0dk1bEoiwd71wOonQLyH+VpgR1XcDH3rtxrLww5ajNTuMvBdL5SOiFZnNdp" crossorigin="anonymous"></script>
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pt-lightning-sandbox.rtfd.io/en/latest/" aria-label="PyTorch Lightning">
      <!--  <img class="call-to-action-img" src="../../_static/images/logo-lightning-icon.png"/> -->
      </a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pt-lightning-sandbox.readthedocs.io/en/latest/introduction_guide.html">Get Started</a>
          </li>

          <li>
            <a href="https://www.pytorchlightning.ai/blog">Blog</a>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch-lightning.readthedocs.io/en/stable/">
                  <span class="dropdown-title">PyTorch Lightning</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://torchmetrics.readthedocs.io/en/stable/">
                  <span class="dropdown-title">TorchMetrics</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning-flash.readthedocs.io/en/stable/">
                  <span class="dropdown-title">Lightning Flash</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning-transformers.readthedocs.io/en/stable/">
                  <span class="dropdown-title">Lightning Transformers</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning-bolts.readthedocs.io/en/stable/">
                  <span class="dropdown-title">Lightning Bolts</span>
                </a>
            </div>
          </li>

          <!--<li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pt-lightning-sandbox.readthedocs.io/en/latest/#community-examples">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>-->

          <li>
            <a href="https://github.com/PytorchLightning/lightning-sandbox">GitHub</a>
          </li>

          <li>
            <a href="https://www.grid.ai/">Train on the cloud</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Start here</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../course_UvA-DL/01-introduction-to-pytorch.html">Tutorial 1: Introduction to PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../course_UvA-DL/02-activation-functions.html">Tutorial 2: Activation Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../course_UvA-DL/03-initialization-and-optimization.html">Tutorial 3: Initialization and Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../course_UvA-DL/04-inception-resnet-densenet.html">Tutorial 4: Inception, ResNet and DenseNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../course_UvA-DL/05-transformers-and-MH-attention.html">Tutorial 5: Transformers and Multi-Head Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="../course_UvA-DL/06-graph-neural-networks.html">Tutorial 6: Basics of Graph Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../course_UvA-DL/07-deep-energy-based-generative-models.html">Tutorial 7: Deep Energy-Based Generative Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../course_UvA-DL/08-deep-autoencoders.html">Tutorial 8: Deep Autoencoders</a></li>
<li class="toctree-l1"><a class="reference internal" href="../course_UvA-DL/09-normalizing-flows.html">Tutorial 9: Normalizing Flows for Image Modeling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../course_UvA-DL/10-autoregressive-image-modeling.html">Tutorial 10: Autoregressive Image Modeling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../course_UvA-DL/11-vision-transformer.html">Tutorial 11: Vision Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../course_UvA-DL/12-meta-learning.html">Tutorial 12: Meta-Learning - Learning to Learn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../course_UvA-DL/13-contrastive-learning.html">Tutorial 13: Self-Supervised Contrastive Learning with SimCLR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../flash_tutorials/electricity_forecasting.html">Electricity Price Forecasting with N-BEATS</a></li>
<li class="toctree-l1"><a class="reference internal" href="augmentation_kornia.html">GPU and batched data augmentation with Kornia and PyTorch-Lightning</a></li>
<li class="toctree-l1"><a class="reference internal" href="barlow-twins.html">Barlow Twins Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="basic-gan.html">PyTorch Lightning Basic GAN Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="cifar10-baseline.html">PyTorch Lightning CIFAR10 ~94% Baseline Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="datamodules.html">PyTorch Lightning DataModules</a></li>
<li class="toctree-l1"><a class="reference internal" href="mnist-hello-world.html">Introduction to Pytorch Lightning</a></li>
<li class="toctree-l1"><a class="reference internal" href="mnist-tpu-training.html">TPU training with PyTorch Lightning</a></li>
<li class="toctree-l1"><a class="reference internal" href="reinforce-learning-DQN.html">How to train a Deep Q Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="text-transformers.html">Finetune Transformers Models with PyTorch Lightning</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Multi-agent Reinforcement Learning With WarpDrive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../templates/img-classify.html">Simple image classification with Lightning Flash</a></li>
<li class="toctree-l1"><a class="reference internal" href="../templates/simple.html">How to write a PyTorch Lightning tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../templates/titanic.html">Solving Titanic dataset with Lightning Flash</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>Multi-agent Reinforcement Learning With WarpDrive</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../../_sources/notebooks/lightning_examples/warp-drive.ipynb.txt" rel="nofollow"><img src="../../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt .copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
</style>
<section id="Multi-agent-Reinforcement-Learning-With-WarpDrive">
<h1>Multi-agent Reinforcement Learning With WarpDrive<a class="headerlink" href="#Multi-agent-Reinforcement-Learning-With-WarpDrive" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p><strong>Author:</strong> Sunil Srinivasa (<a class="reference external" href="mailto:sunil&#46;srinivasa&#37;&#52;&#48;salesforce&#46;com">sunil<span>&#46;</span>srinivasa<span>&#64;</span>salesforce<span>&#46;</span>com</a>), Tian Lan (<a class="reference external" href="mailto:tian&#46;lan&#37;&#52;&#48;salesforce&#46;com">tian<span>&#46;</span>lan<span>&#64;</span>salesforce<span>&#46;</span>com</a>), Huan Wang (<a class="reference external" href="mailto:huan&#46;wang&#37;&#52;&#48;salesforce&#46;com">huan<span>&#46;</span>wang<span>&#64;</span>salesforce<span>&#46;</span>com</a>) and Stephan Zheng(<a class="reference external" href="mailto:stephan&#46;zheng&#37;&#52;&#48;salesforce&#46;com">stephan<span>&#46;</span>zheng<span>&#64;</span>salesforce<span>&#46;</span>com</a>)</p></li>
<li><p><strong>License:</strong> BSD 3-Clause “New” or “Revised” License</p></li>
<li><p><strong>Generated:</strong> 2022-04-22T04:11:41.774547</p></li>
</ul>
<p>This notebook introduces multi-agent reinforcement learning (MARL) with WarpDrive (Lan et al. <a class="reference external" href="https://arxiv.org/abs/2108.13976">https://arxiv.org/abs/2108.13976</a>). WarpDrive is a flexible, lightweight, and easy-to-use open-source framework that implements end-to-end deep MARL on GPUs. WarpDrive enables orders-of-magnitude speedups compared to CPU-GPU implementations, using the parallelization capability of GPUs and several design choices to minimize communication overhead. WarpDrive also prioritizes user-friendliness - it has
utility functions to easily build MARL environments in CUDA and quality-of-life tools to run end-to-end MARL using just a few lines of code, and is compatible with PyTorch. WarpDrive includes the following resources. code - <a class="reference external" href="https://github.com/salesforce/warp-drive">https://github.com/salesforce/warp-drive</a> documentation - <a class="reference external" href="http://opensource.salesforce.com/warp-drive/">http://opensource.salesforce.com/warp-drive/</a>, and white paper - <a class="reference external" href="https://arxiv.org/abs/2108.13976">https://arxiv.org/abs/2108.13976</a>.</p>
<hr class="docutils" />
<p>Open in <a class="reference external" href="https://colab.research.google.com/github/PytorchLightning/lightning-tutorials/blob/publication/.notebooks/lightning_examples/warp-drive.ipynb"><img alt="Open In Colab" src="data:image/png;base64,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" style="width: 117px; height: 20px;" /></a></p>
<p>Give us a ⭐ <a class="reference external" href="https://www.github.com/PytorchLightning/pytorch-lightning/">on Github</a> | Check out <a class="reference external" href="https://pytorch-lightning.readthedocs.io/en/stable/">the documentation</a> | Join us <a class="reference external" href="https://www.pytorchlightning.ai/community">on Slack</a></p>
<section id="Setup">
<h2>Setup<a class="headerlink" href="#Setup" title="Permalink to this headline">¶</a></h2>
<p>This notebook requires some packages besides pytorch-lightning.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span> pip install --quiet <span class="s2">&quot;ffmpeg-python&quot;</span> <span class="s2">&quot;torch&gt;=1.6, &lt;1.9&quot;</span> <span class="s2">&quot;torchmetrics&gt;=0.6&quot;</span> <span class="s2">&quot;ipython[notebook]&quot;</span> <span class="s2">&quot;rl-warp-drive&gt;=1.5.1&quot;</span> <span class="s2">&quot;pytorch-lightning&gt;=1.4&quot;</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">ERROR: Cannot install rl-warp-drive==1.5.1, rl-warp-drive==1.6, rl-warp-drive==1.6.1, rl-warp-drive==1.6.2, rl-warp-drive==1.6.3, torch&lt;1.9 and &gt;=1.6 and torchmetrics==0.6.0 because these package versions have conflicting dependencies.</span>
<span class="ansi-red-fg">ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/user_guide/#fixing-conflicting-dependencies</span>
<span class="ansi-yellow-fg">WARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.
You should consider upgrading via the &#39;/usr/bin/python3.8 -m pip install --upgrade pip&#39; command.</span>
</pre></div></div>
</div>
<p><strong>⚠️ PLEASE NOTE:</strong> This notebook runs on a GPU runtime. If running on Colab, choose Runtime &gt; Change runtime type from the menu, then select <code class="docutils literal notranslate"><span class="pre">GPU</span></code> in the ‘Hardware accelerator’ dropdown menu.</p>
</section>
<section id="Introduction">
<h2>Introduction<a class="headerlink" href="#Introduction" title="Permalink to this headline">¶</a></h2>
<p>This tutorial provides a demonstration of a multi-agent Reinforcement Learning (RL) training loop with <a class="reference external" href="https://github.com/salesforce/warp-drive">WarpDrive</a>. WarpDrive is a flexible, lightweight, and easy-to-use RL framework that implements end-to-end deep multi-agent RL on a single GPU (Graphics Processing Unit). Using the extreme parallelization capability of GPUs, it enables <a class="reference external" href="https://arxiv.org/abs/2108.13976">orders-of-magnitude faster RL</a> compared to common implementations that blend
CPU simulations and GPU models. WarpDrive is extremely efficient as it runs simulations across multiple agents and multiple environment replicas in parallel and completely eliminates the back-and-forth data copying between the CPU and the GPU.</p>
<p>We have integrated WarpDrive with the <a class="reference external" href="https://www.pytorchlightning.ai/">Pytorch Lightning</a> framework, which greatly reduces the trainer boilerplate code, and improves training flexibility.</p>
<p>Below, we demonstrate how to use WarpDrive and PytorchLightning together to train a game of <a class="reference external" href="https://github.com/salesforce/warp-drive/blob/master/example_envs/tag_continuous/tag_continuous.py">Tag</a> where multiple <em>tagger</em> agents are trying to run after and tag multiple other <em>runner</em> agents. As such, the Warpdrive framework comprises several utility functions that help easily implement any (OpenAI-)*gym-style* RL environment, and furthermore, provides quality-of-life tools to train it
end-to-end using just a few lines of code. You may familiarize yourself with WarpDrive with the help of these <a class="reference external" href="https://github.com/salesforce/warp-drive/tree/master/tutorials">tutorials</a>.</p>
<p>We invite everyone to <strong>contribute to WarpDrive</strong>, including adding new multi-agent environments, proposing new features and reporting issues on our open source <a class="reference external" href="https://github.com/salesforce/warp-drive">repository</a>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">mpl_toolkits.mplot3d.art3d</span> <span class="k">as</span> <span class="nn">art3d</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">example_envs.tag_continuous.tag_continuous</span> <span class="kn">import</span> <span class="n">TagContinuous</span>

<span class="c1"># from IPython.display import HTML</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">animation</span>
<span class="kn">from</span> <span class="nn">matplotlib.patches</span> <span class="kn">import</span> <span class="n">Polygon</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning</span> <span class="kn">import</span> <span class="n">Trainer</span>
<span class="kn">from</span> <span class="nn">warp_drive.env_wrapper</span> <span class="kn">import</span> <span class="n">EnvWrapper</span>
<span class="kn">from</span> <span class="nn">warp_drive.training.pytorch_lightning_trainer</span> <span class="kn">import</span> <span class="n">CudaCallback</span><span class="p">,</span> <span class="n">PerfStatsCallback</span><span class="p">,</span> <span class="n">WarpDriveModule</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_NUM_AVAILABLE_GPUS</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>
<span class="k">assert</span> <span class="n">_NUM_AVAILABLE_GPUS</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;This notebook needs a GPU to run!&quot;</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set logger level e.g., DEBUG, INFO, WARNING, ERROR.</span>
<span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">()</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">ERROR</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
<section id="Specify-a-set-of-run-configurations-for-your-experiments">
<h2>Specify a set of run configurations for your experiments<a class="headerlink" href="#Specify-a-set-of-run-configurations-for-your-experiments" title="Permalink to this headline">¶</a></h2>
<p>The run configuration is a dictionary comprising the environment parameters, the trainer and the policy network settings, as well as configurations for saving.</p>
<p>For our experiment, we consider an environment wherein <img class="math" src="../../_images/math/51d4ace3c51c5a5490fc4464952d6c46d518fbc8.png" alt="5"/> taggers and <img class="math" src="../../_images/math/31883b6773226a44daa7885c9e1d2ec87a73eabb.png" alt="100"/> runners play the game of <a class="reference external" href="https://github.com/salesforce/warp-drive/blob/master/example_envs/tag_continuous/tag_continuous.py">Tag</a> on a <img class="math" src="../../_images/math/a39c3dbc8f8b2f5b4db163a5b97bdc36f14a995f.png" alt="20 \times 20"/> plane. The game lasts <img class="math" src="../../_images/math/48585ec6d4fbb0c86c66c8263a87bfa3fd1d4949.png" alt="200"/> timesteps. Each agent chooses it’s own acceleration and turn actions at every timestep, and we use mechanics to determine how the agents move over the grid. When a tagger gets close to a runner, the runner is tagged,
and is eliminated from the game. For the configuration below, the runners and taggers have the same unit skill levels, or top speeds.</p>
<p>We train the agents using <img class="math" src="../../_images/math/4908e97aebcf5cb0526feb107056046fac483f32.png" alt="50"/> environments or simulations running in parallel. With WarpDrive, each simulation runs on sepate GPU blocks.</p>
<p>There are two separate policy networks used for the tagger and runner agents. Each network is a fully-connected model with two layers each of <img class="math" src="../../_images/math/3887776ded3b52dc1ff6606e571ab57f0490ad94.png" alt="256"/> dimensions. We use the Advantage Actor Critic (A2C) algorithm for training. WarpDrive also currently provides the option to use the Proximal Policy Optimization (PPO) algorithm instead.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">run_config</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;tag_continuous&quot;</span><span class="p">,</span>
    <span class="c1"># Environment settings.</span>
    <span class="n">env</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
        <span class="c1"># number of taggers in the environment</span>
        <span class="n">num_taggers</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
        <span class="c1"># number of runners in the environment</span>
        <span class="n">num_runners</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
        <span class="c1"># length of the (square) grid on which the game is played</span>
        <span class="n">grid_length</span><span class="o">=</span><span class="mf">20.0</span><span class="p">,</span>
        <span class="c1"># episode length in timesteps</span>
        <span class="n">episode_length</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
        <span class="c1"># maximum acceleration</span>
        <span class="n">max_acceleration</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="c1"># minimum acceleration</span>
        <span class="n">min_acceleration</span><span class="o">=-</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="c1"># 3*pi/4 radians</span>
        <span class="n">max_turn</span><span class="o">=</span><span class="mf">2.35</span><span class="p">,</span>
        <span class="c1"># -3*pi/4 radians</span>
        <span class="n">min_turn</span><span class="o">=-</span><span class="mf">2.35</span><span class="p">,</span>
        <span class="c1"># number of discretized accelerate actions</span>
        <span class="n">num_acceleration_levels</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="c1"># number of discretized turn actions</span>
        <span class="n">num_turn_levels</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="c1"># skill level for the tagger</span>
        <span class="n">skill_level_tagger</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
        <span class="c1"># skill level for the runner</span>
        <span class="n">skill_level_runner</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
        <span class="c1"># each agent only sees full or partial information</span>
        <span class="n">use_full_observation</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="c1"># flag to indicate if a runner stays in the game after getting tagged</span>
        <span class="n">runner_exits_game_after_tagged</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="c1"># number of other agents each agent can see</span>
        <span class="n">num_other_agents_observed</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="c1"># positive reward for the tagger upon tagging a runner</span>
        <span class="n">tag_reward_for_tagger</span><span class="o">=</span><span class="mf">10.0</span><span class="p">,</span>
        <span class="c1"># negative reward for the runner upon getting tagged</span>
        <span class="n">tag_penalty_for_runner</span><span class="o">=-</span><span class="mf">10.0</span><span class="p">,</span>
        <span class="c1"># reward at the end of the game for a runner that isn&#39;t tagged</span>
        <span class="n">end_of_game_reward_for_runner</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
        <span class="c1"># margin between a tagger and runner to consider the runner as &#39;tagged&#39;.</span>
        <span class="n">tagging_distance</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="c1"># Trainer settings.</span>
    <span class="n">trainer</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
        <span class="c1"># number of environment replicas (number of GPU blocks used)</span>
        <span class="n">num_envs</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
        <span class="c1"># total batch size used for training per iteration (across all the environments)</span>
        <span class="n">train_batch_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
        <span class="c1"># total number of episodes to run the training for (can be arbitrarily high!)</span>
        <span class="n">num_episodes</span><span class="o">=</span><span class="mi">50000</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="c1"># Policy network settings.</span>
    <span class="n">policy</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
        <span class="n">runner</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
            <span class="c1"># flag indicating whether the model needs to be trained</span>
            <span class="n">to_train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="c1"># algorithm used to train the policy</span>
            <span class="n">algorithm</span><span class="o">=</span><span class="s2">&quot;A2C&quot;</span><span class="p">,</span>
            <span class="c1"># discount rate</span>
            <span class="n">gamma</span><span class="o">=</span><span class="mf">0.98</span><span class="p">,</span>
            <span class="c1"># learning rate</span>
            <span class="n">lr</span><span class="o">=</span><span class="mf">0.005</span><span class="p">,</span>
            <span class="c1"># policy model settings</span>
            <span class="n">model</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="nb">type</span><span class="o">=</span><span class="s2">&quot;fully_connected&quot;</span><span class="p">,</span> <span class="n">fc_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">],</span> <span class="n">model_ckpt_filepath</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">),</span>
        <span class="p">),</span>
        <span class="n">tagger</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
            <span class="n">to_train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">algorithm</span><span class="o">=</span><span class="s2">&quot;A2C&quot;</span><span class="p">,</span>
            <span class="n">gamma</span><span class="o">=</span><span class="mf">0.98</span><span class="p">,</span>
            <span class="n">lr</span><span class="o">=</span><span class="mf">0.002</span><span class="p">,</span>
            <span class="n">model</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="nb">type</span><span class="o">=</span><span class="s2">&quot;fully_connected&quot;</span><span class="p">,</span> <span class="n">fc_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">],</span> <span class="n">model_ckpt_filepath</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">),</span>
        <span class="p">),</span>
    <span class="p">),</span>
    <span class="c1"># Checkpoint saving setting.</span>
    <span class="n">saving</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
        <span class="c1"># how often (in iterations) to print the metrics</span>
        <span class="n">metrics_log_freq</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="c1"># how often (in iterations) to save the model parameters</span>
        <span class="n">model_params_save_freq</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span>
        <span class="c1"># base folder used for saving</span>
        <span class="n">basedir</span><span class="o">=</span><span class="s2">&quot;/tmp&quot;</span><span class="p">,</span>
        <span class="c1"># experiment name</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;continuous_tag&quot;</span><span class="p">,</span>
        <span class="c1"># experiment tag</span>
        <span class="n">tag</span><span class="o">=</span><span class="s2">&quot;example&quot;</span><span class="p">,</span>
    <span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</section>
<section id="Instantiate-the-WarpDrive-Module">
<h2>Instantiate the WarpDrive Module<a class="headerlink" href="#Instantiate-the-WarpDrive-Module" title="Permalink to this headline">¶</a></h2>
<p>In order to instantiate the WarpDrive module, we first use an environment wrapper to specify that the environment needs to be run on the GPU (via the <code class="docutils literal notranslate"><span class="pre">use_cuda</span></code> flag). Also, agents in the environment can share policy models; so we specify a dictionary to map each policy network model to the list of agent ids using that model.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a wrapped environment object via the EnvWrapper.</span>
<span class="c1"># Ensure that use_cuda is set to True (in order to run on the GPU).</span>
<span class="n">env_wrapper</span> <span class="o">=</span> <span class="n">EnvWrapper</span><span class="p">(</span>
    <span class="n">TagContinuous</span><span class="p">(</span><span class="o">**</span><span class="n">run_config</span><span class="p">[</span><span class="s2">&quot;env&quot;</span><span class="p">]),</span>
    <span class="n">num_envs</span><span class="o">=</span><span class="n">run_config</span><span class="p">[</span><span class="s2">&quot;trainer&quot;</span><span class="p">][</span><span class="s2">&quot;num_envs&quot;</span><span class="p">],</span>
    <span class="n">use_cuda</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Agents can share policy models: this dictionary maps policy model names to agent ids.</span>
<span class="n">policy_tag_to_agent_id_map</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;tagger&quot;</span><span class="p">:</span> <span class="nb">list</span><span class="p">(</span><span class="n">env_wrapper</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">taggers</span><span class="p">),</span>
    <span class="s2">&quot;runner&quot;</span><span class="p">:</span> <span class="nb">list</span><span class="p">(</span><span class="n">env_wrapper</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">runners</span><span class="p">),</span>
<span class="p">}</span>

<span class="n">wd_module</span> <span class="o">=</span> <span class="n">WarpDriveModule</span><span class="p">(</span>
    <span class="n">env_wrapper</span><span class="o">=</span><span class="n">env_wrapper</span><span class="p">,</span>
    <span class="n">config</span><span class="o">=</span><span class="n">run_config</span><span class="p">,</span>
    <span class="n">policy_tag_to_agent_id_map</span><span class="o">=</span><span class="n">policy_tag_to_agent_id_map</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
Global seed set to 1650601003
</pre></div></div>
</div>
</section>
<section id="Visualizing-an-episode-roll-out-before-training">
<h2>Visualizing an episode roll-out before training<a class="headerlink" href="#Visualizing-an-episode-roll-out-before-training" title="Permalink to this headline">¶</a></h2>
<p>We have created a helper function (see below) to visualize an episode rollout. Internally, this function uses the WarpDrive module’s <code class="docutils literal notranslate"><span class="pre">fetch_episode_states</span></code> API to fetch the data arrays on the GPU for the duration of an entire episode. Specifically, we fetch the state arrays pertaining to agents’ x and y locations on the plane and indicators on which agents are still active in the game. Note that this function may be invoked at any time during training, and it will use the state of the policy
models at that time to sample actions and generate the visualization.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">generate_tag_env_rollout_animation</span><span class="p">(</span>
    <span class="n">warp_drive_module</span><span class="p">,</span>
    <span class="n">fps</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span>
    <span class="n">tagger_color</span><span class="o">=</span><span class="s2">&quot;#C843C3&quot;</span><span class="p">,</span>
    <span class="n">runner_color</span><span class="o">=</span><span class="s2">&quot;#245EB6&quot;</span><span class="p">,</span>
    <span class="n">runner_not_in_game_color</span><span class="o">=</span><span class="s2">&quot;#666666&quot;</span><span class="p">,</span>
    <span class="n">fig_width</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
    <span class="n">fig_height</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
<span class="p">):</span>
    <span class="k">assert</span> <span class="n">warp_drive_module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="n">episode_states</span> <span class="o">=</span> <span class="n">warp_drive_module</span><span class="o">.</span><span class="n">fetch_episode_states</span><span class="p">([</span><span class="s2">&quot;loc_x&quot;</span><span class="p">,</span> <span class="s2">&quot;loc_y&quot;</span><span class="p">,</span> <span class="s2">&quot;still_in_the_game&quot;</span><span class="p">])</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">episode_states</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span>
    <span class="n">env</span> <span class="o">=</span> <span class="n">warp_drive_module</span><span class="o">.</span><span class="n">cuda_envs</span><span class="o">.</span><span class="n">env</span>

    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="n">fig_width</span><span class="p">,</span> <span class="n">fig_height</span><span class="p">))</span>  <span class="c1"># , constrained_layout=True</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s2">&quot;3d&quot;</span><span class="p">)</span>

    <span class="c1"># Bounds</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_zlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>

    <span class="c1"># Surface</span>
    <span class="n">corner_points</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)]</span>

    <span class="n">poly</span> <span class="o">=</span> <span class="n">Polygon</span><span class="p">(</span><span class="n">corner_points</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">poly</span><span class="p">)</span>
    <span class="n">art3d</span><span class="o">.</span><span class="n">pathpatch_2d_to_3d</span><span class="p">(</span><span class="n">poly</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">zdir</span><span class="o">=</span><span class="s2">&quot;z&quot;</span><span class="p">)</span>

    <span class="c1"># &quot;Hide&quot; side panes</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_pane_color</span><span class="p">((</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_pane_color</span><span class="p">((</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">zaxis</span><span class="o">.</span><span class="n">set_pane_color</span><span class="p">((</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">))</span>

    <span class="c1"># Hide grid lines</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># Hide axes ticks</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_zticks</span><span class="p">([])</span>

    <span class="c1"># Hide axes</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>

    <span class="c1"># Set camera</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">elev</span> <span class="o">=</span> <span class="mi">40</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">azim</span> <span class="o">=</span> <span class="o">-</span><span class="mi">55</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">dist</span> <span class="o">=</span> <span class="mi">10</span>

    <span class="c1"># Try to reduce whitespace</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=-</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Plot init data</span>
    <span class="n">lines</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">num_agents</span><span class="p">)]</span>

    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">lines</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">env</span><span class="o">.</span><span class="n">taggers</span><span class="p">:</span>
            <span class="n">lines</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot3D</span><span class="p">(</span>
                <span class="n">episode_states</span><span class="p">[</span><span class="s2">&quot;loc_x&quot;</span><span class="p">][:</span><span class="mi">1</span><span class="p">,</span> <span class="n">idx</span><span class="p">]</span> <span class="o">/</span> <span class="n">env</span><span class="o">.</span><span class="n">grid_length</span><span class="p">,</span>
                <span class="n">episode_states</span><span class="p">[</span><span class="s2">&quot;loc_y&quot;</span><span class="p">][:</span><span class="mi">1</span><span class="p">,</span> <span class="n">idx</span><span class="p">]</span> <span class="o">/</span> <span class="n">env</span><span class="o">.</span><span class="n">grid_length</span><span class="p">,</span>
                <span class="mi">0</span><span class="p">,</span>
                <span class="n">color</span><span class="o">=</span><span class="n">tagger_color</span><span class="p">,</span>
                <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span>
                <span class="n">markersize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
            <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># runners</span>
            <span class="n">lines</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot3D</span><span class="p">(</span>
                <span class="n">episode_states</span><span class="p">[</span><span class="s2">&quot;loc_x&quot;</span><span class="p">][:</span><span class="mi">1</span><span class="p">,</span> <span class="n">idx</span><span class="p">]</span> <span class="o">/</span> <span class="n">env</span><span class="o">.</span><span class="n">grid_length</span><span class="p">,</span>
                <span class="n">episode_states</span><span class="p">[</span><span class="s2">&quot;loc_y&quot;</span><span class="p">][:</span><span class="mi">1</span><span class="p">,</span> <span class="n">idx</span><span class="p">]</span> <span class="o">/</span> <span class="n">env</span><span class="o">.</span><span class="n">grid_length</span><span class="p">,</span>
                <span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">color</span><span class="o">=</span><span class="n">runner_color</span><span class="p">,</span>
                <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span>
                <span class="n">markersize</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
            <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">init_num_runners</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">num_agents</span> <span class="o">-</span> <span class="n">env</span><span class="o">.</span><span class="n">num_taggers</span>

    <span class="k">def</span> <span class="nf">_get_label</span><span class="p">(</span><span class="n">timestep</span><span class="p">,</span> <span class="n">n_runners_alive</span><span class="p">,</span> <span class="n">init_n_runners</span><span class="p">):</span>
        <span class="n">line1</span> <span class="o">=</span> <span class="s2">&quot;Continuous Tag</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="n">line2</span> <span class="o">=</span> <span class="s2">&quot;Time Step:&quot;</span><span class="o">.</span><span class="n">ljust</span><span class="p">(</span><span class="mi">14</span><span class="p">)</span> <span class="o">+</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">timestep</span><span class="si">:</span><span class="s2">4.0f</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="n">frac_runners_alive</span> <span class="o">=</span> <span class="n">n_runners_alive</span> <span class="o">/</span> <span class="n">init_n_runners</span>
        <span class="n">pct_runners_alive</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">n_runners_alive</span><span class="si">:</span><span class="s2">4</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">frac_runners_alive</span> <span class="o">*</span> <span class="mi">100</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">%)&quot;</span>
        <span class="n">line3</span> <span class="o">=</span> <span class="s2">&quot;Runners Left:&quot;</span><span class="o">.</span><span class="n">ljust</span><span class="p">(</span><span class="mi">14</span><span class="p">)</span> <span class="o">+</span> <span class="n">pct_runners_alive</span>
        <span class="k">return</span> <span class="n">line1</span> <span class="o">+</span> <span class="n">line2</span> <span class="o">+</span> <span class="n">line3</span>

    <span class="n">label</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span>
        <span class="mi">0</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>
        <span class="mf">0.02</span><span class="p">,</span>
        <span class="n">_get_label</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">init_num_runners</span><span class="p">,</span> <span class="n">init_num_runners</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">(),</span>
    <span class="p">)</span>

    <span class="n">label</span><span class="o">.</span><span class="n">set_fontsize</span><span class="p">(</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">label</span><span class="o">.</span><span class="n">set_fontweight</span><span class="p">(</span><span class="s2">&quot;normal&quot;</span><span class="p">)</span>
    <span class="n">label</span><span class="o">.</span><span class="n">set_color</span><span class="p">(</span><span class="s2">&quot;#666666&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">animate</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">lines</span><span class="p">):</span>
            <span class="n">line</span><span class="o">.</span><span class="n">set_data_3d</span><span class="p">(</span>
                <span class="n">episode_states</span><span class="p">[</span><span class="s2">&quot;loc_x&quot;</span><span class="p">][</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">idx</span><span class="p">]</span> <span class="o">/</span> <span class="n">env</span><span class="o">.</span><span class="n">grid_length</span><span class="p">,</span>
                <span class="n">episode_states</span><span class="p">[</span><span class="s2">&quot;loc_y&quot;</span><span class="p">][</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">idx</span><span class="p">]</span> <span class="o">/</span> <span class="n">env</span><span class="o">.</span><span class="n">grid_length</span><span class="p">,</span>
                <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
            <span class="p">)</span>

            <span class="n">still_in_game</span> <span class="o">=</span> <span class="n">episode_states</span><span class="p">[</span><span class="s2">&quot;still_in_the_game&quot;</span><span class="p">][</span><span class="n">i</span><span class="p">,</span> <span class="n">idx</span><span class="p">]</span>

            <span class="k">if</span> <span class="n">still_in_game</span><span class="p">:</span>
                <span class="k">pass</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">line</span><span class="o">.</span><span class="n">set_color</span><span class="p">(</span><span class="n">runner_not_in_game_color</span><span class="p">)</span>
                <span class="n">line</span><span class="o">.</span><span class="n">set_marker</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>

        <span class="n">n_runners_alive</span> <span class="o">=</span> <span class="n">episode_states</span><span class="p">[</span><span class="s2">&quot;still_in_the_game&quot;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">-</span> <span class="n">env</span><span class="o">.</span><span class="n">num_taggers</span>
        <span class="n">label</span><span class="o">.</span><span class="n">set_text</span><span class="p">(</span><span class="n">_get_label</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">n_runners_alive</span><span class="p">,</span> <span class="n">init_num_runners</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span>

    <span class="n">ani</span> <span class="o">=</span> <span class="n">animation</span><span class="o">.</span><span class="n">FuncAnimation</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="n">animate</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">episode_length</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">interval</span><span class="o">=</span><span class="mf">1000.0</span> <span class="o">/</span> <span class="n">fps</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">ani</span>
</pre></div>
</div>
</div>
<p>The animation below shows a sample realization of the game episode before training, i.e., with randomly chosen agent actions. The <img class="math" src="../../_images/math/51d4ace3c51c5a5490fc4464952d6c46d518fbc8.png" alt="5"/> taggers are marked in pink, while the <img class="math" src="../../_images/math/31883b6773226a44daa7885c9e1d2ec87a73eabb.png" alt="100"/> blue agents are the runners. Both the taggers and runners move around randomly and about half the runners remain at the end of the episode.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><br/><span></span><span class="c1"># anim = generate_tag_env_rollout_animation(wd_module)</span>
<span class="c1"># HTML(anim.to_html5_video())</span>
</pre></div>
</div>
</div>
</section>
<section id="Create-the-Lightning-Trainer">
<h2>Create the Lightning Trainer<a class="headerlink" href="#Create-the-Lightning-Trainer" title="Permalink to this headline">¶</a></h2>
<p>Next, we create the trainer for training the WarpDrive model. We add the <code class="docutils literal notranslate"><span class="pre">performance</span> <span class="pre">stats</span></code> callbacks to the trainer to view the throughput performance of WarpDrive.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">log_freq</span> <span class="o">=</span> <span class="n">run_config</span><span class="p">[</span><span class="s2">&quot;saving&quot;</span><span class="p">][</span><span class="s2">&quot;metrics_log_freq&quot;</span><span class="p">]</span>

<span class="c1"># Define callbacks.</span>
<span class="n">cuda_callback</span> <span class="o">=</span> <span class="n">CudaCallback</span><span class="p">(</span><span class="n">module</span><span class="o">=</span><span class="n">wd_module</span><span class="p">)</span>
<span class="n">perf_stats_callback</span> <span class="o">=</span> <span class="n">PerfStatsCallback</span><span class="p">(</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">wd_module</span><span class="o">.</span><span class="n">training_batch_size</span><span class="p">,</span>
    <span class="n">num_iters</span><span class="o">=</span><span class="n">wd_module</span><span class="o">.</span><span class="n">num_iters</span><span class="p">,</span>
    <span class="n">log_freq</span><span class="o">=</span><span class="n">log_freq</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Instantiate the PytorchLightning trainer with the callbacks.</span>
<span class="c1"># Also, set the number of gpus to 1, since this notebook uses just a single GPU.</span>
<span class="n">num_gpus</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">num_episodes</span> <span class="o">=</span> <span class="n">run_config</span><span class="p">[</span><span class="s2">&quot;trainer&quot;</span><span class="p">][</span><span class="s2">&quot;num_episodes&quot;</span><span class="p">]</span>
<span class="n">episode_length</span> <span class="o">=</span> <span class="n">run_config</span><span class="p">[</span><span class="s2">&quot;env&quot;</span><span class="p">][</span><span class="s2">&quot;episode_length&quot;</span><span class="p">]</span>
<span class="n">training_batch_size</span> <span class="o">=</span> <span class="n">run_config</span><span class="p">[</span><span class="s2">&quot;trainer&quot;</span><span class="p">][</span><span class="s2">&quot;train_batch_size&quot;</span><span class="p">]</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="n">num_episodes</span> <span class="o">*</span> <span class="n">episode_length</span> <span class="o">/</span> <span class="n">training_batch_size</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
    <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span>
    <span class="n">devices</span><span class="o">=</span><span class="n">num_gpus</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">cuda_callback</span><span class="p">,</span> <span class="n">perf_stats_callback</span><span class="p">],</span>
    <span class="n">max_epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Start tensorboard.</span>
<span class="o">%</span><span class="k">load_ext</span> tensorboard
<span class="o">%</span><span class="k">tensorboard</span> --logdir lightning_logs/
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<iframe id="tensorboard-frame-98f939bd39fd4834" width="100%" height="800" frameborder="0">
</iframe>
<script>
  (function() {
    const frame = document.getElementById("tensorboard-frame-98f939bd39fd4834");
    const url = new URL("/", window.location);
    const port = 6006;
    if (port) {
      url.port = port;
    }
    frame.src = url;
  })();
</script></div>
</div>
</section>
<section id="Train-the-WarpDrive-Module">
<h2>Train the WarpDrive Module<a class="headerlink" href="#Train-the-WarpDrive-Module" title="Permalink to this headline">¶</a></h2>
<p>Finally, we invoke training.</p>
<p>Note: please scroll up to the tensorboard cell to visualize the curves during training.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">wd_module</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/home/AzDevOps_azpcontainer/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.
  rank_zero_deprecation(
/home/AzDevOps_azpcontainer/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.
  rank_zero_deprecation(
Missing logger folder: /__w/1/s/lightning_logs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name | Type | Params
------------------------------
------------------------------
0         Trainable params
0         Non-trainable params
0         Total params
0.000     Total estimated model params size (MB)
/home/AzDevOps_azpcontainer/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/AzDevOps_azpcontainer/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1927: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  rank_zero_warn(
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "ec750af199a94f8091c03c862ee4ce11", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.29561
Policy loss                             :   -0.09944
Value function loss                     :    4.22670
Mean rewards                            :   -0.02825
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.26719
Mean advantages                         :   -0.02164
Mean (norm.) advantages                 :   -0.02164
Mean (discounted) returns               :   -1.28883
Mean normalized returns                 :   -1.28883
Mean entropy                            :    4.76890
Variance explained by the value function:    0.16321
Std. of action_0 over agents            :    3.10107
Std. of action_0 over envs              :    3.09726
Std. of action_0 over time              :    3.10287
Std. of action_1 over agents            :    3.03511
Std. of action_1 over envs              :    3.03153
Std. of action_1 over time              :    3.03523
Current timestep                        : 100000.00000
Gradient norm                           :    0.04779
Mean episodic reward                    : -458.86800
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   71.48376
Policy loss                             :   66.68478
Value function loss                     :  499.21271
Mean rewards                            :    0.58700
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :    4.72709
Mean advantages                         :   17.19324
Mean (norm.) advantages                 :   17.19324
Mean (discounted) returns               :   21.92033
Mean normalized returns                 :   21.92033
Mean entropy                            :    3.86286
Variance explained by the value function:   -0.01202
Std. of action_0 over agents            :    2.20117
Std. of action_0 over envs              :    2.34950
Std. of action_0 over time              :    2.35609
Std. of action_1 over agents            :    2.13672
Std. of action_1 over envs              :    2.26055
Std. of action_1 over time              :    2.26863
Current timestep                        : 100000.00000
Gradient norm                           :    1.10622
Mean episodic reward                    :  496.12000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 10 / 1000
Mean training time per iter (ms)        :     139.64
Mean steps per sec (training time)      :   71614.70


</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/home/AzDevOps_azpcontainer/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:229: UserWarning: You called `self.log(&#39;Current timestep_runner&#39;, ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.
  warning_cache.warn(
/home/AzDevOps_azpcontainer/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:229: UserWarning: You called `self.log(&#39;Current timestep_tagger&#39;, ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.
  warning_cache.warn(
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.13941
Policy loss                             :    0.05974
Value function loss                     :    3.92918
Mean rewards                            :   -0.02666
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.17205
Mean advantages                         :    0.01279
Mean (norm.) advantages                 :    0.01279
Mean (discounted) returns               :   -1.15927
Mean normalized returns                 :   -1.15927
Mean entropy                            :    4.76878
Variance explained by the value function:    0.18872
Std. of action_0 over agents            :    3.16038
Std. of action_0 over envs              :    3.15714
Std. of action_0 over time              :    3.16244
Std. of action_1 over agents            :    3.30826
Std. of action_1 over envs              :    3.30508
Std. of action_1 over time              :    3.30887
Current timestep                        : 200000.00000
Gradient norm                           :    0.03368
Mean episodic reward                    : -563.87200
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    0.28682
Policy loss                             :   -1.30073
Value function loss                     :  165.07660
Mean rewards                            :    0.55720
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   26.87305
Mean advantages                         :   -1.25896
Mean (norm.) advantages                 :   -1.25896
Mean (discounted) returns               :   25.61409
Mean normalized returns                 :   25.61409
Mean entropy                            :    1.26443
Variance explained by the value function:   -0.09630
Std. of action_0 over agents            :    2.01996
Std. of action_0 over envs              :    2.10392
Std. of action_0 over time              :    2.10761
Std. of action_1 over agents            :    0.03658
Std. of action_1 over envs              :    0.10289
Std. of action_1 over time              :    0.16429
Current timestep                        : 200000.00000
Gradient norm                           :    0.77949
Mean episodic reward                    :  585.78000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 20 / 1000
Mean training time per iter (ms)        :     134.83
Mean steps per sec (training time)      :   74165.85


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.36544
Policy loss                             :   -0.16869
Value function loss                     :    4.23149
Mean rewards                            :   -0.02958
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.26441
Mean advantages                         :   -0.03540
Mean (norm.) advantages                 :   -0.03540
Mean (discounted) returns               :   -1.29981
Mean normalized returns                 :   -1.29981
Mean entropy                            :    4.78127
Variance explained by the value function:    0.19381
Std. of action_0 over agents            :    3.17005
Std. of action_0 over envs              :    3.16643
Std. of action_0 over time              :    3.17118
Std. of action_1 over agents            :    3.12740
Std. of action_1 over envs              :    3.12359
Std. of action_1 over time              :    3.12819
Current timestep                        : 300000.00000
Gradient norm                           :    0.05014
Mean episodic reward                    : -568.40400
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    1.72281
Policy loss                             :    0.04230
Value function loss                     :  177.68925
Mean rewards                            :    0.60780
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   28.67383
Mean advantages                         :   -0.02816
Mean (norm.) advantages                 :   -0.02816
Mean (discounted) returns               :   28.64566
Mean normalized returns                 :   28.64566
Mean entropy                            :    1.92779
Variance explained by the value function:   -0.02407
Std. of action_0 over agents            :    0.52569
Std. of action_0 over envs              :    0.58831
Std. of action_0 over time              :    0.59848
Std. of action_1 over agents            :    1.45080
Std. of action_1 over envs              :    1.51973
Std. of action_1 over time              :    1.52414
Current timestep                        : 300000.00000
Gradient norm                           :    0.98253
Mean episodic reward                    :  588.54000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 30 / 1000
Mean training time per iter (ms)        :     134.78
Mean steps per sec (training time)      :   74196.02


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.51885
Policy loss                             :   -0.32013
Value function loss                     :    4.05303
Mean rewards                            :   -0.02816
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.12684
Mean advantages                         :   -0.06705
Mean (norm.) advantages                 :   -0.06705
Mean (discounted) returns               :   -1.19390
Mean normalized returns                 :   -1.19390
Mean entropy                            :    4.78491
Variance explained by the value function:    0.19311
Std. of action_0 over agents            :    3.15052
Std. of action_0 over envs              :    3.14774
Std. of action_0 over time              :    3.15191
Std. of action_1 over agents            :    3.16333
Std. of action_1 over envs              :    3.16009
Std. of action_1 over time              :    3.16444
Current timestep                        : 400000.00000
Gradient norm                           :    0.02620
Mean episodic reward                    : -561.13800
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    3.69027
Policy loss                             :    2.25955
Value function loss                     :  152.33397
Mean rewards                            :    0.58580
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   24.85278
Mean advantages                         :    1.14076
Mean (norm.) advantages                 :    1.14076
Mean (discounted) returns               :   25.99354
Mean normalized returns                 :   25.99354
Mean entropy                            :    1.85237
Variance explained by the value function:   -0.01995
Std. of action_0 over agents            :    0.56588
Std. of action_0 over envs              :    0.64696
Std. of action_0 over time              :    0.65732
Std. of action_1 over agents            :    1.34437
Std. of action_1 over envs              :    1.43722
Std. of action_1 over time              :    1.44228
Current timestep                        : 400000.00000
Gradient norm                           :    0.83519
Mean episodic reward                    :  584.16000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 40 / 1000
Mean training time per iter (ms)        :     133.89
Mean steps per sec (training time)      :   74686.90


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.20911
Policy loss                             :   -0.00956
Value function loss                     :    3.96707
Mean rewards                            :   -0.02621
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.13329
Mean advantages                         :   -0.00148
Mean (norm.) advantages                 :   -0.00148
Mean (discounted) returns               :   -1.13477
Mean normalized returns                 :   -1.13477
Mean entropy                            :    4.78451
Variance explained by the value function:    0.17726
Std. of action_0 over agents            :    3.18450
Std. of action_0 over envs              :    3.18058
Std. of action_0 over time              :    3.18559
Std. of action_1 over agents            :    3.24211
Std. of action_1 over envs              :    3.23863
Std. of action_1 over time              :    3.24272
Current timestep                        : 500000.00000
Gradient norm                           :    0.02311
Mean episodic reward                    : -545.78200
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.63752
Policy loss                             :   -2.03540
Value function loss                     :  151.13307
Mean rewards                            :    0.55380
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   25.83076
Mean advantages                         :   -0.89841
Mean (norm.) advantages                 :   -0.89841
Mean (discounted) returns               :   24.93235
Mean normalized returns                 :   24.93235
Mean entropy                            :    2.26906
Variance explained by the value function:    0.04017
Std. of action_0 over agents            :    0.87775
Std. of action_0 over envs              :    1.06517
Std. of action_0 over time              :    1.07649
Std. of action_1 over agents            :    2.67889
Std. of action_1 over envs              :    2.77255
Std. of action_1 over time              :    2.77483
Current timestep                        : 500000.00000
Gradient norm                           :    1.02381
Mean episodic reward                    :  571.20000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 50 / 1000
Mean training time per iter (ms)        :     133.29
Mean steps per sec (training time)      :   75025.85


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.44279
Policy loss                             :   -0.24555
Value function loss                     :    4.19776
Mean rewards                            :   -0.02866
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.17080
Mean advantages                         :   -0.05098
Mean (norm.) advantages                 :   -0.05098
Mean (discounted) returns               :   -1.22179
Mean normalized returns                 :   -1.22179
Mean entropy                            :    4.78432
Variance explained by the value function:    0.17778
Std. of action_0 over agents            :    3.17138
Std. of action_0 over envs              :    3.16814
Std. of action_0 over time              :    3.17295
Std. of action_1 over agents            :    3.18664
Std. of action_1 over envs              :    3.18346
Std. of action_1 over time              :    3.18842
Current timestep                        : 600000.00000
Gradient norm                           :    0.01910
Mean episodic reward                    : -569.85800
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    0.19026
Policy loss                             :   -1.14752
Value function loss                     :  141.49873
Mean rewards                            :    0.59960
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   27.95154
Mean advantages                         :   -0.79166
Mean (norm.) advantages                 :   -0.79166
Mean (discounted) returns               :   27.15989
Mean normalized returns                 :   27.15989
Mean entropy                            :    1.54421
Variance explained by the value function:    0.03882
Std. of action_0 over agents            :    0.68900
Std. of action_0 over envs              :    0.82649
Std. of action_0 over time              :    0.84436
Std. of action_1 over agents            :    1.53829
Std. of action_1 over envs              :    1.79476
Std. of action_1 over time              :    1.80652
Current timestep                        : 600000.00000
Gradient norm                           :    0.96701
Mean episodic reward                    :  592.64000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 60 / 1000
Mean training time per iter (ms)        :     132.84
Mean steps per sec (training time)      :   75276.37


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.17643
Policy loss                             :    0.02091
Value function loss                     :    4.20877
Mean rewards                            :   -0.02961
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.26399
Mean advantages                         :    0.00456
Mean (norm.) advantages                 :    0.00456
Mean (discounted) returns               :   -1.25943
Mean normalized returns                 :   -1.25943
Mean entropy                            :    4.78850
Variance explained by the value function:    0.19216
Std. of action_0 over agents            :    3.19524
Std. of action_0 over envs              :    3.19180
Std. of action_0 over time              :    3.19651
Std. of action_1 over agents            :    3.19286
Std. of action_1 over envs              :    3.18921
Std. of action_1 over time              :    3.19416
Current timestep                        : 700000.00000
Gradient norm                           :    0.02438
Mean episodic reward                    : -586.91200
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    1.17608
Policy loss                             :   -0.22491
Value function loss                     :  147.56619
Mean rewards                            :    0.61340
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   27.25769
Mean advantages                         :   -0.16004
Mean (norm.) advantages                 :   -0.16004
Mean (discounted) returns               :   27.09766
Mean normalized returns                 :   27.09766
Mean entropy                            :    1.49357
Variance explained by the value function:    0.08838
Std. of action_0 over agents            :    0.45946
Std. of action_0 over envs              :    0.49755
Std. of action_0 over time              :    0.50076
Std. of action_1 over agents            :    1.86805
Std. of action_1 over envs              :    2.02685
Std. of action_1 over time              :    2.03246
Current timestep                        : 700000.00000
Gradient norm                           :    0.84228
Mean episodic reward                    :  607.84000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 70 / 1000
Mean training time per iter (ms)        :     132.56
Mean steps per sec (training time)      :   75434.75


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.37541
Policy loss                             :   -0.17783
Value function loss                     :    4.18084
Mean rewards                            :   -0.02972
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.21098
Mean advantages                         :   -0.03708
Mean (norm.) advantages                 :   -0.03708
Mean (discounted) returns               :   -1.24806
Mean normalized returns                 :   -1.24806
Mean entropy                            :    4.78777
Variance explained by the value function:    0.19617
Std. of action_0 over agents            :    3.16429
Std. of action_0 over envs              :    3.16138
Std. of action_0 over time              :    3.16669
Std. of action_1 over agents            :    3.19662
Std. of action_1 over envs              :    3.19409
Std. of action_1 over time              :    3.19849
Current timestep                        : 800000.00000
Gradient norm                           :    0.02269
Mean episodic reward                    : -583.45200
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    0.97793
Policy loss                             :   -0.35443
Value function loss                     :  140.94687
Mean rewards                            :    0.61060
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   26.67221
Mean advantages                         :   -0.16298
Mean (norm.) advantages                 :   -0.16298
Mean (discounted) returns               :   26.50922
Mean normalized returns                 :   26.50922
Mean entropy                            :    1.54200
Variance explained by the value function:    0.10822
Std. of action_0 over agents            :    0.17347
Std. of action_0 over envs              :    0.28422
Std. of action_0 over time              :    0.29403
Std. of action_1 over agents            :    2.31733
Std. of action_1 over envs              :    2.39203
Std. of action_1 over time              :    2.39413
Current timestep                        : 800000.00000
Gradient norm                           :    0.76039
Mean episodic reward                    :  604.00000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 80 / 1000
Mean training time per iter (ms)        :     132.37
Mean steps per sec (training time)      :   75543.18


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.35127
Policy loss                             :   -0.15415
Value function loss                     :    4.22053
Mean rewards                            :   -0.03042
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.21477
Mean advantages                         :   -0.03194
Mean (norm.) advantages                 :   -0.03194
Mean (discounted) returns               :   -1.24672
Mean normalized returns                 :   -1.24672
Mean entropy                            :    4.78653
Variance explained by the value function:    0.19839
Std. of action_0 over agents            :    3.15758
Std. of action_0 over envs              :    3.15373
Std. of action_0 over time              :    3.15893
Std. of action_1 over agents            :    3.19264
Std. of action_1 over envs              :    3.18938
Std. of action_1 over time              :    3.19414
Current timestep                        : 900000.00000
Gradient norm                           :    0.02952
Mean episodic reward                    : -597.38200
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    1.64220
Policy loss                             :    0.34608
Value function loss                     :  138.46170
Mean rewards                            :    0.62180
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   25.70090
Mean advantages                         :    0.34612
Mean (norm.) advantages                 :    0.34612
Mean (discounted) returns               :   26.04702
Mean normalized returns                 :   26.04702
Mean entropy                            :    1.76990
Variance explained by the value function:    0.16364
Std. of action_0 over agents            :    0.44804
Std. of action_0 over envs              :    0.49295
Std. of action_0 over time              :    0.49739
Std. of action_1 over agents            :    2.28808
Std. of action_1 over envs              :    2.38469
Std. of action_1 over time              :    2.38598
Current timestep                        : 900000.00000
Gradient norm                           :    0.87399
Mean episodic reward                    :  616.32000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 90 / 1000
Mean training time per iter (ms)        :     132.34
Mean steps per sec (training time)      :   75563.67


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    0.10690
Policy loss                             :    0.30437
Value function loss                     :    4.17746
Mean rewards                            :   -0.03019
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.31399
Mean advantages                         :    0.06380
Mean (norm.) advantages                 :    0.06380
Mean (discounted) returns               :   -1.25019
Mean normalized returns                 :   -1.25019
Mean entropy                            :    4.78486
Variance explained by the value function:    0.20020
Std. of action_0 over agents            :    3.20304
Std. of action_0 over envs              :    3.19988
Std. of action_0 over time              :    3.20410
Std. of action_1 over agents            :    3.16478
Std. of action_1 over envs              :    3.16108
Std. of action_1 over time              :    3.16590
Current timestep                        : 1000000.00000
Gradient norm                           :    0.01797
Mean episodic reward                    : -609.90000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    1.83317
Policy loss                             :    0.55187
Value function loss                     :  135.31894
Mean rewards                            :    0.62240
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   25.07043
Mean advantages                         :    0.60529
Mean (norm.) advantages                 :    0.60529
Mean (discounted) returns               :   25.67571
Mean normalized returns                 :   25.67571
Mean entropy                            :    1.43778
Variance explained by the value function:    0.19644
Std. of action_0 over agents            :    0.41113
Std. of action_0 over envs              :    0.47021
Std. of action_0 over time              :    0.47753
Std. of action_1 over agents            :    1.64128
Std. of action_1 over envs              :    1.94565
Std. of action_1 over time              :    1.96274
Current timestep                        : 1000000.00000
Gradient norm                           :    0.85278
Mean episodic reward                    :  627.50000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 100 / 1000
Mean training time per iter (ms)        :     132.41
Mean steps per sec (training time)      :   75521.56


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.39392
Policy loss                             :   -0.19717
Value function loss                     :    4.26200
Mean rewards                            :   -0.03067
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.22065
Mean advantages                         :   -0.04117
Mean (norm.) advantages                 :   -0.04117
Mean (discounted) returns               :   -1.26182
Mean normalized returns                 :   -1.26182
Mean entropy                            :    4.78732
Variance explained by the value function:    0.19860
Std. of action_0 over agents            :    3.18279
Std. of action_0 over envs              :    3.17968
Std. of action_0 over time              :    3.18411
Std. of action_1 over agents            :    3.16197
Std. of action_1 over envs              :    3.15870
Std. of action_1 over time              :    3.16361
Current timestep                        : 1100000.00000
Gradient norm                           :    0.02074
Mean episodic reward                    : -604.74600
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    2.75310
Policy loss                             :    1.46008
Value function loss                     :  139.07147
Mean rewards                            :    0.62660
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   24.75457
Mean advantages                         :    0.96995
Mean (norm.) advantages                 :    0.96995
Mean (discounted) returns               :   25.72451
Mean normalized returns                 :   25.72451
Mean entropy                            :    1.95383
Variance explained by the value function:    0.23910
Std. of action_0 over agents            :    0.33641
Std. of action_0 over envs              :    0.44263
Std. of action_0 over time              :    0.46271
Std. of action_1 over agents            :    2.21386
Std. of action_1 over envs              :    2.32202
Std. of action_1 over time              :    2.32489
Current timestep                        : 1100000.00000
Gradient norm                           :    0.92750
Mean episodic reward                    :  622.28000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 110 / 1000
Mean training time per iter (ms)        :     132.54
Mean steps per sec (training time)      :   75450.48


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.39002
Policy loss                             :   -0.19326
Value function loss                     :    4.27940
Mean rewards                            :   -0.03079
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.22350
Mean advantages                         :   -0.04023
Mean (norm.) advantages                 :   -0.04023
Mean (discounted) returns               :   -1.26373
Mean normalized returns                 :   -1.26373
Mean entropy                            :    4.79115
Variance explained by the value function:    0.19672
Std. of action_0 over agents            :    3.17452
Std. of action_0 over envs              :    3.17079
Std. of action_0 over time              :    3.17553
Std. of action_1 over agents            :    3.19096
Std. of action_1 over envs              :    3.18752
Std. of action_1 over time              :    3.19235
Current timestep                        : 1200000.00000
Gradient norm                           :    0.02688
Mean episodic reward                    : -605.87600
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    1.38344
Policy loss                             :    0.15169
Value function loss                     :  132.42581
Mean rewards                            :    0.63200
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   25.21257
Mean advantages                         :    0.38275
Mean (norm.) advantages                 :    0.38275
Mean (discounted) returns               :   25.59531
Mean normalized returns                 :   25.59531
Mean entropy                            :    1.85001
Variance explained by the value function:    0.26496
Std. of action_0 over agents            :    0.54090
Std. of action_0 over envs              :    0.59882
Std. of action_0 over time              :    0.61370
Std. of action_1 over agents            :    1.77144
Std. of action_1 over envs              :    2.02314
Std. of action_1 over time              :    2.04445
Current timestep                        : 1200000.00000
Gradient norm                           :    1.05419
Mean episodic reward                    :  624.70000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 120 / 1000
Mean training time per iter (ms)        :     132.37
Mean steps per sec (training time)      :   75547.62


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    0.07034
Policy loss                             :    0.26758
Value function loss                     :    4.21671
Mean rewards                            :   -0.03039
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.32277
Mean advantages                         :    0.05596
Mean (norm.) advantages                 :    0.05596
Mean (discounted) returns               :   -1.26681
Mean normalized returns                 :   -1.26681
Mean entropy                            :    4.78819
Variance explained by the value function:    0.20097
Std. of action_0 over agents            :    3.18787
Std. of action_0 over envs              :    3.18447
Std. of action_0 over time              :    3.18937
Std. of action_1 over agents            :    3.18469
Std. of action_1 over envs              :    3.18079
Std. of action_1 over time              :    3.18619
Current timestep                        : 1300000.00000
Gradient norm                           :    0.02152
Mean episodic reward                    : -614.82200
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -1.28483
Policy loss                             :   -2.41039
Value function loss                     :  120.68580
Mean rewards                            :    0.62540
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   26.84370
Mean advantages                         :   -1.51105
Mean (norm.) advantages                 :   -1.51105
Mean (discounted) returns               :   25.33265
Mean normalized returns                 :   25.33265
Mean entropy                            :    1.62589
Variance explained by the value function:    0.29816
Std. of action_0 over agents            :    0.50313
Std. of action_0 over envs              :    0.59231
Std. of action_0 over time              :    0.61607
Std. of action_1 over agents            :    1.22392
Std. of action_1 over envs              :    1.56280
Std. of action_1 over time              :    1.62526
Current timestep                        : 1300000.00000
Gradient norm                           :    0.96978
Mean episodic reward                    :  631.48000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 130 / 1000
Mean training time per iter (ms)        :     132.22
Mean steps per sec (training time)      :   75632.89


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    0.00544
Policy loss                             :    0.20296
Value function loss                     :    4.18451
Mean rewards                            :   -0.02998
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.27892
Mean advantages                         :    0.04237
Mean (norm.) advantages                 :    0.04237
Mean (discounted) returns               :   -1.23654
Mean normalized returns                 :   -1.23654
Mean entropy                            :    4.78746
Variance explained by the value function:    0.19364
Std. of action_0 over agents            :    3.16542
Std. of action_0 over envs              :    3.16184
Std. of action_0 over time              :    3.16614
Std. of action_1 over agents            :    3.19429
Std. of action_1 over envs              :    3.19155
Std. of action_1 over time              :    3.19567
Current timestep                        : 1400000.00000
Gradient norm                           :    0.02590
Mean episodic reward                    : -608.21800
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.18401
Policy loss                             :   -1.24549
Value function loss                     :  117.24991
Mean rewards                            :    0.62040
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   24.58486
Mean advantages                         :   -0.49179
Mean (norm.) advantages                 :   -0.49179
Mean (discounted) returns               :   24.09308
Mean normalized returns                 :   24.09308
Mean entropy                            :    2.22046
Variance explained by the value function:    0.33025
Std. of action_0 over agents            :    1.10082
Std. of action_0 over envs              :    1.21927
Std. of action_0 over time              :    1.27917
Std. of action_1 over agents            :    1.50083
Std. of action_1 over envs              :    1.78938
Std. of action_1 over time              :    1.83957
Current timestep                        : 1400000.00000
Gradient norm                           :    0.86178
Mean episodic reward                    :  626.04000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 140 / 1000
Mean training time per iter (ms)        :     132.06
Mean steps per sec (training time)      :   75722.11


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.41031
Policy loss                             :   -0.21337
Value function loss                     :    4.25794
Mean rewards                            :   -0.03000
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.20284
Mean advantages                         :   -0.04444
Mean (norm.) advantages                 :   -0.04444
Mean (discounted) returns               :   -1.24728
Mean normalized returns                 :   -1.24728
Mean entropy                            :    4.79041
Variance explained by the value function:    0.19141
Std. of action_0 over agents            :    3.17172
Std. of action_0 over envs              :    3.16820
Std. of action_0 over time              :    3.17362
Std. of action_1 over agents            :    3.18974
Std. of action_1 over envs              :    3.18661
Std. of action_1 over time              :    3.19123
Current timestep                        : 1500000.00000
Gradient norm                           :    0.01839
Mean episodic reward                    : -598.17600
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    1.32612
Policy loss                             :    0.24926
Value function loss                     :  121.56894
Mean rewards                            :    0.61660
Max. rewards                            :   30.00000
Min. rewards                            :    0.00000
Mean value function                     :   23.76665
Mean advantages                         :    0.15925
Mean (norm.) advantages                 :    0.15925
Mean (discounted) returns               :   23.92591
Mean normalized returns                 :   23.92591
Mean entropy                            :    2.77672
Variance explained by the value function:    0.37911
Std. of action_0 over agents            :    1.59717
Std. of action_0 over envs              :    1.64885
Std. of action_0 over time              :    1.65476
Std. of action_1 over agents            :    2.29025
Std. of action_1 over envs              :    2.40091
Std. of action_1 over time              :    2.41089
Current timestep                        : 1500000.00000
Gradient norm                           :    1.04956
Mean episodic reward                    :  617.52000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 150 / 1000
Mean training time per iter (ms)        :     131.96
Mean steps per sec (training time)      :   75778.85


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.30629
Policy loss                             :   -0.10944
Value function loss                     :    4.25746
Mean rewards                            :   -0.02960
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.22184
Mean advantages                         :   -0.02271
Mean (norm.) advantages                 :   -0.02271
Mean (discounted) returns               :   -1.24455
Mean normalized returns                 :   -1.24455
Mean entropy                            :    4.78840
Variance explained by the value function:    0.18661
Std. of action_0 over agents            :    3.18294
Std. of action_0 over envs              :    3.17974
Std. of action_0 over time              :    3.18427
Std. of action_1 over agents            :    3.18077
Std. of action_1 over envs              :    3.17743
Std. of action_1 over time              :    3.18236
Current timestep                        : 1600000.00000
Gradient norm                           :    0.02283
Mean episodic reward                    : -594.66800
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    0.11075
Policy loss                             :   -0.99814
Value function loss                     :  124.14204
Mean rewards                            :    0.61020
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   24.20610
Mean advantages                         :   -0.38055
Mean (norm.) advantages                 :   -0.38055
Mean (discounted) returns               :   23.82555
Mean normalized returns                 :   23.82555
Mean entropy                            :    2.65065
Variance explained by the value function:    0.36260
Std. of action_0 over agents            :    1.31017
Std. of action_0 over envs              :    1.38641
Std. of action_0 over time              :    1.39297
Std. of action_1 over agents            :    2.24370
Std. of action_1 over envs              :    2.36572
Std. of action_1 over time              :    2.37353
Current timestep                        : 1600000.00000
Gradient norm                           :    0.86174
Mean episodic reward                    :  614.16000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 160 / 1000
Mean training time per iter (ms)        :     131.89
Mean steps per sec (training time)      :   75822.87


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    0.02414
Policy loss                             :    0.22181
Value function loss                     :    4.18004
Mean rewards                            :   -0.03014
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.30193
Mean advantages                         :    0.04642
Mean (norm.) advantages                 :    0.04642
Mean (discounted) returns               :   -1.25551
Mean normalized returns                 :   -1.25551
Mean entropy                            :    4.78947
Variance explained by the value function:    0.20049
Std. of action_0 over agents            :    3.19362
Std. of action_0 over envs              :    3.19033
Std. of action_0 over time              :    3.19515
Std. of action_1 over agents            :    3.13914
Std. of action_1 over envs              :    3.13572
Std. of action_1 over time              :    3.14080
Current timestep                        : 1700000.00000
Gradient norm                           :    0.01649
Mean episodic reward                    : -610.47400
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.32916
Policy loss                             :   -1.36375
Value function loss                     :  115.13750
Mean rewards                            :    0.62180
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   24.30268
Mean advantages                         :   -0.59685
Mean (norm.) advantages                 :   -0.59685
Mean (discounted) returns               :   23.70583
Mean normalized returns                 :   23.70583
Mean entropy                            :    2.33580
Variance explained by the value function:    0.39727
Std. of action_0 over agents            :    0.94601
Std. of action_0 over envs              :    1.08593
Std. of action_0 over time              :    1.21527
Std. of action_1 over agents            :    2.09410
Std. of action_1 over envs              :    2.25233
Std. of action_1 over time              :    2.26746
Current timestep                        : 1700000.00000
Gradient norm                           :    1.10716
Mean episodic reward                    :  628.58000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 170 / 1000
Mean training time per iter (ms)        :     131.79
Mean steps per sec (training time)      :   75876.73


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.20829
Policy loss                             :   -0.01122
Value function loss                     :    4.24434
Mean rewards                            :   -0.03009
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.24558
Mean advantages                         :   -0.00225
Mean (norm.) advantages                 :   -0.00225
Mean (discounted) returns               :   -1.24783
Mean normalized returns                 :   -1.24783
Mean entropy                            :    4.79017
Variance explained by the value function:    0.19038
Std. of action_0 over agents            :    3.15255
Std. of action_0 over envs              :    3.14831
Std. of action_0 over time              :    3.15344
Std. of action_1 over agents            :    3.16293
Std. of action_1 over envs              :    3.15896
Std. of action_1 over time              :    3.16376
Current timestep                        : 1800000.00000
Gradient norm                           :    0.01817
Mean episodic reward                    : -606.14600
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    0.19917
Policy loss                             :   -0.77462
Value function loss                     :  109.51952
Mean rewards                            :    0.62060
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   23.86355
Mean advantages                         :   -0.33225
Mean (norm.) advantages                 :   -0.33225
Mean (discounted) returns               :   23.53131
Mean normalized returns                 :   23.53131
Mean entropy                            :    2.42816
Variance explained by the value function:    0.42407
Std. of action_0 over agents            :    1.38931
Std. of action_0 over envs              :    1.44448
Std. of action_0 over time              :    1.47183
Std. of action_1 over agents            :    2.31901
Std. of action_1 over envs              :    2.42196
Std. of action_1 over time              :    2.43184
Current timestep                        : 1800000.00000
Gradient norm                           :    1.05716
Mean episodic reward                    :  623.88000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 180 / 1000
Mean training time per iter (ms)        :     131.91
Mean steps per sec (training time)      :   75809.11


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.42851
Policy loss                             :   -0.23205
Value function loss                     :    4.29204
Mean rewards                            :   -0.03097
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.20935
Mean advantages                         :   -0.04841
Mean (norm.) advantages                 :   -0.04841
Mean (discounted) returns               :   -1.25776
Mean normalized returns                 :   -1.25776
Mean entropy                            :    4.78749
Variance explained by the value function:    0.19585
Std. of action_0 over agents            :    3.18641
Std. of action_0 over envs              :    3.18215
Std. of action_0 over time              :    3.18681
Std. of action_1 over agents            :    3.19706
Std. of action_1 over envs              :    3.19332
Std. of action_1 over time              :    3.19855
Current timestep                        : 1900000.00000
Gradient norm                           :    0.02202
Mean episodic reward                    : -615.55000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    0.98144
Policy loss                             :   -0.16142
Value function loss                     :  124.75267
Mean rewards                            :    0.63580
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   24.12560
Mean advantages                         :   -0.11957
Mean (norm.) advantages                 :   -0.11957
Mean (discounted) returns               :   24.00603
Mean normalized returns                 :   24.00603
Mean entropy                            :    2.09317
Variance explained by the value function:    0.38898
Std. of action_0 over agents            :    1.16746
Std. of action_0 over envs              :    1.26150
Std. of action_0 over time              :    1.32636
Std. of action_1 over agents            :    1.95041
Std. of action_1 over envs              :    2.18977
Std. of action_1 over time              :    2.27316
Current timestep                        : 1900000.00000
Gradient norm                           :    1.04704
Mean episodic reward                    :  633.00000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 190 / 1000
Mean training time per iter (ms)        :     131.84
Mean steps per sec (training time)      :   75852.18


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.08543
Policy loss                             :    0.11124
Value function loss                     :    4.27781
Mean rewards                            :   -0.03067
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.29198
Mean advantages                         :    0.02349
Mean (norm.) advantages                 :    0.02349
Mean (discounted) returns               :   -1.26849
Mean normalized returns                 :   -1.26849
Mean entropy                            :    4.78886
Variance explained by the value function:    0.19626
Std. of action_0 over agents            :    3.17970
Std. of action_0 over envs              :    3.17575
Std. of action_0 over time              :    3.18013
Std. of action_1 over agents            :    3.17027
Std. of action_1 over envs              :    3.16672
Std. of action_1 over time              :    3.17144
Current timestep                        : 2000000.00000
Gradient norm                           :    0.02273
Mean episodic reward                    : -616.72000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    1.64687
Policy loss                             :    0.58691
Value function loss                     :  116.93488
Mean rewards                            :    0.63000
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   23.57109
Mean advantages                         :    0.33452
Mean (norm.) advantages                 :    0.33452
Mean (discounted) returns               :   23.90561
Mean normalized returns                 :   23.90561
Mean entropy                            :    2.18783
Variance explained by the value function:    0.43835
Std. of action_0 over agents            :    1.66334
Std. of action_0 over envs              :    1.77110
Std. of action_0 over time              :    1.80041
Std. of action_1 over agents            :    2.40191
Std. of action_1 over envs              :    2.50231
Std. of action_1 over time              :    2.53251
Current timestep                        : 2000000.00000
Gradient norm                           :    0.77135
Mean episodic reward                    :  632.76000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 200 / 1000
Mean training time per iter (ms)        :     131.79
Mean steps per sec (training time)      :   75880.73


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.13355
Policy loss                             :    0.06291
Value function loss                     :    4.27997
Mean rewards                            :   -0.03061
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.28207
Mean advantages                         :    0.01329
Mean (norm.) advantages                 :    0.01329
Mean (discounted) returns               :   -1.26878
Mean normalized returns                 :   -1.26878
Mean entropy                            :    4.78521
Variance explained by the value function:    0.19319
Std. of action_0 over agents            :    3.16152
Std. of action_0 over envs              :    3.15819
Std. of action_0 over time              :    3.16267
Std. of action_1 over agents            :    3.17672
Std. of action_1 over envs              :    3.17344
Std. of action_1 over time              :    3.17818
Current timestep                        : 2100000.00000
Gradient norm                           :    0.02232
Mean episodic reward                    : -621.98600
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    1.42578
Policy loss                             :    0.31507
Value function loss                     :  122.08124
Mean rewards                            :    0.62960
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   23.93601
Mean advantages                         :    0.13993
Mean (norm.) advantages                 :    0.13993
Mean (discounted) returns               :   24.07594
Mean normalized returns                 :   24.07594
Mean entropy                            :    2.20208
Variance explained by the value function:    0.38437
Std. of action_0 over agents            :    1.76383
Std. of action_0 over envs              :    1.86312
Std. of action_0 over time              :    1.88367
Std. of action_1 over agents            :    2.17340
Std. of action_1 over envs              :    2.34448
Std. of action_1 over time              :    2.41432
Current timestep                        : 2100000.00000
Gradient norm                           :    1.08965
Mean episodic reward                    :  635.40000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 210 / 1000
Mean training time per iter (ms)        :     131.77
Mean steps per sec (training time)      :   75891.75


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.55194
Policy loss                             :   -0.35554
Value function loss                     :    4.29110
Mean rewards                            :   -0.03062
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.18174
Mean advantages                         :   -0.07408
Mean (norm.) advantages                 :   -0.07408
Mean (discounted) returns               :   -1.25582
Mean normalized returns                 :   -1.25582
Mean entropy                            :    4.78629
Variance explained by the value function:    0.19081
Std. of action_0 over agents            :    3.18329
Std. of action_0 over envs              :    3.17954
Std. of action_0 over time              :    3.18449
Std. of action_1 over agents            :    3.19019
Std. of action_1 over envs              :    3.18702
Std. of action_1 over time              :    3.19155
Current timestep                        : 2200000.00000
Gradient norm                           :    0.01847
Mean episodic reward                    : -615.82000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    4.05776
Policy loss                             :    2.98207
Value function loss                     :  119.69363
Mean rewards                            :    0.62780
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   22.43901
Mean advantages                         :    1.34366
Mean (norm.) advantages                 :    1.34366
Mean (discounted) returns               :   23.78267
Mean normalized returns                 :   23.78267
Mean entropy                            :    2.42494
Variance explained by the value function:    0.40296
Std. of action_0 over agents            :    1.55592
Std. of action_0 over envs              :    1.75418
Std. of action_0 over time              :    1.88715
Std. of action_1 over agents            :    2.20406
Std. of action_1 over envs              :    2.36238
Std. of action_1 over time              :    2.42003
Current timestep                        : 2200000.00000
Gradient norm                           :    1.11617
Mean episodic reward                    :  631.42000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 220 / 1000
Mean training time per iter (ms)        :     131.79
Mean steps per sec (training time)      :   75877.68


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.31791
Policy loss                             :   -0.12191
Value function loss                     :    4.29101
Mean rewards                            :   -0.03028
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.24447
Mean advantages                         :   -0.02517
Mean (norm.) advantages                 :   -0.02517
Mean (discounted) returns               :   -1.26964
Mean normalized returns                 :   -1.26964
Mean entropy                            :    4.77825
Variance explained by the value function:    0.19055
Std. of action_0 over agents            :    3.23531
Std. of action_0 over envs              :    3.23149
Std. of action_0 over time              :    3.23582
Std. of action_1 over agents            :    3.23770
Std. of action_1 over envs              :    3.23386
Std. of action_1 over time              :    3.23866
Current timestep                        : 2300000.00000
Gradient norm                           :    0.03656
Mean episodic reward                    : -620.32200
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    4.11510
Policy loss                             :    2.96850
Value function loss                     :  127.32042
Mean rewards                            :    0.62220
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   22.82997
Mean advantages                         :    1.24553
Mean (norm.) advantages                 :    1.24553
Mean (discounted) returns               :   24.07550
Mean normalized returns                 :   24.07550
Mean entropy                            :    2.53215
Variance explained by the value function:    0.36879
Std. of action_0 over agents            :    3.15088
Std. of action_0 over envs              :    3.35648
Std. of action_0 over time              :    3.37690
Std. of action_1 over agents            :    2.19397
Std. of action_1 over envs              :    2.34609
Std. of action_1 over time              :    2.40001
Current timestep                        : 2300000.00000
Gradient norm                           :    1.12607
Mean episodic reward                    :  637.84000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 230 / 1000
Mean training time per iter (ms)        :     131.74
Mean steps per sec (training time)      :   75909.50


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.12808
Policy loss                             :    0.06856
Value function loss                     :    4.27281
Mean rewards                            :   -0.03011
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.29624
Mean advantages                         :    0.01435
Mean (norm.) advantages                 :    0.01435
Mean (discounted) returns               :   -1.28189
Mean normalized returns                 :   -1.28189
Mean entropy                            :    4.78748
Variance explained by the value function:    0.19714
Std. of action_0 over agents            :    3.19865
Std. of action_0 over envs              :    3.19564
Std. of action_0 over time              :    3.20017
Std. of action_1 over agents            :    3.17714
Std. of action_1 over envs              :    3.17289
Std. of action_1 over time              :    3.17825
Current timestep                        : 2400000.00000
Gradient norm                           :    0.02217
Mean episodic reward                    : -594.38400
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    1.22460
Policy loss                             :    0.09114
Value function loss                     :  123.45505
Mean rewards                            :    0.61940
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   23.95072
Mean advantages                         :    0.15197
Mean (norm.) advantages                 :    0.15197
Mean (discounted) returns               :   24.10269
Mean normalized returns                 :   24.10269
Mean entropy                            :    2.02174
Variance explained by the value function:    0.40530
Std. of action_0 over agents            :    1.24395
Std. of action_0 over envs              :    1.61606
Std. of action_0 over time              :    1.74416
Std. of action_1 over agents            :    2.19025
Std. of action_1 over envs              :    2.32380
Std. of action_1 over time              :    2.38768
Current timestep                        : 2400000.00000
Gradient norm                           :    0.82868
Mean episodic reward                    :  613.96000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 240 / 1000
Mean training time per iter (ms)        :     131.65
Mean steps per sec (training time)      :   75958.06


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.30096
Policy loss                             :   -0.10410
Value function loss                     :    4.26636
Mean rewards                            :   -0.03140
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.26093
Mean advantages                         :   -0.02166
Mean (norm.) advantages                 :   -0.02166
Mean (discounted) returns               :   -1.28259
Mean normalized returns                 :   -1.28259
Mean entropy                            :    4.79065
Variance explained by the value function:    0.20772
Std. of action_0 over agents            :    3.16324
Std. of action_0 over envs              :    3.16009
Std. of action_0 over time              :    3.16499
Std. of action_1 over agents            :    3.14587
Std. of action_1 over envs              :    3.14183
Std. of action_1 over time              :    3.14709
Current timestep                        : 2500000.00000
Gradient norm                           :    0.01473
Mean episodic reward                    : -615.28600
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    2.87641
Policy loss                             :    1.84401
Value function loss                     :  115.04574
Mean rewards                            :    0.64240
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   23.24988
Mean advantages                         :    0.87736
Mean (norm.) advantages                 :    0.87736
Mean (discounted) returns               :   24.12724
Mean normalized returns                 :   24.12724
Mean entropy                            :    2.36099
Variance explained by the value function:    0.42961
Std. of action_0 over agents            :    1.85476
Std. of action_0 over envs              :    1.97942
Std. of action_0 over time              :    2.03621
Std. of action_1 over agents            :    1.88634
Std. of action_1 over envs              :    2.13732
Std. of action_1 over time              :    2.25540
Current timestep                        : 2500000.00000
Gradient norm                           :    0.95807
Mean episodic reward                    :  630.76000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 250 / 1000
Mean training time per iter (ms)        :     131.64
Mean steps per sec (training time)      :   75964.50


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.38766
Policy loss                             :   -0.19114
Value function loss                     :    4.29504
Mean rewards                            :   -0.03145
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.24266
Mean advantages                         :   -0.04000
Mean (norm.) advantages                 :   -0.04000
Mean (discounted) returns               :   -1.28266
Mean normalized returns                 :   -1.28266
Mean entropy                            :    4.78925
Variance explained by the value function:    0.20545
Std. of action_0 over agents            :    3.16964
Std. of action_0 over envs              :    3.16577
Std. of action_0 over time              :    3.17155
Std. of action_1 over agents            :    3.19146
Std. of action_1 over envs              :    3.18798
Std. of action_1 over time              :    3.19313
Current timestep                        : 2600000.00000
Gradient norm                           :    0.02372
Mean episodic reward                    : -621.43600
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    4.27217
Policy loss                             :    3.13305
Value function loss                     :  125.62384
Mean rewards                            :    0.63980
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   22.81732
Mean advantages                         :    1.42910
Mean (norm.) advantages                 :    1.42910
Mean (discounted) returns               :   24.24642
Mean normalized returns                 :   24.24642
Mean entropy                            :    2.34247
Variance explained by the value function:    0.39670
Std. of action_0 over agents            :    1.66779
Std. of action_0 over envs              :    1.81548
Std. of action_0 over time              :    1.87217
Std. of action_1 over agents            :    2.11618
Std. of action_1 over envs              :    2.30114
Std. of action_1 over time              :    2.37297
Current timestep                        : 2600000.00000
Gradient norm                           :    1.16582
Mean episodic reward                    :  637.22000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 260 / 1000
Mean training time per iter (ms)        :     131.64
Mean steps per sec (training time)      :   75966.86


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    0.18943
Policy loss                             :    0.38655
Value function loss                     :    4.21191
Mean rewards                            :   -0.02945
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.33643
Mean advantages                         :    0.08120
Mean (norm.) advantages                 :    0.08120
Mean (discounted) returns               :   -1.25523
Mean normalized returns                 :   -1.25523
Mean entropy                            :    4.78478
Variance explained by the value function:    0.19130
Std. of action_0 over agents            :    3.16777
Std. of action_0 over envs              :    3.16463
Std. of action_0 over time              :    3.16860
Std. of action_1 over agents            :    3.17843
Std. of action_1 over envs              :    3.17478
Std. of action_1 over time              :    3.17978
Current timestep                        : 2700000.00000
Gradient norm                           :    0.01877
Mean episodic reward                    : -602.01000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    2.00235
Policy loss                             :    0.90322
Value function loss                     :  123.10245
Mean rewards                            :    0.60860
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   23.18112
Mean advantages                         :    0.37243
Mean (norm.) advantages                 :    0.37243
Mean (discounted) returns               :   23.55355
Mean normalized returns                 :   23.55355
Mean entropy                            :    2.63800
Variance explained by the value function:    0.38624
Std. of action_0 over agents            :    3.19479
Std. of action_0 over envs              :    3.35118
Std. of action_0 over time              :    3.35966
Std. of action_1 over agents            :    1.39973
Std. of action_1 over envs              :    1.70670
Std. of action_1 over time              :    1.85653
Current timestep                        : 2700000.00000
Gradient norm                           :    1.10109
Mean episodic reward                    :  619.46000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 270 / 1000
Mean training time per iter (ms)        :     131.60
Mean steps per sec (training time)      :   75988.60


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.13503
Policy loss                             :    0.06095
Value function loss                     :    4.27977
Mean rewards                            :   -0.02966
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.25697
Mean advantages                         :    0.01310
Mean (norm.) advantages                 :    0.01310
Mean (discounted) returns               :   -1.24387
Mean normalized returns                 :   -1.24387
Mean entropy                            :    4.77557
Variance explained by the value function:    0.17858
Std. of action_0 over agents            :    3.18454
Std. of action_0 over envs              :    3.18118
Std. of action_0 over time              :    3.18539
Std. of action_1 over agents            :    3.25448
Std. of action_1 over envs              :    3.25142
Std. of action_1 over time              :    3.25563
Current timestep                        : 2800000.00000
Gradient norm                           :    0.03724
Mean episodic reward                    : -597.61600
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.25420
Policy loss                             :   -1.37512
Value function loss                     :  126.51085
Mean rewards                            :    0.61000
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   24.40313
Mean advantages                         :   -0.50740
Mean (norm.) advantages                 :   -0.50740
Mean (discounted) returns               :   23.89573
Mean normalized returns                 :   23.89573
Mean entropy                            :    2.88382
Variance explained by the value function:    0.34452
Std. of action_0 over agents            :    2.32531
Std. of action_0 over envs              :    2.68173
Std. of action_0 over time              :    2.76141
Std. of action_1 over agents            :    2.44716
Std. of action_1 over envs              :    2.54633
Std. of action_1 over time              :    2.55902
Current timestep                        : 2800000.00000
Gradient norm                           :    0.77421
Mean episodic reward                    :  614.42000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 280 / 1000
Mean training time per iter (ms)        :     131.68
Mean steps per sec (training time)      :   75943.75


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.31438
Policy loss                             :   -0.11753
Value function loss                     :    4.26448
Mean rewards                            :   -0.03045
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.21748
Mean advantages                         :   -0.02442
Mean (norm.) advantages                 :   -0.02442
Mean (discounted) returns               :   -1.24190
Mean normalized returns                 :   -1.24190
Mean entropy                            :    4.78994
Variance explained by the value function:    0.19299
Std. of action_0 over agents            :    3.17974
Std. of action_0 over envs              :    3.17640
Std. of action_0 over time              :    3.18113
Std. of action_1 over agents            :    3.18217
Std. of action_1 over envs              :    3.17814
Std. of action_1 over time              :    3.18369
Current timestep                        : 2900000.00000
Gradient norm                           :    0.01567
Mean episodic reward                    : -612.18400
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.98043
Policy loss                             :   -2.14283
Value function loss                     :  128.53186
Mean rewards                            :    0.62820
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   24.90007
Mean advantages                         :   -0.76071
Mean (norm.) advantages                 :   -0.76071
Mean (discounted) returns               :   24.13935
Mean normalized returns                 :   24.13935
Mean entropy                            :    2.45836
Variance explained by the value function:    0.38491
Std. of action_0 over agents            :    1.84017
Std. of action_0 over envs              :    2.07527
Std. of action_0 over time              :    2.17418
Std. of action_1 over agents            :    2.30525
Std. of action_1 over envs              :    2.42692
Std. of action_1 over time              :    2.47542
Current timestep                        : 2900000.00000
Gradient norm                           :    0.94925
Mean episodic reward                    :  629.04000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 290 / 1000
Mean training time per iter (ms)        :     131.64
Mean steps per sec (training time)      :   75963.11


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.45456
Policy loss                             :   -0.25884
Value function loss                     :    4.35929
Mean rewards                            :   -0.03154
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.25759
Mean advantages                         :   -0.05392
Mean (norm.) advantages                 :   -0.05392
Mean (discounted) returns               :   -1.31151
Mean normalized returns                 :   -1.31151
Mean entropy                            :    4.78627
Variance explained by the value function:    0.20286
Std. of action_0 over agents            :    3.18172
Std. of action_0 over envs              :    3.17854
Std. of action_0 over time              :    3.18299
Std. of action_1 over agents            :    3.19275
Std. of action_1 over envs              :    3.18950
Std. of action_1 over time              :    3.19387
Current timestep                        : 3000000.00000
Gradient norm                           :    0.02393
Mean episodic reward                    : -610.13600
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    2.42917
Policy loss                             :    1.30917
Value function loss                     :  124.75227
Mean rewards                            :    0.64600
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   24.75241
Mean advantages                         :    0.43349
Mean (norm.) advantages                 :    0.43349
Mean (discounted) returns               :   25.18590
Mean normalized returns                 :   25.18590
Mean entropy                            :    2.55037
Variance explained by the value function:    0.39099
Std. of action_0 over agents            :    2.04294
Std. of action_0 over envs              :    2.32819
Std. of action_0 over time              :    2.38869
Std. of action_1 over agents            :    1.97610
Std. of action_1 over envs              :    2.19584
Std. of action_1 over time              :    2.28606
Current timestep                        : 3000000.00000
Gradient norm                           :    0.91051
Mean episodic reward                    :  627.40000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 300 / 1000
Mean training time per iter (ms)        :     131.69
Mean steps per sec (training time)      :   75936.21


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.15840
Policy loss                             :    0.03821
Value function loss                     :    4.27348
Mean rewards                            :   -0.02991
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.26943
Mean advantages                         :    0.00830
Mean (norm.) advantages                 :    0.00830
Mean (discounted) returns               :   -1.26113
Mean normalized returns                 :   -1.26113
Mean entropy                            :    4.78688
Variance explained by the value function:    0.18601
Std. of action_0 over agents            :    3.16144
Std. of action_0 over envs              :    3.15830
Std. of action_0 over time              :    3.16259
Std. of action_1 over agents            :    3.16965
Std. of action_1 over envs              :    3.16622
Std. of action_1 over time              :    3.17115
Current timestep                        : 3100000.00000
Gradient norm                           :    0.01593
Mean episodic reward                    : -609.42800
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -1.26713
Policy loss                             :   -2.42429
Value function loss                     :  129.74275
Mean rewards                            :    0.61660
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   24.99495
Mean advantages                         :   -0.96598
Mean (norm.) advantages                 :   -0.96598
Mean (discounted) returns               :   24.02897
Mean normalized returns                 :   24.02897
Mean entropy                            :    2.80518
Variance explained by the value function:    0.34719
Std. of action_0 over agents            :    3.25251
Std. of action_0 over envs              :    3.42312
Std. of action_0 over time              :    3.43372
Std. of action_1 over agents            :    2.22983
Std. of action_1 over envs              :    2.37029
Std. of action_1 over time              :    2.41865
Current timestep                        : 3100000.00000
Gradient norm                           :    1.10009
Mean episodic reward                    :  626.56000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 310 / 1000
Mean training time per iter (ms)        :     131.64
Mean steps per sec (training time)      :   75964.74


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.17846
Policy loss                             :    0.01729
Value function loss                     :    4.35013
Mean rewards                            :   -0.03173
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.32387
Mean advantages                         :    0.00398
Mean (norm.) advantages                 :    0.00398
Mean (discounted) returns               :   -1.31989
Mean normalized returns                 :   -1.31989
Mean entropy                            :    4.78514
Variance explained by the value function:    0.20404
Std. of action_0 over agents            :    3.18521
Std. of action_0 over envs              :    3.18170
Std. of action_0 over time              :    3.18635
Std. of action_1 over agents            :    3.16628
Std. of action_1 over envs              :    3.16292
Std. of action_1 over time              :    3.16696
Current timestep                        : 3200000.00000
Gradient norm                           :    0.02498
Mean episodic reward                    : -610.14200
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.11171
Policy loss                             :   -1.31491
Value function loss                     :  132.98608
Mean rewards                            :    0.64780
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   26.15527
Mean advantages                         :   -0.61356
Mean (norm.) advantages                 :   -0.61356
Mean (discounted) returns               :   25.54171
Mean normalized returns                 :   25.54171
Mean entropy                            :    2.53319
Variance explained by the value function:    0.34439
Std. of action_0 over agents            :    1.60258
Std. of action_0 over envs              :    1.73586
Std. of action_0 over time              :    1.78980
Std. of action_1 over agents            :    2.26054
Std. of action_1 over envs              :    2.39555
Std. of action_1 over time              :    2.46336
Current timestep                        : 3200000.00000
Gradient norm                           :    1.01753
Mean episodic reward                    :  627.02000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 320 / 1000
Mean training time per iter (ms)        :     131.63
Mean steps per sec (training time)      :   75969.76


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.46290
Policy loss                             :   -0.26678
Value function loss                     :    4.30847
Mean rewards                            :   -0.03139
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.22942
Mean advantages                         :   -0.05558
Mean (norm.) advantages                 :   -0.05558
Mean (discounted) returns               :   -1.28500
Mean normalized returns                 :   -1.28500
Mean entropy                            :    4.78396
Variance explained by the value function:    0.20248
Std. of action_0 over agents            :    3.22506
Std. of action_0 over envs              :    3.22058
Std. of action_0 over time              :    3.22621
Std. of action_1 over agents            :    3.19499
Std. of action_1 over envs              :    3.19075
Std. of action_1 over time              :    3.19572
Current timestep                        : 3300000.00000
Gradient norm                           :    0.01744
Mean episodic reward                    : -625.75200
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    1.06215
Policy loss                             :    0.01200
Value function loss                     :  117.38914
Mean rewards                            :    0.64280
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   24.93075
Mean advantages                         :    0.02592
Mean (norm.) advantages                 :    0.02592
Mean (discounted) returns               :   24.95667
Mean normalized returns                 :   24.95667
Mean entropy                            :    2.47481
Variance explained by the value function:    0.39445
Std. of action_0 over agents            :    1.94186
Std. of action_0 over envs              :    2.04290
Std. of action_0 over time              :    2.08669
Std. of action_1 over agents            :    1.84323
Std. of action_1 over envs              :    2.08935
Std. of action_1 over time              :    2.22527
Current timestep                        : 3300000.00000
Gradient norm                           :    1.10526
Mean episodic reward                    :  641.36000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 330 / 1000
Mean training time per iter (ms)        :     131.62
Mean steps per sec (training time)      :   75973.41


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.16995
Policy loss                             :    0.02676
Value function loss                     :    4.26990
Mean rewards                            :   -0.03074
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.27002
Mean advantages                         :    0.00586
Mean (norm.) advantages                 :    0.00586
Mean (discounted) returns               :   -1.26416
Mean normalized returns                 :   -1.26416
Mean entropy                            :    4.78805
Variance explained by the value function:    0.19528
Std. of action_0 over agents            :    3.20214
Std. of action_0 over envs              :    3.19784
Std. of action_0 over time              :    3.20232
Std. of action_1 over agents            :    3.15333
Std. of action_1 over envs              :    3.14942
Std. of action_1 over time              :    3.15518
Current timestep                        : 3400000.00000
Gradient norm                           :    0.01288
Mean episodic reward                    : -601.46200
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    0.71623
Policy loss                             :   -0.37041
Value function loss                     :  120.23585
Mean rewards                            :    0.62880
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   24.40251
Mean advantages                         :   -0.18308
Mean (norm.) advantages                 :   -0.18308
Mean (discounted) returns               :   24.21943
Mean normalized returns                 :   24.21943
Mean entropy                            :    2.31428
Variance explained by the value function:    0.38382
Std. of action_0 over agents            :    1.43672
Std. of action_0 over envs              :    1.54338
Std. of action_0 over time              :    1.58719
Std. of action_1 over agents            :    2.04946
Std. of action_1 over envs              :    2.24324
Std. of action_1 over time              :    2.34314
Current timestep                        : 3400000.00000
Gradient norm                           :    1.12347
Mean episodic reward                    :  619.60000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 340 / 1000
Mean training time per iter (ms)        :     131.68
Mean steps per sec (training time)      :   75942.90


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.13966
Policy loss                             :    0.05720
Value function loss                     :    4.23574
Mean rewards                            :   -0.03018
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.25712
Mean advantages                         :    0.01206
Mean (norm.) advantages                 :    0.01206
Mean (discounted) returns               :   -1.24506
Mean normalized returns                 :   -1.24506
Mean entropy                            :    4.78433
Variance explained by the value function:    0.19050
Std. of action_0 over agents            :    3.16613
Std. of action_0 over envs              :    3.16289
Std. of action_0 over time              :    3.16717
Std. of action_1 over agents            :    3.16475
Std. of action_1 over envs              :    3.16101
Std. of action_1 over time              :    3.16588
Current timestep                        : 3500000.00000
Gradient norm                           :    0.01915
Mean episodic reward                    : -610.17800
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.14786
Policy loss                             :   -1.23002
Value function loss                     :  121.78609
Mean rewards                            :    0.61880
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   23.97610
Mean advantages                         :   -0.48854
Mean (norm.) advantages                 :   -0.48854
Mean (discounted) returns               :   23.48756
Mean normalized returns                 :   23.48756
Mean entropy                            :    2.71398
Variance explained by the value function:    0.40043
Std. of action_0 over agents            :    1.67529
Std. of action_0 over envs              :    1.79216
Std. of action_0 over time              :    1.89752
Std. of action_1 over agents            :    2.03197
Std. of action_1 over envs              :    2.23206
Std. of action_1 over time              :    2.30676
Current timestep                        : 3500000.00000
Gradient norm                           :    0.90047
Mean episodic reward                    :  626.12000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 350 / 1000
Mean training time per iter (ms)        :     131.62
Mean steps per sec (training time)      :   75976.63


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.33324
Policy loss                             :   -0.13489
Value function loss                     :    4.11399
Mean rewards                            :   -0.02933
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.16604
Mean advantages                         :   -0.02844
Mean (norm.) advantages                 :   -0.02844
Mean (discounted) returns               :   -1.19448
Mean normalized returns                 :   -1.19448
Mean entropy                            :    4.78983
Variance explained by the value function:    0.19314
Std. of action_0 over agents            :    3.16849
Std. of action_0 over envs              :    3.16473
Std. of action_0 over time              :    3.17010
Std. of action_1 over agents            :    3.16927
Std. of action_1 over envs              :    3.16522
Std. of action_1 over time              :    3.17072
Current timestep                        : 3600000.00000
Gradient norm                           :    0.01620
Mean episodic reward                    : -605.11200
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    5.07056
Policy loss                             :    3.97148
Value function loss                     :  124.88232
Mean rewards                            :    0.60600
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   20.94012
Mean advantages                         :    1.41260
Mean (norm.) advantages                 :    1.41260
Mean (discounted) returns               :   22.35272
Mean normalized returns                 :   22.35272
Mean entropy                            :    2.99488
Variance explained by the value function:    0.38770
Std. of action_0 over agents            :    1.48510
Std. of action_0 over envs              :    1.61645
Std. of action_0 over time              :    1.74834
Std. of action_1 over agents            :    1.84253
Std. of action_1 over envs              :    2.03710
Std. of action_1 over time              :    2.09422
Current timestep                        : 3600000.00000
Gradient norm                           :    0.83337
Mean episodic reward                    :  623.70000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 360 / 1000
Mean training time per iter (ms)        :     131.58
Mean steps per sec (training time)      :   76001.51


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.62326
Policy loss                             :   -0.42676
Value function loss                     :    4.26578
Mean rewards                            :   -0.02989
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.13312
Mean advantages                         :   -0.08850
Mean (norm.) advantages                 :   -0.08850
Mean (discounted) returns               :   -1.22162
Mean normalized returns                 :   -1.22162
Mean entropy                            :    4.78316
Variance explained by the value function:    0.18194
Std. of action_0 over agents            :    3.19093
Std. of action_0 over envs              :    3.18740
Std. of action_0 over time              :    3.19163
Std. of action_1 over agents            :    3.18128
Std. of action_1 over envs              :    3.17857
Std. of action_1 over time              :    3.18246
Current timestep                        : 3700000.00000
Gradient norm                           :    0.02691
Mean episodic reward                    : -594.08600
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    4.58368
Policy loss                             :    3.41873
Value function loss                     :  130.18330
Mean rewards                            :    0.62100
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   22.15666
Mean advantages                         :    1.35365
Mean (norm.) advantages                 :    1.35365
Mean (discounted) returns               :   23.51031
Mean normalized returns                 :   23.51031
Mean entropy                            :    2.73768
Variance explained by the value function:    0.36659
Std. of action_0 over agents            :    1.42152
Std. of action_0 over envs              :    1.55856
Std. of action_0 over time              :    1.68198
Std. of action_1 over agents            :    1.83403
Std. of action_1 over envs              :    2.07478
Std. of action_1 over time              :    2.16052
Current timestep                        : 3700000.00000
Gradient norm                           :    1.06849
Mean episodic reward                    :  612.66000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 370 / 1000
Mean training time per iter (ms)        :     131.55
Mean steps per sec (training time)      :   76015.16


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    0.04268
Policy loss                             :    0.23911
Value function loss                     :    4.27183
Mean rewards                            :   -0.03005
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.29984
Mean advantages                         :    0.05027
Mean (norm.) advantages                 :    0.05027
Mean (discounted) returns               :   -1.24957
Mean normalized returns                 :   -1.24957
Mean entropy                            :    4.78297
Variance explained by the value function:    0.18250
Std. of action_0 over agents            :    3.21299
Std. of action_0 over envs              :    3.20958
Std. of action_0 over time              :    3.21325
Std. of action_1 over agents            :    3.20177
Std. of action_1 over envs              :    3.19858
Std. of action_1 over time              :    3.20289
Current timestep                        : 3800000.00000
Gradient norm                           :    0.02021
Mean episodic reward                    : -602.57800
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    1.56292
Policy loss                             :    0.49697
Value function loss                     :  120.66508
Mean rewards                            :    0.61880
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   23.59735
Mean advantages                         :    0.23922
Mean (norm.) advantages                 :    0.23922
Mean (discounted) returns               :   23.83657
Mean normalized returns                 :   23.83657
Mean entropy                            :    2.81394
Variance explained by the value function:    0.37444
Std. of action_0 over agents            :    1.64359
Std. of action_0 over envs              :    1.74559
Std. of action_0 over time              :    1.80098
Std. of action_1 over agents            :    1.76290
Std. of action_1 over envs              :    2.01482
Std. of action_1 over time              :    2.09230
Current timestep                        : 3800000.00000
Gradient norm                           :    0.79187
Mean episodic reward                    :  619.90000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 380 / 1000
Mean training time per iter (ms)        :     131.54
Mean steps per sec (training time)      :   76021.43


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.17471
Policy loss                             :    0.02226
Value function loss                     :    4.23532
Mean rewards                            :   -0.03032
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.26071
Mean advantages                         :    0.00465
Mean (norm.) advantages                 :    0.00465
Mean (discounted) returns               :   -1.25606
Mean normalized returns                 :   -1.25606
Mean entropy                            :    4.78651
Variance explained by the value function:    0.19371
Std. of action_0 over agents            :    3.18377
Std. of action_0 over envs              :    3.18081
Std. of action_0 over time              :    3.18555
Std. of action_1 over agents            :    3.15707
Std. of action_1 over envs              :    3.15388
Std. of action_1 over time              :    3.15852
Current timestep                        : 3900000.00000
Gradient norm                           :    0.01720
Mean episodic reward                    : -603.35600
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    1.81672
Policy loss                             :    0.63851
Value function loss                     :  130.94235
Mean rewards                            :    0.62260
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   23.75616
Mean advantages                         :    0.25667
Mean (norm.) advantages                 :    0.25667
Mean (discounted) returns               :   24.01283
Mean normalized returns                 :   24.01283
Mean entropy                            :    2.62433
Variance explained by the value function:    0.35954
Std. of action_0 over agents            :    2.16298
Std. of action_0 over envs              :    2.26112
Std. of action_0 over time              :    2.30542
Std. of action_1 over agents            :    1.24336
Std. of action_1 over envs              :    1.49583
Std. of action_1 over time              :    1.63456
Current timestep                        : 3900000.00000
Gradient norm                           :    0.92593
Mean episodic reward                    :  621.42000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 390 / 1000
Mean training time per iter (ms)        :     131.53
Mean steps per sec (training time)      :   76030.31


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.48521
Policy loss                             :   -0.28761
Value function loss                     :    4.12874
Mean rewards                            :   -0.02863
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.11529
Mean advantages                         :   -0.05942
Mean (norm.) advantages                 :   -0.05942
Mean (discounted) returns               :   -1.17470
Mean normalized returns                 :   -1.17470
Mean entropy                            :    4.77769
Variance explained by the value function:    0.18224
Std. of action_0 over agents            :    3.16015
Std. of action_0 over envs              :    3.15622
Std. of action_0 over time              :    3.16059
Std. of action_1 over agents            :    3.17417
Std. of action_1 over envs              :    3.17037
Std. of action_1 over time              :    3.17463
Current timestep                        : 4000000.00000
Gradient norm                           :    0.01883
Mean episodic reward                    : -584.42000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    3.04338
Policy loss                             :    1.89450
Value function loss                     :  129.88705
Mean rewards                            :    0.59020
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   21.50470
Mean advantages                         :    0.79050
Mean (norm.) advantages                 :    0.79050
Mean (discounted) returns               :   22.29520
Mean normalized returns                 :   22.29520
Mean entropy                            :    2.99987
Variance explained by the value function:    0.36567
Std. of action_0 over agents            :    1.66942
Std. of action_0 over envs              :    1.79491
Std. of action_0 over time              :    1.82833
Std. of action_1 over agents            :    1.43861
Std. of action_1 over envs              :    1.64330
Std. of action_1 over time              :    1.70277
Current timestep                        : 4000000.00000
Gradient norm                           :    1.00001
Mean episodic reward                    :  603.58000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 400 / 1000
Mean training time per iter (ms)        :     131.55
Mean steps per sec (training time)      :   76018.11


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.20110
Policy loss                             :   -0.00258
Value function loss                     :    4.09311
Mean rewards                            :   -0.02699
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.16695
Mean advantages                         :   -0.00034
Mean (norm.) advantages                 :   -0.00034
Mean (discounted) returns               :   -1.16729
Mean normalized returns                 :   -1.16729
Mean entropy                            :    4.78913
Variance explained by the value function:    0.16174
Std. of action_0 over agents            :    3.16714
Std. of action_0 over envs              :    3.16367
Std. of action_0 over time              :    3.16835
Std. of action_1 over agents            :    3.14600
Std. of action_1 over envs              :    3.14261
Std. of action_1 over time              :    3.14728
Current timestep                        : 4100000.00000
Gradient norm                           :    0.01767
Mean episodic reward                    : -573.87600
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.69126
Policy loss                             :   -1.79528
Value function loss                     :  125.64751
Mean rewards                            :    0.56740
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   23.28480
Mean advantages                         :   -0.87379
Mean (norm.) advantages                 :   -0.87379
Mean (discounted) returns               :   22.41101
Mean normalized returns                 :   22.41101
Mean entropy                            :    3.04909
Variance explained by the value function:    0.26955
Std. of action_0 over agents            :    1.63592
Std. of action_0 over envs              :    1.74235
Std. of action_0 over time              :    1.77057
Std. of action_1 over agents            :    2.47124
Std. of action_1 over envs              :    2.56823
Std. of action_1 over time              :    2.58002
Current timestep                        : 4100000.00000
Gradient norm                           :    1.12786
Mean episodic reward                    :  595.22000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 410 / 1000
Mean training time per iter (ms)        :     131.57
Mean steps per sec (training time)      :   76004.67


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    0.06058
Policy loss                             :    0.25825
Value function loss                     :    4.10964
Mean rewards                            :   -0.02905
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.27422
Mean advantages                         :    0.05514
Mean (norm.) advantages                 :    0.05514
Mean (discounted) returns               :   -1.21909
Mean normalized returns                 :   -1.21909
Mean entropy                            :    4.77531
Variance explained by the value function:    0.19852
Std. of action_0 over agents            :    3.18710
Std. of action_0 over envs              :    3.18347
Std. of action_0 over time              :    3.18824
Std. of action_1 over agents            :    3.21761
Std. of action_1 over envs              :    3.21327
Std. of action_1 over time              :    3.21761
Current timestep                        : 4200000.00000
Gradient norm                           :    0.03322
Mean episodic reward                    : -553.77200
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -2.21798
Policy loss                             :   -3.26730
Value function loss                     :  119.04290
Mean rewards                            :    0.60380
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   24.84565
Mean advantages                         :   -1.31369
Mean (norm.) advantages                 :   -1.31369
Mean (discounted) returns               :   23.53195
Mean normalized returns                 :   23.53195
Mean entropy                            :    2.82212
Variance explained by the value function:    0.36619
Std. of action_0 over agents            :    1.38333
Std. of action_0 over envs              :    1.53752
Std. of action_0 over time              :    1.59793
Std. of action_1 over agents            :    1.75281
Std. of action_1 over envs              :    1.93683
Std. of action_1 over time              :    1.97255
Current timestep                        : 4200000.00000
Gradient norm                           :    0.90726
Mean episodic reward                    :  577.66000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 420 / 1000
Mean training time per iter (ms)        :     131.56
Mean steps per sec (training time)      :   76009.21


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.18474
Policy loss                             :    0.01189
Value function loss                     :    4.26499
Mean rewards                            :   -0.02940
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.24892
Mean advantages                         :    0.00309
Mean (norm.) advantages                 :    0.00309
Mean (discounted) returns               :   -1.24584
Mean normalized returns                 :   -1.24584
Mean entropy                            :    4.78547
Variance explained by the value function:    0.17893
Std. of action_0 over agents            :    3.18786
Std. of action_0 over envs              :    3.18400
Std. of action_0 over time              :    3.18841
Std. of action_1 over agents            :    3.16584
Std. of action_1 over envs              :    3.16185
Std. of action_1 over time              :    3.16704
Current timestep                        : 4300000.00000
Gradient norm                           :    0.02370
Mean episodic reward                    : -574.09200
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    2.79387
Policy loss                             :    1.66883
Value function loss                     :  129.00156
Mean rewards                            :    0.60700
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   23.32399
Mean advantages                         :    0.48242
Mean (norm.) advantages                 :    0.48242
Mean (discounted) returns               :   23.80641
Mean normalized returns                 :   23.80641
Mean entropy                            :    3.29962
Variance explained by the value function:    0.33807
Std. of action_0 over agents            :    1.97080
Std. of action_0 over envs              :    2.13823
Std. of action_0 over time              :    2.19951
Std. of action_1 over agents            :    2.51205
Std. of action_1 over envs              :    2.61524
Std. of action_1 over time              :    2.63861
Current timestep                        : 4300000.00000
Gradient norm                           :    1.07733
Mean episodic reward                    :  595.68000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 430 / 1000
Mean training time per iter (ms)        :     131.55
Mean steps per sec (training time)      :   76015.14


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.35016
Policy loss                             :   -0.15345
Value function loss                     :    4.22506
Mean rewards                            :   -0.03015
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.20514
Mean advantages                         :   -0.03215
Mean (norm.) advantages                 :   -0.03215
Mean (discounted) returns               :   -1.23729
Mean normalized returns                 :   -1.23729
Mean entropy                            :    4.77925
Variance explained by the value function:    0.19107
Std. of action_0 over agents            :    3.15297
Std. of action_0 over envs              :    3.14875
Std. of action_0 over time              :    3.15251
Std. of action_1 over agents            :    3.16852
Std. of action_1 over envs              :    3.16440
Std. of action_1 over time              :    3.16943
Current timestep                        : 4400000.00000
Gradient norm                           :    0.01779
Mean episodic reward                    : -593.65600
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    4.81203
Policy loss                             :    3.68636
Value function loss                     :  127.10030
Mean rewards                            :    0.61940
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   22.12769
Mean advantages                         :    1.42967
Mean (norm.) advantages                 :    1.42967
Mean (discounted) returns               :   23.55736
Mean normalized returns                 :   23.55736
Mean entropy                            :    2.90670
Variance explained by the value function:    0.36861
Std. of action_0 over agents            :    2.65032
Std. of action_0 over envs              :    2.97717
Std. of action_0 over time              :    3.14853
Std. of action_1 over agents            :    1.35858
Std. of action_1 over envs              :    1.61464
Std. of action_1 over time              :    1.71680
Current timestep                        : 4400000.00000
Gradient norm                           :    1.16344
Mean episodic reward                    :  612.14000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 440 / 1000
Mean training time per iter (ms)        :     131.52
Mean steps per sec (training time)      :   76035.64


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.19392
Policy loss                             :    0.00368
Value function loss                     :    4.15307
Mean rewards                            :   -0.02844
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.18998
Mean advantages                         :    0.00096
Mean (norm.) advantages                 :    0.00096
Mean (discounted) returns               :   -1.18902
Mean normalized returns                 :   -1.18902
Mean entropy                            :    4.78275
Variance explained by the value function:    0.17421
Std. of action_0 over agents            :    3.14666
Std. of action_0 over envs              :    3.14257
Std. of action_0 over time              :    3.14725
Std. of action_1 over agents            :    3.13846
Std. of action_1 over envs              :    3.13396
Std. of action_1 over time              :    3.13949
Current timestep                        : 4500000.00000
Gradient norm                           :    0.02253
Mean episodic reward                    : -590.33800
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    2.83398
Policy loss                             :    1.78551
Value function loss                     :  118.63917
Mean rewards                            :    0.58880
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   21.62008
Mean advantages                         :    0.78197
Mean (norm.) advantages                 :    0.78197
Mean (discounted) returns               :   22.40204
Mean normalized returns                 :   22.40204
Mean entropy                            :    2.75842
Variance explained by the value function:    0.38188
Std. of action_0 over agents            :    1.63994
Std. of action_0 over envs              :    1.79117
Std. of action_0 over time              :    1.84512
Std. of action_1 over agents            :    1.06583
Std. of action_1 over envs              :    1.25039
Std. of action_1 over time              :    1.37107
Current timestep                        : 4500000.00000
Gradient norm                           :    1.16807
Mean episodic reward                    :  609.46000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 450 / 1000
Mean training time per iter (ms)        :     131.51
Mean steps per sec (training time)      :   76041.84


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.28916
Policy loss                             :   -0.09137
Value function loss                     :    4.15352
Mean rewards                            :   -0.02930
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.19019
Mean advantages                         :   -0.01906
Mean (norm.) advantages                 :   -0.01906
Mean (discounted) returns               :   -1.20925
Mean normalized returns                 :   -1.20925
Mean entropy                            :    4.78651
Variance explained by the value function:    0.18866
Std. of action_0 over agents            :    3.17974
Std. of action_0 over envs              :    3.17585
Std. of action_0 over time              :    3.18113
Std. of action_1 over agents            :    3.15586
Std. of action_1 over envs              :    3.15264
Std. of action_1 over time              :    3.15776
Current timestep                        : 4600000.00000
Gradient norm                           :    0.01948
Mean episodic reward                    : -586.58000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    4.54603
Policy loss                             :    3.42859
Value function loss                     :  127.13714
Mean rewards                            :    0.60380
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   21.68981
Mean advantages                         :    1.11002
Mean (norm.) advantages                 :    1.11002
Mean (discounted) returns               :   22.79983
Mean normalized returns                 :   22.79983
Mean entropy                            :    3.07853
Variance explained by the value function:    0.36333
Std. of action_0 over agents            :    1.48807
Std. of action_0 over envs              :    1.63037
Std. of action_0 over time              :    1.73430
Std. of action_1 over agents            :    1.26435
Std. of action_1 over envs              :    1.41115
Std. of action_1 over time              :    1.50269
Current timestep                        : 4600000.00000
Gradient norm                           :    0.83134
Mean episodic reward                    :  606.76000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 460 / 1000
Mean training time per iter (ms)        :     131.46
Mean steps per sec (training time)      :   76066.93


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.65366
Policy loss                             :   -0.45661
Value function loss                     :    4.18933
Mean rewards                            :   -0.02874
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.10567
Mean advantages                         :   -0.09550
Mean (norm.) advantages                 :   -0.09550
Mean (discounted) returns               :   -1.20117
Mean normalized returns                 :   -1.20117
Mean entropy                            :    4.77879
Variance explained by the value function:    0.18222
Std. of action_0 over agents            :    3.18494
Std. of action_0 over envs              :    3.18091
Std. of action_0 over time              :    3.18582
Std. of action_1 over agents            :    3.17401
Std. of action_1 over envs              :    3.16946
Std. of action_1 over time              :    3.17463
Current timestep                        : 4700000.00000
Gradient norm                           :    0.02612
Mean episodic reward                    : -575.90400
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    7.23932
Policy loss                             :    6.03731
Value function loss                     :  135.60860
Mean rewards                            :    0.59320
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   20.80790
Mean advantages                         :    2.10014
Mean (norm.) advantages                 :    2.10014
Mean (discounted) returns               :   22.90804
Mean normalized returns                 :   22.90804
Mean entropy                            :    3.08149
Variance explained by the value function:    0.34040
Std. of action_0 over agents            :    1.32211
Std. of action_0 over envs              :    1.51414
Std. of action_0 over time              :    1.61080
Std. of action_1 over agents            :    1.69204
Std. of action_1 over envs              :    1.85639
Std. of action_1 over time              :    1.89515
Current timestep                        : 4700000.00000
Gradient norm                           :    1.17552
Mean episodic reward                    :  596.60000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 470 / 1000
Mean training time per iter (ms)        :     131.44
Mean steps per sec (training time)      :   76082.82


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.46113
Policy loss                             :   -0.26462
Value function loss                     :    4.28217
Mean rewards                            :   -0.03009
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.20165
Mean advantages                         :   -0.05508
Mean (norm.) advantages                 :   -0.05508
Mean (discounted) returns               :   -1.25673
Mean normalized returns                 :   -1.25673
Mean entropy                            :    4.78655
Variance explained by the value function:    0.18939
Std. of action_0 over agents            :    3.17332
Std. of action_0 over envs              :    3.16987
Std. of action_0 over time              :    3.17413
Std. of action_1 over agents            :    3.14360
Std. of action_1 over envs              :    3.13937
Std. of action_1 over time              :    3.14530
Current timestep                        : 4800000.00000
Gradient norm                           :    0.01813
Mean episodic reward                    : -587.58400
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    2.19960
Policy loss                             :    1.14623
Value function loss                     :  120.46364
Mean rewards                            :    0.62260
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   24.07528
Mean advantages                         :    0.49286
Mean (norm.) advantages                 :    0.49286
Mean (discounted) returns               :   24.56815
Mean normalized returns                 :   24.56815
Mean entropy                            :    3.02521
Variance explained by the value function:    0.35859
Std. of action_0 over agents            :    1.62654
Std. of action_0 over envs              :    1.77381
Std. of action_0 over time              :    1.81114
Std. of action_1 over agents            :    2.07103
Std. of action_1 over envs              :    2.23856
Std. of action_1 over time              :    2.26556
Current timestep                        : 4800000.00000
Gradient norm                           :    1.04710
Mean episodic reward                    :  607.02000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 480 / 1000
Mean training time per iter (ms)        :     131.48
Mean steps per sec (training time)      :   76059.64


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.15208
Policy loss                             :    0.04440
Value function loss                     :    4.23780
Mean rewards                            :   -0.02952
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.25399
Mean advantages                         :    0.00979
Mean (norm.) advantages                 :    0.00979
Mean (discounted) returns               :   -1.24420
Mean normalized returns                 :   -1.24420
Mean entropy                            :    4.77725
Variance explained by the value function:    0.18121
Std. of action_0 over agents            :    3.14573
Std. of action_0 over envs              :    3.14131
Std. of action_0 over time              :    3.14561
Std. of action_1 over agents            :    3.16076
Std. of action_1 over envs              :    3.15667
Std. of action_1 over time              :    3.16215
Current timestep                        : 4900000.00000
Gradient norm                           :    0.01975
Mean episodic reward                    : -586.44200
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    1.42122
Policy loss                             :    0.38212
Value function loss                     :  118.11294
Mean rewards                            :    0.61200
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   23.95471
Mean advantages                         :    0.04253
Mean (norm.) advantages                 :    0.04253
Mean (discounted) returns               :   23.99724
Mean normalized returns                 :   23.99724
Mean entropy                            :    2.84069
Variance explained by the value function:    0.32629
Std. of action_0 over agents            :    1.95773
Std. of action_0 over envs              :    2.19158
Std. of action_0 over time              :    2.35496
Std. of action_1 over agents            :    2.15653
Std. of action_1 over envs              :    2.30828
Std. of action_1 over time              :    2.34185
Current timestep                        : 4900000.00000
Gradient norm                           :    0.99588
Mean episodic reward                    :  605.08000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 490 / 1000
Mean training time per iter (ms)        :     131.47
Mean steps per sec (training time)      :   76061.29


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.17971
Policy loss                             :    0.01790
Value function loss                     :    4.12042
Mean rewards                            :   -0.02852
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.20077
Mean advantages                         :    0.00373
Mean (norm.) advantages                 :    0.00373
Mean (discounted) returns               :   -1.19703
Mean normalized returns                 :   -1.19703
Mean entropy                            :    4.77635
Variance explained by the value function:    0.18098
Std. of action_0 over agents            :    3.17309
Std. of action_0 over envs              :    3.16883
Std. of action_0 over time              :    3.17305
Std. of action_1 over agents            :    3.18067
Std. of action_1 over envs              :    3.17674
Std. of action_1 over time              :    3.18183
Current timestep                        : 5000000.00000
Gradient norm                           :    0.02287
Mean episodic reward                    : -588.65000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    0.95276
Policy loss                             :   -0.21302
Value function loss                     :  131.84268
Mean rewards                            :    0.59280
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   23.20027
Mean advantages                         :   -0.19128
Mean (norm.) advantages                 :   -0.19128
Mean (discounted) returns               :   23.00900
Mean normalized returns                 :   23.00900
Mean entropy                            :    3.05294
Variance explained by the value function:    0.30400
Std. of action_0 over agents            :    3.36361
Std. of action_0 over envs              :    3.57013
Std. of action_0 over time              :    3.65972
Std. of action_1 over agents            :    2.04779
Std. of action_1 over envs              :    2.18085
Std. of action_1 over time              :    2.21191
Current timestep                        : 5000000.00000
Gradient norm                           :    0.94232
Mean episodic reward                    :  608.54000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 500 / 1000
Mean training time per iter (ms)        :     131.45
Mean steps per sec (training time)      :   76073.00


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.49737
Policy loss                             :   -0.30150
Value function loss                     :    4.28888
Mean rewards                            :   -0.02987
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.17837
Mean advantages                         :   -0.06192
Mean (norm.) advantages                 :   -0.06192
Mean (discounted) returns               :   -1.24030
Mean normalized returns                 :   -1.24030
Mean entropy                            :    4.77517
Variance explained by the value function:    0.17681
Std. of action_0 over agents            :    3.18182
Std. of action_0 over envs              :    3.17764
Std. of action_0 over time              :    3.18207
Std. of action_1 over agents            :    3.18461
Std. of action_1 over envs              :    3.18019
Std. of action_1 over time              :    3.18581
Current timestep                        : 5100000.00000
Gradient norm                           :    0.02515
Mean episodic reward                    : -577.27000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    0.43010
Policy loss                             :   -0.74591
Value function loss                     :  130.29968
Mean rewards                            :    0.61540
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   24.74081
Mean advantages                         :   -0.46125
Mean (norm.) advantages                 :   -0.46125
Mean (discounted) returns               :   24.27957
Mean normalized returns                 :   24.27957
Mean entropy                            :    2.53975
Variance explained by the value function:    0.28724
Std. of action_0 over agents            :    3.75676
Std. of action_0 over envs              :    3.96237
Std. of action_0 over time              :    4.03317
Std. of action_1 over agents            :    2.25058
Std. of action_1 over envs              :    2.37366
Std. of action_1 over time              :    2.40704
Current timestep                        : 5100000.00000
Gradient norm                           :    0.93709
Mean episodic reward                    :  598.44000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 510 / 1000
Mean training time per iter (ms)        :     131.46
Mean steps per sec (training time)      :   76065.93


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.20546
Policy loss                             :   -0.00907
Value function loss                     :    4.22638
Mean rewards                            :   -0.02972
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.24474
Mean advantages                         :   -0.00145
Mean (norm.) advantages                 :   -0.00145
Mean (discounted) returns               :   -1.24619
Mean normalized returns                 :   -1.24619
Mean entropy                            :    4.77288
Variance explained by the value function:    0.18467
Std. of action_0 over agents            :    3.17872
Std. of action_0 over envs              :    3.17436
Std. of action_0 over time              :    3.17863
Std. of action_1 over agents            :    3.14629
Std. of action_1 over envs              :    3.14274
Std. of action_1 over time              :    3.14751
Current timestep                        : 5200000.00000
Gradient norm                           :    0.01992
Mean episodic reward                    : -578.99600
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    2.52694
Policy loss                             :    1.41140
Value function loss                     :  125.54841
Mean rewards                            :    0.61180
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   23.70210
Mean advantages                         :    0.27958
Mean (norm.) advantages                 :    0.27958
Mean (discounted) returns               :   23.98168
Mean normalized returns                 :   23.98168
Mean entropy                            :    2.79895
Variance explained by the value function:    0.31139
Std. of action_0 over agents            :    3.32953
Std. of action_0 over envs              :    3.55246
Std. of action_0 over time              :    3.64770
Std. of action_1 over agents            :    1.49516
Std. of action_1 over envs              :    1.73328
Std. of action_1 over time              :    1.81478
Current timestep                        : 5200000.00000
Gradient norm                           :    1.01528
Mean episodic reward                    :  600.02000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 520 / 1000
Mean training time per iter (ms)        :     131.45
Mean steps per sec (training time)      :   76077.36


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.23724
Policy loss                             :   -0.03543
Value function loss                     :    3.73188
Mean rewards                            :   -0.02359
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.01206
Mean advantages                         :   -0.00738
Mean (norm.) advantages                 :   -0.00738
Mean (discounted) returns               :   -1.01945
Mean normalized returns                 :   -1.01945
Mean entropy                            :    4.78256
Variance explained by the value function:    0.14703
Std. of action_0 over agents            :    3.16255
Std. of action_0 over envs              :    3.15823
Std. of action_0 over time              :    3.16335
Std. of action_1 over agents            :    3.15058
Std. of action_1 over envs              :    3.14680
Std. of action_1 over time              :    3.15191
Current timestep                        : 5300000.00000
Gradient norm                           :    0.02115
Mean episodic reward                    : -529.06200
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    7.90190
Policy loss                             :    6.72238
Value function loss                     :  136.52116
Mean rewards                            :    0.50360
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   16.90433
Mean advantages                         :    1.87322
Mean (norm.) advantages                 :    1.87322
Mean (discounted) returns               :   18.77756
Mean normalized returns                 :   18.77756
Mean entropy                            :    3.71373
Variance explained by the value function:    0.28331
Std. of action_0 over agents            :    2.32177
Std. of action_0 over envs              :    2.52142
Std. of action_0 over time              :    2.56877
Std. of action_1 over agents            :    1.98013
Std. of action_1 over envs              :    2.10089
Std. of action_1 over time              :    2.12323
Current timestep                        : 5300000.00000
Gradient norm                           :    1.12699
Mean episodic reward                    :  555.30000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 530 / 1000
Mean training time per iter (ms)        :     131.43
Mean steps per sec (training time)      :   76085.21


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.17573
Policy loss                             :    0.02127
Value function loss                     :    4.15137
Mean rewards                            :   -0.02704
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.21360
Mean advantages                         :    0.00617
Mean (norm.) advantages                 :    0.00617
Mean (discounted) returns               :   -1.20743
Mean normalized returns                 :   -1.20743
Mean entropy                            :    4.77022
Variance explained by the value function:    0.15318
Std. of action_0 over agents            :    3.17482
Std. of action_0 over envs              :    3.17058
Std. of action_0 over time              :    3.17549
Std. of action_1 over agents            :    3.20741
Std. of action_1 over envs              :    3.20365
Std. of action_1 over time              :    3.20852
Current timestep                        : 5400000.00000
Gradient norm                           :    0.03017
Mean episodic reward                    : -532.63800
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -3.30575
Policy loss                             :   -4.49441
Value function loss                     :  136.40163
Mean rewards                            :    0.56540
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   25.17593
Mean advantages                         :   -1.35405
Mean (norm.) advantages                 :   -1.35405
Mean (discounted) returns               :   23.82188
Mean normalized returns                 :   23.82188
Mean entropy                            :    3.50715
Variance explained by the value function:    0.15117
Std. of action_0 over agents            :    2.40030
Std. of action_0 over envs              :    2.63203
Std. of action_0 over time              :    2.67567
Std. of action_1 over agents            :    1.73678
Std. of action_1 over envs              :    1.86570
Std. of action_1 over time              :    1.89477
Current timestep                        : 5400000.00000
Gradient norm                           :    1.09371
Mean episodic reward                    :  557.24000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 540 / 1000
Mean training time per iter (ms)        :     131.45
Mean steps per sec (training time)      :   76073.65


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.08114
Policy loss                             :    0.11695
Value function loss                     :    4.07153
Mean rewards                            :   -0.02625
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.20011
Mean advantages                         :    0.02562
Mean (norm.) advantages                 :    0.02562
Mean (discounted) returns               :   -1.17449
Mean normalized returns                 :   -1.17449
Mean entropy                            :    4.77622
Variance explained by the value function:    0.15503
Std. of action_0 over agents            :    3.15382
Std. of action_0 over envs              :    3.15006
Std. of action_0 over time              :    3.15490
Std. of action_1 over agents            :    3.13384
Std. of action_1 over envs              :    3.13046
Std. of action_1 over time              :    3.13516
Current timestep                        : 5500000.00000
Gradient norm                           :    0.03553
Mean episodic reward                    : -513.48600
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    3.18594
Policy loss                             :    1.75481
Value function loss                     :  161.62814
Mean rewards                            :    0.55020
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   22.06698
Mean advantages                         :    0.48769
Mean (norm.) advantages                 :    0.48769
Mean (discounted) returns               :   22.55467
Mean normalized returns                 :   22.55467
Mean entropy                            :    3.70296
Variance explained by the value function:    0.20894
Std. of action_0 over agents            :    2.45892
Std. of action_0 over envs              :    2.68662
Std. of action_0 over time              :    2.72122
Std. of action_1 over agents            :    1.90494
Std. of action_1 over envs              :    2.00838
Std. of action_1 over time              :    2.01924
Current timestep                        : 5500000.00000
Gradient norm                           :    0.91830
Mean episodic reward                    :  540.78000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 550 / 1000
Mean training time per iter (ms)        :     131.43
Mean steps per sec (training time)      :   76086.58


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.19464
Policy loss                             :    0.00367
Value function loss                     :    4.04854
Mean rewards                            :   -0.02771
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.19101
Mean advantages                         :    0.00057
Mean (norm.) advantages                 :    0.00057
Mean (discounted) returns               :   -1.19045
Mean normalized returns                 :   -1.19045
Mean entropy                            :    4.77594
Variance explained by the value function:    0.18235
Std. of action_0 over agents            :    3.18406
Std. of action_0 over envs              :    3.17920
Std. of action_0 over time              :    3.18416
Std. of action_1 over agents            :    3.16415
Std. of action_1 over envs              :    3.16005
Std. of action_1 over time              :    3.16502
Current timestep                        : 5600000.00000
Gradient norm                           :    0.02373
Mean episodic reward                    : -551.89200
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    4.25344
Policy loss                             :    3.14105
Value function loss                     :  127.98193
Mean rewards                            :    0.57700
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   21.67739
Mean advantages                         :    1.05651
Mean (norm.) advantages                 :    1.05651
Mean (discounted) returns               :   22.73390
Mean normalized returns                 :   22.73390
Mean entropy                            :    3.34863
Variance explained by the value function:    0.29933
Std. of action_0 over agents            :    2.32948
Std. of action_0 over envs              :    2.51772
Std. of action_0 over time              :    2.56449
Std. of action_1 over agents            :    1.50135
Std. of action_1 over envs              :    1.70499
Std. of action_1 over time              :    1.74539
Current timestep                        : 5600000.00000
Gradient norm                           :    0.91653
Mean episodic reward                    :  575.04000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 560 / 1000
Mean training time per iter (ms)        :     131.41
Mean steps per sec (training time)      :   76096.32


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.46283
Policy loss                             :   -0.26692
Value function loss                     :    4.21191
Mean rewards                            :   -0.02879
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.15552
Mean advantages                         :   -0.05532
Mean (norm.) advantages                 :   -0.05532
Mean (discounted) returns               :   -1.21085
Mean normalized returns                 :   -1.21085
Mean entropy                            :    4.76057
Variance explained by the value function:    0.16959
Std. of action_0 over agents            :    3.20372
Std. of action_0 over envs              :    3.19806
Std. of action_0 over time              :    3.20266
Std. of action_1 over agents            :    3.14751
Std. of action_1 over envs              :    3.14285
Std. of action_1 over time              :    3.14869
Current timestep                        : 5700000.00000
Gradient norm                           :    0.02271
Mean episodic reward                    : -570.32200
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    1.98510
Policy loss                             :    0.91608
Value function loss                     :  121.61179
Mean rewards                            :    0.60160
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   23.22964
Mean advantages                         :    0.32673
Mean (norm.) advantages                 :    0.32673
Mean (discounted) returns               :   23.55637
Mean normalized returns                 :   23.55637
Mean entropy                            :    2.94206
Variance explained by the value function:    0.32381
Std. of action_0 over agents            :    2.20196
Std. of action_0 over envs              :    2.38378
Std. of action_0 over time              :    2.46216
Std. of action_1 over agents            :    1.78699
Std. of action_1 over envs              :    2.02661
Std. of action_1 over time              :    2.11268
Current timestep                        : 5700000.00000
Gradient norm                           :    0.96420
Mean episodic reward                    :  593.58000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 570 / 1000
Mean training time per iter (ms)        :     131.40
Mean steps per sec (training time)      :   76104.39


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.33392
Policy loss                             :   -0.13712
Value function loss                     :    4.18797
Mean rewards                            :   -0.02823
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.17718
Mean advantages                         :   -0.02808
Mean (norm.) advantages                 :   -0.02808
Mean (discounted) returns               :   -1.20526
Mean normalized returns                 :   -1.20526
Mean entropy                            :    4.77350
Variance explained by the value function:    0.16641
Std. of action_0 over agents            :    3.19262
Std. of action_0 over envs              :    3.18818
Std. of action_0 over time              :    3.19217
Std. of action_1 over agents            :    3.18164
Std. of action_1 over envs              :    3.17761
Std. of action_1 over time              :    3.18330
Current timestep                        : 5800000.00000
Gradient norm                           :    0.01604
Mean episodic reward                    : -568.16600
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    2.92832
Policy loss                             :    1.86666
Value function loss                     :  122.10934
Mean rewards                            :    0.58640
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   22.52959
Mean advantages                         :    0.61113
Mean (norm.) advantages                 :    0.61113
Mean (discounted) returns               :   23.14072
Mean normalized returns                 :   23.14072
Mean entropy                            :    3.18853
Variance explained by the value function:    0.30759
Std. of action_0 over agents            :    2.44358
Std. of action_0 over envs              :    2.66187
Std. of action_0 over time              :    2.77153
Std. of action_1 over agents            :    2.05370
Std. of action_1 over envs              :    2.21585
Std. of action_1 over time              :    2.26112
Current timestep                        : 5800000.00000
Gradient norm                           :    0.94365
Mean episodic reward                    :  590.36000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 580 / 1000
Mean training time per iter (ms)        :     131.40
Mean steps per sec (training time)      :   76101.31


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.15272
Policy loss                             :    0.04362
Value function loss                     :    4.20945
Mean rewards                            :   -0.02781
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.22614
Mean advantages                         :    0.00973
Mean (norm.) advantages                 :    0.00973
Mean (discounted) returns               :   -1.21641
Mean normalized returns                 :   -1.21641
Mean entropy                            :    4.76871
Variance explained by the value function:    0.15643
Std. of action_0 over agents            :    3.17519
Std. of action_0 over envs              :    3.17035
Std. of action_0 over time              :    3.17581
Std. of action_1 over agents            :    3.14275
Std. of action_1 over envs              :    3.13791
Std. of action_1 over time              :    3.14412
Current timestep                        : 5900000.00000
Gradient norm                           :    0.01892
Mean episodic reward                    : -565.07400
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    0.98290
Policy loss                             :    0.00967
Value function loss                     :  111.87753
Mean rewards                            :    0.58160
Max. rewards                            :   30.00000
Min. rewards                            :    0.00000
Mean value function                     :   23.63635
Mean advantages                         :   -0.05174
Mean (norm.) advantages                 :   -0.05174
Mean (discounted) returns               :   23.58461
Mean normalized returns                 :   23.58461
Mean entropy                            :    2.91079
Variance explained by the value function:    0.27158
Std. of action_0 over agents            :    3.37658
Std. of action_0 over envs              :    3.48958
Std. of action_0 over time              :    3.52277
Std. of action_1 over agents            :    1.59840
Std. of action_1 over envs              :    1.84466
Std. of action_1 over time              :    1.91278
Current timestep                        : 5900000.00000
Gradient norm                           :    0.87798
Mean episodic reward                    :  588.56000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 590 / 1000
Mean training time per iter (ms)        :     131.39
Mean steps per sec (training time)      :   76110.99


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    0.03581
Policy loss                             :    0.23299
Value function loss                     :    4.11153
Mean rewards                            :   -0.02778
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.26051
Mean advantages                         :    0.04970
Mean (norm.) advantages                 :    0.04970
Mean (discounted) returns               :   -1.21081
Mean normalized returns                 :   -1.21081
Mean entropy                            :    4.76589
Variance explained by the value function:    0.17432
Std. of action_0 over agents            :    3.20113
Std. of action_0 over envs              :    3.19792
Std. of action_0 over time              :    3.20065
Std. of action_1 over agents            :    3.15068
Std. of action_1 over envs              :    3.14644
Std. of action_1 over time              :    3.15218
Current timestep                        : 6000000.00000
Gradient norm                           :    0.02276
Mean episodic reward                    : -563.92800
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.69090
Policy loss                             :   -1.77003
Value function loss                     :  123.61098
Mean rewards                            :    0.57840
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   23.90685
Mean advantages                         :   -0.56174
Mean (norm.) advantages                 :   -0.56174
Mean (discounted) returns               :   23.34510
Mean normalized returns                 :   23.34510
Mean entropy                            :    3.13955
Variance explained by the value function:    0.28989
Std. of action_0 over agents            :    2.08701
Std. of action_0 over envs              :    2.29075
Std. of action_0 over time              :    2.33911
Std. of action_1 over agents            :    1.29443
Std. of action_1 over envs              :    1.44860
Std. of action_1 over time              :    1.51060
Current timestep                        : 6000000.00000
Gradient norm                           :    1.08993
Mean episodic reward                    :  587.28000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 600 / 1000
Mean training time per iter (ms)        :     131.42
Mean steps per sec (training time)      :   76090.07


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    0.14761
Policy loss                             :    0.34345
Value function loss                     :    4.21633
Mean rewards                            :   -0.02780
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.31222
Mean advantages                         :    0.07579
Mean (norm.) advantages                 :    0.07579
Mean (discounted) returns               :   -1.23644
Mean normalized returns                 :   -1.23644
Mean entropy                            :    4.76005
Variance explained by the value function:    0.15355
Std. of action_0 over agents            :    3.17760
Std. of action_0 over envs              :    3.17276
Std. of action_0 over time              :    3.17647
Std. of action_1 over agents            :    3.14696
Std. of action_1 over envs              :    3.14291
Std. of action_1 over time              :    3.14829
Current timestep                        : 6100000.00000
Gradient norm                           :    0.03321
Mean episodic reward                    : -552.25200
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.73585
Policy loss                             :   -1.80803
Value function loss                     :  121.95171
Mean rewards                            :    0.57760
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   24.66278
Mean advantages                         :   -0.74601
Mean (norm.) advantages                 :   -0.74601
Mean (discounted) returns               :   23.91677
Mean normalized returns                 :   23.91677
Mean entropy                            :    2.94674
Variance explained by the value function:    0.21478
Std. of action_0 over agents            :    2.05188
Std. of action_0 over envs              :    2.26733
Std. of action_0 over time              :    2.36736
Std. of action_1 over agents            :    1.46500
Std. of action_1 over envs              :    1.67268
Std. of action_1 over time              :    1.71890
Current timestep                        : 6100000.00000
Gradient norm                           :    1.01474
Mean episodic reward                    :  577.78000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 610 / 1000
Mean training time per iter (ms)        :     131.43
Mean steps per sec (training time)      :   76085.63


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.08280
Policy loss                             :    0.11374
Value function loss                     :    4.11694
Mean rewards                            :   -0.02763
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.22817
Mean advantages                         :    0.02428
Mean (norm.) advantages                 :    0.02428
Mean (discounted) returns               :   -1.20389
Mean normalized returns                 :   -1.20389
Mean entropy                            :    4.75429
Variance explained by the value function:    0.16423
Std. of action_0 over agents            :    3.14703
Std. of action_0 over envs              :    3.14286
Std. of action_0 over time              :    3.14541
Std. of action_1 over agents            :    3.13414
Std. of action_1 over envs              :    3.13102
Std. of action_1 over time              :    3.13585
Current timestep                        : 6200000.00000
Gradient norm                           :    0.02878
Mean episodic reward                    : -545.17800
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    3.54672
Policy loss                             :    2.57837
Value function loss                     :  112.35289
Mean rewards                            :    0.58040
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   22.38323
Mean advantages                         :    0.83881
Mean (norm.) advantages                 :    0.83881
Mean (discounted) returns               :   23.22205
Mean normalized returns                 :   23.22205
Mean entropy                            :    3.10364
Variance explained by the value function:    0.27547
Std. of action_0 over agents            :    2.27488
Std. of action_0 over envs              :    2.41966
Std. of action_0 over time              :    2.47452
Std. of action_1 over agents            :    1.79500
Std. of action_1 over envs              :    1.96269
Std. of action_1 over time              :    1.98537
Current timestep                        : 6200000.00000
Gradient norm                           :    0.93361
Mean episodic reward                    :  570.42000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 620 / 1000
Mean training time per iter (ms)        :     131.41
Mean steps per sec (training time)      :   76097.81


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.40477
Policy loss                             :   -0.20684
Value function loss                     :    4.04879
Mean rewards                            :   -0.02623
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.10154
Mean advantages                         :   -0.04308
Mean (norm.) advantages                 :   -0.04308
Mean (discounted) returns               :   -1.14462
Mean normalized returns                 :   -1.14462
Mean entropy                            :    4.76828
Variance explained by the value function:    0.15046
Std. of action_0 over agents            :    3.14149
Std. of action_0 over envs              :    3.13660
Std. of action_0 over time              :    3.14045
Std. of action_1 over agents            :    3.17385
Std. of action_1 over envs              :    3.16958
Std. of action_1 over time              :    3.17540
Current timestep                        : 6300000.00000
Gradient norm                           :    0.03669
Mean episodic reward                    : -526.33000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    5.03141
Policy loss                             :    4.02790
Value function loss                     :  116.06092
Mean rewards                            :    0.55300
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   20.37084
Mean advantages                         :    1.37528
Mean (norm.) advantages                 :    1.37528
Mean (discounted) returns               :   21.74612
Mean normalized returns                 :   21.74612
Mean entropy                            :    3.14194
Variance explained by the value function:    0.29223
Std. of action_0 over agents            :    2.36502
Std. of action_0 over envs              :    2.54840
Std. of action_0 over time              :    2.60389
Std. of action_1 over agents            :    2.02204
Std. of action_1 over envs              :    2.19609
Std. of action_1 over time              :    2.21513
Current timestep                        : 6300000.00000
Gradient norm                           :    1.18241
Mean episodic reward                    :  554.18000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 630 / 1000
Mean training time per iter (ms)        :     131.39
Mean steps per sec (training time)      :   76108.91


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.14864
Policy loss                             :    0.04910
Value function loss                     :    4.03307
Mean rewards                            :   -0.02687
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.16444
Mean advantages                         :    0.01123
Mean (norm.) advantages                 :    0.01123
Mean (discounted) returns               :   -1.15321
Mean normalized returns                 :   -1.15321
Mean entropy                            :    4.76137
Variance explained by the value function:    0.16492
Std. of action_0 over agents            :    3.14725
Std. of action_0 over envs              :    3.14324
Std. of action_0 over time              :    3.14561
Std. of action_1 over agents            :    3.17265
Std. of action_1 over envs              :    3.16767
Std. of action_1 over time              :    3.17416
Current timestep                        : 6400000.00000
Gradient norm                           :    0.03134
Mean episodic reward                    : -553.48800
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    1.77719
Policy loss                             :    0.80037
Value function loss                     :  112.58723
Mean rewards                            :    0.56360
Max. rewards                            :   30.00000
Min. rewards                            :    0.00000
Mean value function                     :   21.42217
Mean advantages                         :    0.48211
Mean (norm.) advantages                 :    0.48211
Mean (discounted) returns               :   21.90427
Mean normalized returns                 :   21.90427
Mean entropy                            :    2.98098
Variance explained by the value function:    0.34493
Std. of action_0 over agents            :    2.58535
Std. of action_0 over envs              :    2.78940
Std. of action_0 over time              :    2.85302
Std. of action_1 over agents            :    1.46955
Std. of action_1 over envs              :    1.72610
Std. of action_1 over time              :    1.81765
Current timestep                        : 6400000.00000
Gradient norm                           :    1.04245
Mean episodic reward                    :  577.16000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 640 / 1000
Mean training time per iter (ms)        :     131.42
Mean steps per sec (training time)      :   76093.31


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.58180
Policy loss                             :   -0.38580
Value function loss                     :    4.17155
Mean rewards                            :   -0.02745
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.08903
Mean advantages                         :   -0.07911
Mean (norm.) advantages                 :   -0.07911
Mean (discounted) returns               :   -1.16814
Mean normalized returns                 :   -1.16814
Mean entropy                            :    4.75421
Variance explained by the value function:    0.15157
Std. of action_0 over agents            :    3.19460
Std. of action_0 over envs              :    3.18915
Std. of action_0 over time              :    3.19294
Std. of action_1 over agents            :    3.13581
Std. of action_1 over envs              :    3.13178
Std. of action_1 over time              :    3.13717
Current timestep                        : 6500000.00000
Gradient norm                           :    0.02576
Mean episodic reward                    : -533.82400
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    2.05177
Policy loss                             :    0.94913
Value function loss                     :  125.45804
Mean rewards                            :    0.57420
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   22.12563
Mean advantages                         :    0.29807
Mean (norm.) advantages                 :    0.29807
Mean (discounted) returns               :   22.42369
Mean normalized returns                 :   22.42369
Mean entropy                            :    3.03881
Variance explained by the value function:    0.30923
Std. of action_0 over agents            :    2.19948
Std. of action_0 over envs              :    2.40877
Std. of action_0 over time              :    2.53814
Std. of action_1 over agents            :    2.32494
Std. of action_1 over envs              :    2.41558
Std. of action_1 over time              :    2.44702
Current timestep                        : 6500000.00000
Gradient norm                           :    0.96988
Mean episodic reward                    :  560.52000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 650 / 1000
Mean training time per iter (ms)        :     131.40
Mean steps per sec (training time)      :   76100.93


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.29131
Policy loss                             :   -0.09424
Value function loss                     :    4.17410
Mean rewards                            :   -0.02822
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.19050
Mean advantages                         :   -0.01948
Mean (norm.) advantages                 :   -0.01948
Mean (discounted) returns               :   -1.20998
Mean normalized returns                 :   -1.20998
Mean entropy                            :    4.77623
Variance explained by the value function:    0.16383
Std. of action_0 over agents            :    3.18287
Std. of action_0 over envs              :    3.17907
Std. of action_0 over time              :    3.18387
Std. of action_1 over agents            :    3.13487
Std. of action_1 over envs              :    3.13069
Std. of action_1 over time              :    3.13606
Current timestep                        : 6600000.00000
Gradient norm                           :    0.02440
Mean episodic reward                    : -556.43200
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    2.26123
Policy loss                             :    1.24621
Value function loss                     :  116.68912
Mean rewards                            :    0.58600
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   23.01954
Mean advantages                         :    0.35317
Mean (norm.) advantages                 :    0.35317
Mean (discounted) returns               :   23.37271
Mean normalized returns                 :   23.37271
Mean entropy                            :    3.03736
Variance explained by the value function:    0.25681
Std. of action_0 over agents            :    2.46176
Std. of action_0 over envs              :    2.67155
Std. of action_0 over time              :    2.79139
Std. of action_1 over agents            :    2.28172
Std. of action_1 over envs              :    2.36076
Std. of action_1 over time              :    2.38159
Current timestep                        : 6600000.00000
Gradient norm                           :    0.89056
Mean episodic reward                    :  580.68000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 660 / 1000
Mean training time per iter (ms)        :     131.44
Mean steps per sec (training time)      :   76082.24


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.39086
Policy loss                             :   -0.19448
Value function loss                     :    4.14688
Mean rewards                            :   -0.02802
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.13990
Mean advantages                         :   -0.03972
Mean (norm.) advantages                 :   -0.03972
Mean (discounted) returns               :   -1.17961
Mean normalized returns                 :   -1.17961
Mean entropy                            :    4.75702
Variance explained by the value function:    0.16162
Std. of action_0 over agents            :    3.19944
Std. of action_0 over envs              :    3.19508
Std. of action_0 over time              :    3.19645
Std. of action_1 over agents            :    3.13580
Std. of action_1 over envs              :    3.13226
Std. of action_1 over time              :    3.13786
Current timestep                        : 6700000.00000
Gradient norm                           :    0.01965
Mean episodic reward                    : -551.67200
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.60327
Policy loss                             :   -1.72414
Value function loss                     :  127.66254
Mean rewards                            :    0.58420
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   23.66537
Mean advantages                         :   -0.43469
Mean (norm.) advantages                 :   -0.43469
Mean (discounted) returns               :   23.23068
Mean normalized returns                 :   23.23068
Mean entropy                            :    3.11519
Variance explained by the value function:    0.28490
Std. of action_0 over agents            :    2.52292
Std. of action_0 over envs              :    2.73544
Std. of action_0 over time              :    2.82339
Std. of action_1 over agents            :    1.70259
Std. of action_1 over envs              :    1.91424
Std. of action_1 over time              :    1.96555
Current timestep                        : 6700000.00000
Gradient norm                           :    0.96987
Mean episodic reward                    :  576.56000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 670 / 1000
Mean training time per iter (ms)        :     131.42
Mean steps per sec (training time)      :   76092.68


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    0.00764
Policy loss                             :    0.20491
Value function loss                     :    4.04347
Mean rewards                            :   -0.02699
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.21515
Mean advantages                         :    0.04446
Mean (norm.) advantages                 :    0.04446
Mean (discounted) returns               :   -1.17069
Mean normalized returns                 :   -1.17069
Mean entropy                            :    4.75394
Variance explained by the value function:    0.16005
Std. of action_0 over agents            :    3.18199
Std. of action_0 over envs              :    3.17782
Std. of action_0 over time              :    3.17947
Std. of action_1 over agents            :    3.15115
Std. of action_1 over envs              :    3.14654
Std. of action_1 over time              :    3.15266
Current timestep                        : 6800000.00000
Gradient norm                           :    0.04202
Mean episodic reward                    : -553.08600
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.71318
Policy loss                             :   -1.68677
Value function loss                     :  111.41720
Mean rewards                            :    0.56480
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   23.17813
Mean advantages                         :   -0.65383
Mean (norm.) advantages                 :   -0.65383
Mean (discounted) returns               :   22.52431
Mean normalized returns                 :   22.52431
Mean entropy                            :    2.81166
Variance explained by the value function:    0.27513
Std. of action_0 over agents            :    1.74926
Std. of action_0 over envs              :    2.03433
Std. of action_0 over time              :    2.19792
Std. of action_1 over agents            :    1.28842
Std. of action_1 over envs              :    1.45515
Std. of action_1 over time              :    1.51473
Current timestep                        : 6800000.00000
Gradient norm                           :    1.10341
Mean episodic reward                    :  576.80000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 680 / 1000
Mean training time per iter (ms)        :     131.41
Mean steps per sec (training time)      :   76100.57


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.74266
Policy loss                             :   -0.54684
Value function loss                     :    4.24369
Mean rewards                            :   -0.02948
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.12354
Mean advantages                         :   -0.11521
Mean (norm.) advantages                 :   -0.11521
Mean (discounted) returns               :   -1.23875
Mean normalized returns                 :   -1.23875
Mean entropy                            :    4.76507
Variance explained by the value function:    0.18181
Std. of action_0 over agents            :    3.16529
Std. of action_0 over envs              :    3.16118
Std. of action_0 over time              :    3.16448
Std. of action_1 over agents            :    3.14710
Std. of action_1 over envs              :    3.14322
Std. of action_1 over time              :    3.14840
Current timestep                        : 6900000.00000
Gradient norm                           :    0.03772
Mean episodic reward                    : -561.00200
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    4.27629
Policy loss                             :    3.15523
Value function loss                     :  127.43267
Mean rewards                            :    0.60880
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   23.02739
Mean advantages                         :    1.15527
Mean (norm.) advantages                 :    1.15527
Mean (discounted) returns               :   24.18266
Mean normalized returns                 :   24.18266
Mean entropy                            :    3.06529
Variance explained by the value function:    0.30697
Std. of action_0 over agents            :    1.91152
Std. of action_0 over envs              :    2.19590
Std. of action_0 over time              :    2.34553
Std. of action_1 over agents            :    1.78696
Std. of action_1 over envs              :    1.96121
Std. of action_1 over time              :    2.00706
Current timestep                        : 6900000.00000
Gradient norm                           :    1.04680
Mean episodic reward                    :  584.86000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 690 / 1000
Mean training time per iter (ms)        :     131.42
Mean steps per sec (training time)      :   76093.64


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.64072
Policy loss                             :   -0.44373
Value function loss                     :    4.12863
Mean rewards                            :   -0.02815
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.10326
Mean advantages                         :   -0.09225
Mean (norm.) advantages                 :   -0.09225
Mean (discounted) returns               :   -1.19551
Mean normalized returns                 :   -1.19551
Mean entropy                            :    4.76550
Variance explained by the value function:    0.17491
Std. of action_0 over agents            :    3.19077
Std. of action_0 over envs              :    3.18680
Std. of action_0 over time              :    3.18932
Std. of action_1 over agents            :    3.15081
Std. of action_1 over envs              :    3.14729
Std. of action_1 over time              :    3.15248
Current timestep                        : 7000000.00000
Gradient norm                           :    0.03479
Mean episodic reward                    : -573.15400
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    5.54484
Policy loss                             :    4.50058
Value function loss                     :  121.07406
Mean rewards                            :    0.58600
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   21.50246
Mean advantages                         :    1.38172
Mean (norm.) advantages                 :    1.38172
Mean (discounted) returns               :   22.88417
Mean normalized returns                 :   22.88417
Mean entropy                            :    3.32958
Variance explained by the value function:    0.30807
Std. of action_0 over agents            :    2.30922
Std. of action_0 over envs              :    2.58125
Std. of action_0 over time              :    2.67993
Std. of action_1 over agents            :    2.14134
Std. of action_1 over envs              :    2.21932
Std. of action_1 over time              :    2.24002
Current timestep                        : 7000000.00000
Gradient norm                           :    1.14815
Mean episodic reward                    :  594.32000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 700 / 1000
Mean training time per iter (ms)        :     131.42
Mean steps per sec (training time)      :   76091.61


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -1.18202
Policy loss                             :   -0.98700
Value function loss                     :    4.33388
Mean rewards                            :   -0.02840
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -0.99263
Mean advantages                         :   -0.20896
Mean (norm.) advantages                 :   -0.20896
Mean (discounted) returns               :   -1.20159
Mean normalized returns                 :   -1.20159
Mean entropy                            :    4.76709
Variance explained by the value function:    0.15117
Std. of action_0 over agents            :    3.15687
Std. of action_0 over envs              :    3.15258
Std. of action_0 over time              :    3.15823
Std. of action_1 over agents            :    3.18768
Std. of action_1 over envs              :    3.18350
Std. of action_1 over time              :    3.18890
Current timestep                        : 7100000.00000
Gradient norm                           :    0.03177
Mean episodic reward                    : -542.66800
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    7.05401
Policy loss                             :    5.93539
Value function loss                     :  127.55536
Mean rewards                            :    0.59100
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   21.64320
Mean advantages                         :    1.88968
Mean (norm.) advantages                 :    1.88968
Mean (discounted) returns               :   23.53288
Mean normalized returns                 :   23.53288
Mean entropy                            :    3.13867
Variance explained by the value function:    0.26558
Std. of action_0 over agents            :    3.19659
Std. of action_0 over envs              :    3.39083
Std. of action_0 over time              :    3.43133
Std. of action_1 over agents            :    2.17190
Std. of action_1 over envs              :    2.33291
Std. of action_1 over time              :    2.39964
Current timestep                        : 7100000.00000
Gradient norm                           :    1.13644
Mean episodic reward                    :  568.54000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 710 / 1000
Mean training time per iter (ms)        :     131.42
Mean steps per sec (training time)      :   76092.99


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    0.12173
Policy loss                             :    0.32024
Value function loss                     :    3.97932
Mean rewards                            :   -0.02645
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.24298
Mean advantages                         :    0.06853
Mean (norm.) advantages                 :    0.06853
Mean (discounted) returns               :   -1.17445
Mean normalized returns                 :   -1.17445
Mean entropy                            :    4.76600
Variance explained by the value function:    0.17313
Std. of action_0 over agents            :    3.17387
Std. of action_0 over envs              :    3.16990
Std. of action_0 over time              :    3.17308
Std. of action_1 over agents            :    3.14205
Std. of action_1 over envs              :    3.13698
Std. of action_1 over time              :    3.14322
Current timestep                        : 7200000.00000
Gradient norm                           :    0.03182
Mean episodic reward                    : -568.01400
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -9.84317
Policy loss                             :  -10.87781
Value function loss                     :  120.48790
Mean rewards                            :    0.55800
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   26.32091
Mean advantages                         :   -3.23924
Mean (norm.) advantages                 :   -3.23924
Mean (discounted) returns               :   23.08167
Mean normalized returns                 :   23.08167
Mean entropy                            :    3.40477
Variance explained by the value function:    0.26949
Std. of action_0 over agents            :    3.00225
Std. of action_0 over envs              :    3.19219
Std. of action_0 over time              :    3.22891
Std. of action_1 over agents            :    2.06082
Std. of action_1 over envs              :    2.15521
Std. of action_1 over time              :    2.17320
Current timestep                        : 7200000.00000
Gradient norm                           :    1.17321
Mean episodic reward                    :  591.24000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 720 / 1000
Mean training time per iter (ms)        :     131.47
Mean steps per sec (training time)      :   76064.20


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.45367
Policy loss                             :   -0.25851
Value function loss                     :    4.21425
Mean rewards                            :   -0.02754
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.16541
Mean advantages                         :   -0.05316
Mean (norm.) advantages                 :   -0.05316
Mean (discounted) returns               :   -1.21857
Mean normalized returns                 :   -1.21857
Mean entropy                            :    4.74592
Variance explained by the value function:    0.15314
Std. of action_0 over agents            :    3.21593
Std. of action_0 over envs              :    3.21080
Std. of action_0 over time              :    3.21304
Std. of action_1 over agents            :    3.13688
Std. of action_1 over envs              :    3.13147
Std. of action_1 over time              :    3.13815
Current timestep                        : 7300000.00000
Gradient norm                           :    0.03804
Mean episodic reward                    : -537.87800
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    1.23690
Policy loss                             :    0.18524
Value function loss                     :  121.67068
Mean rewards                            :    0.57540
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   23.67240
Mean advantages                         :    0.05390
Mean (norm.) advantages                 :    0.05390
Mean (discounted) returns               :   23.72630
Mean normalized returns                 :   23.72630
Mean entropy                            :    3.30091
Variance explained by the value function:    0.24785
Std. of action_0 over agents            :    2.23300
Std. of action_0 over envs              :    2.36538
Std. of action_0 over time              :    2.39389
Std. of action_1 over agents            :    2.31113
Std. of action_1 over envs              :    2.41149
Std. of action_1 over time              :    2.42972
Current timestep                        : 7300000.00000
Gradient norm                           :    1.18424
Mean episodic reward                    :  563.80000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 730 / 1000
Mean training time per iter (ms)        :     131.46
Mean steps per sec (training time)      :   76070.91


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.40600
Policy loss                             :   -0.20858
Value function loss                     :    4.09301
Mean rewards                            :   -0.02705
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.11920
Mean advantages                         :   -0.04473
Mean (norm.) advantages                 :   -0.04473
Mean (discounted) returns               :   -1.16393
Mean normalized returns                 :   -1.16393
Mean entropy                            :    4.76709
Variance explained by the value function:    0.15559
Std. of action_0 over agents            :    3.20028
Std. of action_0 over envs              :    3.19613
Std. of action_0 over time              :    3.20053
Std. of action_1 over agents            :    3.15291
Std. of action_1 over envs              :    3.14953
Std. of action_1 over time              :    3.15431
Current timestep                        : 7400000.00000
Gradient norm                           :    0.02327
Mean episodic reward                    : -541.72600
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    8.94342
Policy loss                             :    7.82372
Value function loss                     :  128.49229
Mean rewards                            :    0.56620
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   19.54300
Mean advantages                         :    2.46796
Mean (norm.) advantages                 :    2.46796
Mean (discounted) returns               :   22.01096
Mean normalized returns                 :   22.01096
Mean entropy                            :    3.30451
Variance explained by the value function:    0.27110
Std. of action_0 over agents            :    2.15864
Std. of action_0 over envs              :    2.29840
Std. of action_0 over time              :    2.33618
Std. of action_1 over agents            :    2.10426
Std. of action_1 over envs              :    2.24872
Std. of action_1 over time              :    2.26983
Current timestep                        : 7400000.00000
Gradient norm                           :    1.19526
Mean episodic reward                    :  567.74000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 740 / 1000
Mean training time per iter (ms)        :     131.46
Mean steps per sec (training time)      :   76066.34


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.08838
Policy loss                             :    0.10906
Value function loss                     :    4.02433
Mean rewards                            :   -0.02716
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.19793
Mean advantages                         :    0.02257
Mean (norm.) advantages                 :    0.02257
Mean (discounted) returns               :   -1.17536
Mean normalized returns                 :   -1.17536
Mean entropy                            :    4.75362
Variance explained by the value function:    0.17242
Std. of action_0 over agents            :    3.22729
Std. of action_0 over envs              :    3.22363
Std. of action_0 over time              :    3.22619
Std. of action_1 over agents            :    3.12576
Std. of action_1 over envs              :    3.12240
Std. of action_1 over time              :    3.12763
Current timestep                        : 7500000.00000
Gradient norm                           :    0.02426
Mean episodic reward                    : -554.52800
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -2.23446
Policy loss                             :   -3.19990
Value function loss                     :  112.95174
Mean rewards                            :    0.56820
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   23.66832
Mean advantages                         :   -0.97887
Mean (norm.) advantages                 :   -0.97887
Mean (discounted) returns               :   22.68945
Mean normalized returns                 :   22.68945
Mean entropy                            :    3.28147
Variance explained by the value function:    0.30452
Std. of action_0 over agents            :    2.36275
Std. of action_0 over envs              :    2.51470
Std. of action_0 over time              :    2.57744
Std. of action_1 over agents            :    1.75850
Std. of action_1 over envs              :    1.89775
Std. of action_1 over time              :    1.92031
Current timestep                        : 7500000.00000
Gradient norm                           :    1.14334
Mean episodic reward                    :  577.88000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 750 / 1000
Mean training time per iter (ms)        :     131.51
Mean steps per sec (training time)      :   76039.39


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.43181
Policy loss                             :   -0.23549
Value function loss                     :    4.12539
Mean rewards                            :   -0.02811
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.14847
Mean advantages                         :   -0.04847
Mean (norm.) advantages                 :   -0.04847
Mean (discounted) returns               :   -1.19694
Mean normalized returns                 :   -1.19694
Mean entropy                            :    4.75150
Variance explained by the value function:    0.17034
Std. of action_0 over agents            :    3.18893
Std. of action_0 over envs              :    3.18408
Std. of action_0 over time              :    3.18777
Std. of action_1 over agents            :    3.12208
Std. of action_1 over envs              :    3.11801
Std. of action_1 over time              :    3.12358
Current timestep                        : 7600000.00000
Gradient norm                           :    0.02734
Mean episodic reward                    : -535.45400
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.49228
Policy loss                             :   -1.60367
Value function loss                     :  127.20393
Mean rewards                            :    0.58680
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   23.82341
Mean advantages                         :   -0.46620
Mean (norm.) advantages                 :   -0.46620
Mean (discounted) returns               :   23.35721
Mean normalized returns                 :   23.35721
Mean entropy                            :    3.21292
Variance explained by the value function:    0.28449
Std. of action_0 over agents            :    2.70679
Std. of action_0 over envs              :    2.89904
Std. of action_0 over time              :    2.94759
Std. of action_1 over agents            :    1.79010
Std. of action_1 over envs              :    1.92182
Std. of action_1 over time              :    1.94822
Current timestep                        : 7600000.00000
Gradient norm                           :    0.91335
Mean episodic reward                    :  561.52000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 760 / 1000
Mean training time per iter (ms)        :     131.50
Mean steps per sec (training time)      :   76045.84


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.48779
Policy loss                             :   -0.29094
Value function loss                     :    4.14791
Mean rewards                            :   -0.02790
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.15647
Mean advantages                         :   -0.06015
Mean (norm.) advantages                 :   -0.06015
Mean (discounted) returns               :   -1.21662
Mean normalized returns                 :   -1.21662
Mean entropy                            :    4.76647
Variance explained by the value function:    0.16544
Std. of action_0 over agents            :    3.17417
Std. of action_0 over envs              :    3.16971
Std. of action_0 over time              :    3.17336
Std. of action_1 over agents            :    3.14666
Std. of action_1 over envs              :    3.14271
Std. of action_1 over time              :    3.14805
Current timestep                        : 7700000.00000
Gradient norm                           :    0.02122
Mean episodic reward                    : -539.06000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    7.92600
Policy loss                             :    6.82271
Value function loss                     :  127.35061
Mean rewards                            :    0.58140
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   21.23464
Mean advantages                         :    2.02077
Mean (norm.) advantages                 :    2.02077
Mean (discounted) returns               :   23.25541
Mean normalized returns                 :   23.25541
Mean entropy                            :    3.40440
Variance explained by the value function:    0.23544
Std. of action_0 over agents            :    2.59771
Std. of action_0 over envs              :    2.80697
Std. of action_0 over time              :    2.85103
Std. of action_1 over agents            :    2.06664
Std. of action_1 over envs              :    2.21571
Std. of action_1 over time              :    2.23328
Current timestep                        : 7700000.00000
Gradient norm                           :    1.12072
Mean episodic reward                    :  565.10000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 770 / 1000
Mean training time per iter (ms)        :     131.51
Mean steps per sec (training time)      :   76041.69


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.45866
Policy loss                             :   -0.26265
Value function loss                     :    4.19262
Mean rewards                            :   -0.02856
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.17547
Mean advantages                         :   -0.05442
Mean (norm.) advantages                 :   -0.05442
Mean (discounted) returns               :   -1.22989
Mean normalized returns                 :   -1.22989
Mean entropy                            :    4.75884
Variance explained by the value function:    0.17298
Std. of action_0 over agents            :    3.17663
Std. of action_0 over envs              :    3.17194
Std. of action_0 over time              :    3.17501
Std. of action_1 over agents            :    3.13906
Std. of action_1 over envs              :    3.13573
Std. of action_1 over time              :    3.14113
Current timestep                        : 7800000.00000
Gradient norm                           :    0.02224
Mean episodic reward                    : -559.48400
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    3.26426
Policy loss                             :    2.20624
Value function loss                     :  122.67312
Mean rewards                            :    0.59340
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   23.28918
Mean advantages                         :    0.71340
Mean (norm.) advantages                 :    0.71340
Mean (discounted) returns               :   24.00257
Mean normalized returns                 :   24.00257
Mean entropy                            :    3.37422
Variance explained by the value function:    0.27916
Std. of action_0 over agents            :    3.19995
Std. of action_0 over envs              :    3.38429
Std. of action_0 over time              :    3.40444
Std. of action_1 over agents            :    2.05925
Std. of action_1 over envs              :    2.17518
Std. of action_1 over time              :    2.18703
Current timestep                        : 7800000.00000
Gradient norm                           :    1.05449
Mean episodic reward                    :  583.20000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 780 / 1000
Mean training time per iter (ms)        :     131.52
Mean steps per sec (training time)      :   76035.50


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.41926
Policy loss                             :   -0.22322
Value function loss                     :    4.18300
Mean rewards                            :   -0.02820
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.16410
Mean advantages                         :   -0.04672
Mean (norm.) advantages                 :   -0.04672
Mean (discounted) returns               :   -1.21083
Mean normalized returns                 :   -1.21083
Mean entropy                            :    4.75737
Variance explained by the value function:    0.16522
Std. of action_0 over agents            :    3.17312
Std. of action_0 over envs              :    3.16929
Std. of action_0 over time              :    3.17282
Std. of action_1 over agents            :    3.13632
Std. of action_1 over envs              :    3.13291
Std. of action_1 over time              :    3.13719
Current timestep                        : 7900000.00000
Gradient norm                           :    0.02719
Mean episodic reward                    : -558.58000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    3.08698
Policy loss                             :    2.03170
Value function loss                     :  123.22449
Mean rewards                            :    0.58680
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   23.02211
Mean advantages                         :    0.57950
Mean (norm.) advantages                 :    0.57950
Mean (discounted) returns               :   23.60161
Mean normalized returns                 :   23.60161
Mean entropy                            :    3.53928
Variance explained by the value function:    0.26366
Std. of action_0 over agents            :    3.07204
Std. of action_0 over envs              :    3.31728
Std. of action_0 over time              :    3.37429
Std. of action_1 over agents            :    1.98237
Std. of action_1 over envs              :    2.11455
Std. of action_1 over time              :    2.13927
Current timestep                        : 7900000.00000
Gradient norm                           :    1.00784
Mean episodic reward                    :  582.08000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 790 / 1000
Mean training time per iter (ms)        :     131.51
Mean steps per sec (training time)      :   76040.35


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.30213
Policy loss                             :   -0.10642
Value function loss                     :    4.15044
Mean rewards                            :   -0.02711
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.17433
Mean advantages                         :   -0.02023
Mean (norm.) advantages                 :   -0.02023
Mean (discounted) returns               :   -1.19456
Mean normalized returns                 :   -1.19456
Mean entropy                            :    4.74421
Variance explained by the value function:    0.15254
Std. of action_0 over agents            :    3.20932
Std. of action_0 over envs              :    3.20477
Std. of action_0 over time              :    3.20619
Std. of action_1 over agents            :    3.15346
Std. of action_1 over envs              :    3.14944
Std. of action_1 over time              :    3.15497
Current timestep                        : 8000000.00000
Gradient norm                           :    0.04136
Mean episodic reward                    : -546.62600
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    0.69346
Policy loss                             :   -0.29057
Value function loss                     :  116.33506
Mean rewards                            :    0.56920
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   23.33390
Mean advantages                         :   -0.11530
Mean (norm.) advantages                 :   -0.11530
Mean (discounted) returns               :   23.21861
Mean normalized returns                 :   23.21861
Mean entropy                            :    3.58649
Variance explained by the value function:    0.23829
Std. of action_0 over agents            :    2.38557
Std. of action_0 over envs              :    2.59163
Std. of action_0 over time              :    2.66794
Std. of action_1 over agents            :    2.56035
Std. of action_1 over envs              :    2.66890
Std. of action_1 over time              :    2.67845
Current timestep                        : 8000000.00000
Gradient norm                           :    1.11066
Mean episodic reward                    :  571.22000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 800 / 1000
Mean training time per iter (ms)        :     131.50
Mean steps per sec (training time)      :   76046.89


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    0.16496
Policy loss                             :    0.36233
Value function loss                     :    4.00833
Mean rewards                            :   -0.02715
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.27469
Mean advantages                         :    0.07587
Mean (norm.) advantages                 :    0.07587
Mean (discounted) returns               :   -1.19882
Mean normalized returns                 :   -1.19882
Mean entropy                            :    4.74901
Variance explained by the value function:    0.17956
Std. of action_0 over agents            :    3.19388
Std. of action_0 over envs              :    3.18879
Std. of action_0 over time              :    3.19184
Std. of action_1 over agents            :    3.17218
Std. of action_1 over envs              :    3.16777
Std. of action_1 over time              :    3.17340
Current timestep                        : 8100000.00000
Gradient norm                           :    0.04113
Mean episodic reward                    : -541.02800
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    0.09909
Policy loss                             :   -0.92593
Value function loss                     :  119.10222
Mean rewards                            :    0.56820
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   23.41670
Mean advantages                         :   -0.36695
Mean (norm.) advantages                 :   -0.36695
Mean (discounted) returns               :   23.04976
Mean normalized returns                 :   23.04976
Mean entropy                            :    3.32007
Variance explained by the value function:    0.27340
Std. of action_0 over agents            :    2.27286
Std. of action_0 over envs              :    2.45923
Std. of action_0 over time              :    2.54284
Std. of action_1 over agents            :    1.76278
Std. of action_1 over envs              :    1.89776
Std. of action_1 over time              :    1.92484
Current timestep                        : 8100000.00000
Gradient norm                           :    1.09730
Mean episodic reward                    :  566.34000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 810 / 1000
Mean training time per iter (ms)        :     131.49
Mean steps per sec (training time)      :   76050.27


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.18113
Policy loss                             :    0.01574
Value function loss                     :    4.08524
Mean rewards                            :   -0.02647
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.18495
Mean advantages                         :    0.00469
Mean (norm.) advantages                 :    0.00469
Mean (discounted) returns               :   -1.18026
Mean normalized returns                 :   -1.18026
Mean entropy                            :    4.75449
Variance explained by the value function:    0.14952
Std. of action_0 over agents            :    3.16292
Std. of action_0 over envs              :    3.15804
Std. of action_0 over time              :    3.16273
Std. of action_1 over agents            :    3.16270
Std. of action_1 over envs              :    3.15975
Std. of action_1 over time              :    3.16472
Current timestep                        : 8200000.00000
Gradient norm                           :    0.05318
Mean episodic reward                    : -553.83400
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -3.44440
Policy loss                             :   -4.39817
Value function loss                     :  110.89462
Mean rewards                            :    0.55520
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   24.62424
Mean advantages                         :   -1.54790
Mean (norm.) advantages                 :   -1.54790
Mean (discounted) returns               :   23.07634
Mean normalized returns                 :   23.07634
Mean entropy                            :    3.10346
Variance explained by the value function:    0.21372
Std. of action_0 over agents            :    1.90018
Std. of action_0 over envs              :    2.07247
Std. of action_0 over time              :    2.11348
Std. of action_1 over agents            :    2.92395
Std. of action_1 over envs              :    3.06655
Std. of action_1 over time              :    3.11144
Current timestep                        : 8200000.00000
Gradient norm                           :    1.11373
Mean episodic reward                    :  577.52000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 820 / 1000
Mean training time per iter (ms)        :     131.50
Mean steps per sec (training time)      :   76043.32


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.54713
Policy loss                             :   -0.35098
Value function loss                     :    4.16105
Mean rewards                            :   -0.02791
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.13524
Mean advantages                         :   -0.07293
Mean (norm.) advantages                 :   -0.07293
Mean (discounted) returns               :   -1.20816
Mean normalized returns                 :   -1.20816
Mean entropy                            :    4.75520
Variance explained by the value function:    0.17070
Std. of action_0 over agents            :    3.18017
Std. of action_0 over envs              :    3.17518
Std. of action_0 over time              :    3.17946
Std. of action_1 over agents            :    3.16972
Std. of action_1 over envs              :    3.16651
Std. of action_1 over time              :    3.17093
Current timestep                        : 8300000.00000
Gradient norm                           :    0.02979
Mean episodic reward                    : -531.13600
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    1.88991
Policy loss                             :    0.69926
Value function loss                     :  136.24913
Mean rewards                            :    0.57500
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   22.94963
Mean advantages                         :    0.26201
Mean (norm.) advantages                 :    0.26201
Mean (discounted) returns               :   23.21163
Mean normalized returns                 :   23.21163
Mean entropy                            :    3.43683
Variance explained by the value function:    0.27667
Std. of action_0 over agents            :    2.21936
Std. of action_0 over envs              :    2.43651
Std. of action_0 over time              :    2.47854
Std. of action_1 over agents            :    3.44087
Std. of action_1 over envs              :    3.56183
Std. of action_1 over time              :    3.57987
Current timestep                        : 8300000.00000
Gradient norm                           :    1.15732
Mean episodic reward                    :  557.46000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 830 / 1000
Mean training time per iter (ms)        :     131.51
Mean steps per sec (training time)      :   76040.70


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.25760
Policy loss                             :   -0.06048
Value function loss                     :    4.00246
Mean rewards                            :   -0.02635
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.16154
Mean advantages                         :   -0.01203
Mean (norm.) advantages                 :   -0.01203
Mean (discounted) returns               :   -1.17357
Mean normalized returns                 :   -1.17357
Mean entropy                            :    4.74290
Variance explained by the value function:    0.16384
Std. of action_0 over agents            :    3.17982
Std. of action_0 over envs              :    3.17352
Std. of action_0 over time              :    3.17499
Std. of action_1 over agents            :    3.14941
Std. of action_1 over envs              :    3.14560
Std. of action_1 over time              :    3.15093
Current timestep                        : 8400000.00000
Gradient norm                           :    0.02441
Mean episodic reward                    : -526.12800
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    8.96353
Policy loss                             :    7.83867
Value function loss                     :  129.46356
Mean rewards                            :    0.55260
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   19.66001
Mean advantages                         :    2.29976
Mean (norm.) advantages                 :    2.29976
Mean (discounted) returns               :   21.95976
Mean normalized returns                 :   21.95976
Mean entropy                            :    3.39555
Variance explained by the value function:    0.23797
Std. of action_0 over agents            :    2.23432
Std. of action_0 over envs              :    2.43526
Std. of action_0 over time              :    2.49098
Std. of action_1 over agents            :    2.92587
Std. of action_1 over envs              :    3.10808
Std. of action_1 over time              :    3.12148
Current timestep                        : 8400000.00000
Gradient norm                           :    1.17941
Mean episodic reward                    :  552.88000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 840 / 1000
Mean training time per iter (ms)        :     131.51
Mean steps per sec (training time)      :   76042.39


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.55394
Policy loss                             :   -0.35504
Value function loss                     :    3.90331
Mean rewards                            :   -0.02618
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.06174
Mean advantages                         :   -0.07353
Mean (norm.) advantages                 :   -0.07353
Mean (discounted) returns               :   -1.13527
Mean normalized returns                 :   -1.13527
Mean entropy                            :    4.75863
Variance explained by the value function:    0.17935
Std. of action_0 over agents            :    3.15988
Std. of action_0 over envs              :    3.15444
Std. of action_0 over time              :    3.15931
Std. of action_1 over agents            :    3.14429
Std. of action_1 over envs              :    3.14045
Std. of action_1 over time              :    3.14564
Current timestep                        : 8500000.00000
Gradient norm                           :    0.03652
Mean episodic reward                    : -532.17600
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -5.34770
Policy loss                             :   -6.53455
Value function loss                     :  136.23773
Mean rewards                            :    0.54840
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   24.07002
Mean advantages                         :   -1.92602
Mean (norm.) advantages                 :   -1.92602
Mean (discounted) returns               :   22.14400
Mean normalized returns                 :   22.14400
Mean entropy                            :    3.51055
Variance explained by the value function:    0.22309
Std. of action_0 over agents            :    2.53538
Std. of action_0 over envs              :    2.72877
Std. of action_0 over time              :    2.77942
Std. of action_1 over agents            :    3.29677
Std. of action_1 over envs              :    3.46886
Std. of action_1 over time              :    3.47977
Current timestep                        : 8500000.00000
Gradient norm                           :    1.19676
Mean episodic reward                    :  557.78000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 850 / 1000
Mean training time per iter (ms)        :     131.50
Mean steps per sec (training time)      :   76045.86


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.14986
Policy loss                             :    0.04689
Value function loss                     :    4.17001
Mean rewards                            :   -0.02834
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.23012
Mean advantages                         :    0.01202
Mean (norm.) advantages                 :    0.01202
Mean (discounted) returns               :   -1.21811
Mean normalized returns                 :   -1.21811
Mean entropy                            :    4.76905
Variance explained by the value function:    0.16626
Std. of action_0 over agents            :    3.21452
Std. of action_0 over envs              :    3.20996
Std. of action_0 over time              :    3.21457
Std. of action_1 over agents            :    3.15778
Std. of action_1 over envs              :    3.15413
Std. of action_1 over time              :    3.15927
Current timestep                        : 8600000.00000
Gradient norm                           :    0.03752
Mean episodic reward                    : -569.90600
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -6.14388
Policy loss                             :   -7.34946
Value function loss                     :  136.72488
Mean rewards                            :    0.59260
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   26.56596
Mean advantages                         :   -2.38842
Mean (norm.) advantages                 :   -2.38842
Mean (discounted) returns               :   24.17754
Mean normalized returns                 :   24.17754
Mean entropy                            :    3.23335
Variance explained by the value function:    0.22871
Std. of action_0 over agents            :    2.42495
Std. of action_0 over envs              :    2.63417
Std. of action_0 over time              :    2.70486
Std. of action_1 over agents            :    3.07826
Std. of action_1 over envs              :    3.21062
Std. of action_1 over time              :    3.23284
Current timestep                        : 8600000.00000
Gradient norm                           :    1.08104
Mean episodic reward                    :  590.64000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 860 / 1000
Mean training time per iter (ms)        :     131.52
Mean steps per sec (training time)      :   76031.23


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.50445
Policy loss                             :   -0.30974
Value function loss                     :    4.26361
Mean rewards                            :   -0.02970
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.20826
Mean advantages                         :   -0.06295
Mean (norm.) advantages                 :   -0.06295
Mean (discounted) returns               :   -1.27120
Mean normalized returns                 :   -1.27120
Mean entropy                            :    4.74693
Variance explained by the value function:    0.18125
Std. of action_0 over agents            :    3.21300
Std. of action_0 over envs              :    3.20802
Std. of action_0 over time              :    3.21187
Std. of action_1 over agents            :    3.17699
Std. of action_1 over envs              :    3.17196
Std. of action_1 over time              :    3.17810
Current timestep                        : 8700000.00000
Gradient norm                           :    0.03724
Mean episodic reward                    : -557.93800
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   11.62696
Policy loss                             :   10.38947
Value function loss                     :  140.95999
Mean rewards                            :    0.61020
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   21.09140
Mean advantages                         :    3.08616
Mean (norm.) advantages                 :    3.08616
Mean (discounted) returns               :   24.17756
Mean normalized returns                 :   24.17756
Mean entropy                            :    3.44212
Variance explained by the value function:    0.27942
Std. of action_0 over agents            :    2.64000
Std. of action_0 over envs              :    2.85386
Std. of action_0 over time              :    2.91016
Std. of action_1 over agents            :    2.54975
Std. of action_1 over envs              :    2.75341
Std. of action_1 over time              :    2.77565
Current timestep                        : 8700000.00000
Gradient norm                           :    1.13154
Mean episodic reward                    :  579.78000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 870 / 1000
Mean training time per iter (ms)        :     131.51
Mean steps per sec (training time)      :   76037.99


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -1.18635
Policy loss                             :   -0.99188
Value function loss                     :    4.25193
Mean rewards                            :   -0.02911
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -0.99437
Mean advantages                         :   -0.21242
Mean (norm.) advantages                 :   -0.21242
Mean (discounted) returns               :   -1.20678
Mean normalized returns                 :   -1.20678
Mean entropy                            :    4.73981
Variance explained by the value function:    0.18426
Std. of action_0 over agents            :    3.16924
Std. of action_0 over envs              :    3.16311
Std. of action_0 over time              :    3.16743
Std. of action_1 over agents            :    3.16646
Std. of action_1 over envs              :    3.16223
Std. of action_1 over time              :    3.16647
Current timestep                        : 8800000.00000
Gradient norm                           :    0.03852
Mean episodic reward                    : -537.97600
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    8.07780
Policy loss                             :    6.92204
Value function loss                     :  131.69608
Mean rewards                            :    0.60440
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   21.59939
Mean advantages                         :    2.28520
Mean (norm.) advantages                 :    2.28520
Mean (discounted) returns               :   23.88459
Mean normalized returns                 :   23.88459
Mean entropy                            :    3.22398
Variance explained by the value function:    0.31692
Std. of action_0 over agents            :    2.79544
Std. of action_0 over envs              :    3.00661
Std. of action_0 over time              :    3.04387
Std. of action_1 over agents            :    2.94543
Std. of action_1 over envs              :    3.11991
Std. of action_1 over time              :    3.14387
Current timestep                        : 8800000.00000
Gradient norm                           :    1.11211
Mean episodic reward                    :  564.10000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 880 / 1000
Mean training time per iter (ms)        :     131.49
Mean steps per sec (training time)      :   76052.14


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.48378
Policy loss                             :   -0.28822
Value function loss                     :    4.29793
Mean rewards                            :   -0.02909
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.18003
Mean advantages                         :   -0.05871
Mean (norm.) advantages                 :   -0.05871
Mean (discounted) returns               :   -1.23874
Mean normalized returns                 :   -1.23874
Mean entropy                            :    4.77072
Variance explained by the value function:    0.16179
Std. of action_0 over agents            :    3.14665
Std. of action_0 over envs              :    3.14209
Std. of action_0 over time              :    3.14798
Std. of action_1 over agents            :    3.13769
Std. of action_1 over envs              :    3.13413
Std. of action_1 over time              :    3.13955
Current timestep                        : 8900000.00000
Gradient norm                           :    0.03870
Mean episodic reward                    : -587.85400
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -3.38747
Policy loss                             :   -4.39543
Value function loss                     :  115.76301
Mean rewards                            :    0.60300
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   26.36319
Mean advantages                         :   -1.55360
Mean (norm.) advantages                 :   -1.55360
Mean (discounted) returns               :   24.80959
Mean normalized returns                 :   24.80959
Mean entropy                            :    2.99344
Variance explained by the value function:    0.24313
Std. of action_0 over agents            :    2.48637
Std. of action_0 over envs              :    2.68999
Std. of action_0 over time              :    2.75907
Std. of action_1 over agents            :    2.60556
Std. of action_1 over envs              :    2.81442
Std. of action_1 over time              :    2.85503
Current timestep                        : 8900000.00000
Gradient norm                           :    1.17420
Mean episodic reward                    :  606.92000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 890 / 1000
Mean training time per iter (ms)        :     131.47
Mean steps per sec (training time)      :   76061.94


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.36912
Policy loss                             :   -0.17359
Value function loss                     :    4.14300
Mean rewards                            :   -0.02975
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.22767
Mean advantages                         :   -0.03739
Mean (norm.) advantages                 :   -0.03739
Mean (discounted) returns               :   -1.26506
Mean normalized returns                 :   -1.26506
Mean entropy                            :    4.73914
Variance explained by the value function:    0.20622
Std. of action_0 over agents            :    3.20277
Std. of action_0 over envs              :    3.19774
Std. of action_0 over time              :    3.19983
Std. of action_1 over agents            :    3.17064
Std. of action_1 over envs              :    3.16686
Std. of action_1 over time              :    3.17190
Current timestep                        : 9000000.00000
Gradient norm                           :    0.02715
Mean episodic reward                    : -584.72800
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.03105
Policy loss                             :   -1.10422
Value function loss                     :  122.57107
Mean rewards                            :    0.61460
Max. rewards                            :   30.00000
Min. rewards                            :    0.00000
Mean value function                     :   25.14753
Mean advantages                         :   -0.43192
Mean (norm.) advantages                 :   -0.43192
Mean (discounted) returns               :   24.71561
Mean normalized returns                 :   24.71561
Mean entropy                            :    3.05089
Variance explained by the value function:    0.31449
Std. of action_0 over agents            :    2.89194
Std. of action_0 over envs              :    3.08357
Std. of action_0 over time              :    3.13042
Std. of action_1 over agents            :    3.30961
Std. of action_1 over envs              :    3.47596
Std. of action_1 over time              :    3.51229
Current timestep                        : 9000000.00000
Gradient norm                           :    1.06997
Mean episodic reward                    :  604.72000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 900 / 1000
Mean training time per iter (ms)        :     131.46
Mean steps per sec (training time)      :   76070.81


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.14291
Policy loss                             :    0.05394
Value function loss                     :    4.06345
Mean rewards                            :   -0.02845
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.23909
Mean advantages                         :    0.01223
Mean (norm.) advantages                 :    0.01223
Mean (discounted) returns               :   -1.22685
Mean normalized returns                 :   -1.22685
Mean entropy                            :    4.74955
Variance explained by the value function:    0.19480
Std. of action_0 over agents            :    3.17530
Std. of action_0 over envs              :    3.16798
Std. of action_0 over time              :    3.17234
Std. of action_1 over agents            :    3.12031
Std. of action_1 over envs              :    3.11724
Std. of action_1 over time              :    3.12251
Current timestep                        : 9100000.00000
Gradient norm                           :    0.02538
Mean episodic reward                    : -566.91800
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    3.05968
Policy loss                             :    2.11430
Value function loss                     :  110.66602
Mean rewards                            :    0.59020
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   23.04869
Mean advantages                         :    0.63539
Mean (norm.) advantages                 :    0.63539
Mean (discounted) returns               :   23.68408
Mean normalized returns                 :   23.68408
Mean entropy                            :    3.22544
Variance explained by the value function:    0.30168
Std. of action_0 over agents            :    3.18260
Std. of action_0 over envs              :    3.37691
Std. of action_0 over time              :    3.42207
Std. of action_1 over agents            :    3.68669
Std. of action_1 over envs              :    3.82059
Std. of action_1 over time              :    3.84825
Current timestep                        : 9100000.00000
Gradient norm                           :    1.06569
Mean episodic reward                    :  589.60000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 910 / 1000
Mean training time per iter (ms)        :     131.45
Mean steps per sec (training time)      :   76072.72


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.05486
Policy loss                             :    0.14089
Value function loss                     :    4.18044
Mean rewards                            :   -0.02834
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.24300
Mean advantages                         :    0.03174
Mean (norm.) advantages                 :    0.03174
Mean (discounted) returns               :   -1.21126
Mean normalized returns                 :   -1.21126
Mean entropy                            :    4.75115
Variance explained by the value function:    0.16665
Std. of action_0 over agents            :    3.15765
Std. of action_0 over envs              :    3.15171
Std. of action_0 over time              :    3.15776
Std. of action_1 over agents            :    3.10956
Std. of action_1 over envs              :    3.10617
Std. of action_1 over time              :    3.11089
Current timestep                        : 9200000.00000
Gradient norm                           :    0.03981
Mean episodic reward                    : -572.90000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    1.20769
Policy loss                             :    0.15536
Value function loss                     :  121.40173
Mean rewards                            :    0.58820
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   23.63122
Mean advantages                         :    0.00830
Mean (norm.) advantages                 :    0.00830
Mean (discounted) returns               :   23.63952
Mean normalized returns                 :   23.63952
Mean entropy                            :    3.23377
Variance explained by the value function:    0.23743
Std. of action_0 over agents            :    2.58762
Std. of action_0 over envs              :    2.77729
Std. of action_0 over time              :    2.80728
Std. of action_1 over agents            :    3.64288
Std. of action_1 over envs              :    3.82928
Std. of action_1 over time              :    3.86244
Current timestep                        : 9200000.00000
Gradient norm                           :    1.04532
Mean episodic reward                    :  592.98000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 920 / 1000
Mean training time per iter (ms)        :     131.46
Mean steps per sec (training time)      :   76069.51


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.27581
Policy loss                             :   -0.07884
Value function loss                     :    4.04371
Mean rewards                            :   -0.02736
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.17853
Mean advantages                         :   -0.01491
Mean (norm.) advantages                 :   -0.01491
Mean (discounted) returns               :   -1.19344
Mean normalized returns                 :   -1.19344
Mean entropy                            :    4.74810
Variance explained by the value function:    0.17657
Std. of action_0 over agents            :    3.16767
Std. of action_0 over envs              :    3.16128
Std. of action_0 over time              :    3.16500
Std. of action_1 over agents            :    3.12557
Std. of action_1 over envs              :    3.12226
Std. of action_1 over time              :    3.12740
Current timestep                        : 9300000.00000
Gradient norm                           :    0.03736
Mean episodic reward                    : -573.03400
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -1.60792
Policy loss                             :   -2.63305
Value function loss                     :  119.45473
Mean rewards                            :    0.57280
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   24.32656
Mean advantages                         :   -0.82561
Mean (norm.) advantages                 :   -0.82561
Mean (discounted) returns               :   23.50094
Mean normalized returns                 :   23.50094
Mean entropy                            :    3.38838
Variance explained by the value function:    0.22539
Std. of action_0 over agents            :    2.50192
Std. of action_0 over envs              :    2.70398
Std. of action_0 over time              :    2.72757
Std. of action_1 over agents            :    3.30610
Std. of action_1 over envs              :    3.43749
Std. of action_1 over time              :    3.46206
Current timestep                        : 9300000.00000
Gradient norm                           :    1.13402
Mean episodic reward                    :  594.66000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 930 / 1000
Mean training time per iter (ms)        :     131.43
Mean steps per sec (training time)      :   76083.31


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.49547
Policy loss                             :   -0.30134
Value function loss                     :    4.25694
Mean rewards                            :   -0.02857
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.19271
Mean advantages                         :   -0.06326
Mean (norm.) advantages                 :   -0.06326
Mean (discounted) returns               :   -1.25597
Mean normalized returns                 :   -1.25597
Mean entropy                            :    4.73399
Variance explained by the value function:    0.16531
Std. of action_0 over agents            :    3.22218
Std. of action_0 over envs              :    3.21744
Std. of action_0 over time              :    3.22140
Std. of action_1 over agents            :    3.19984
Std. of action_1 over envs              :    3.19650
Std. of action_1 over time              :    3.20160
Current timestep                        : 9400000.00000
Gradient norm                           :    0.03201
Mean episodic reward                    : -554.25600
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -1.66757
Policy loss                             :   -2.83409
Value function loss                     :  133.35826
Mean rewards                            :    0.59520
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   26.13985
Mean advantages                         :   -0.88884
Mean (norm.) advantages                 :   -0.88884
Mean (discounted) returns               :   25.25101
Mean normalized returns                 :   25.25101
Mean entropy                            :    3.34109
Variance explained by the value function:    0.17955
Std. of action_0 over agents            :    2.55535
Std. of action_0 over envs              :    2.76395
Std. of action_0 over time              :    2.79454
Std. of action_1 over agents            :    2.09338
Std. of action_1 over envs              :    2.25748
Std. of action_1 over time              :    2.27124
Current timestep                        : 9400000.00000
Gradient norm                           :    1.04642
Mean episodic reward                    :  577.54000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 940 / 1000
Mean training time per iter (ms)        :     131.41
Mean steps per sec (training time)      :   76098.14


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.20821
Policy loss                             :   -0.01201
Value function loss                     :    4.15328
Mean rewards                            :   -0.02957
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.27709
Mean advantages                         :   -0.00364
Mean (norm.) advantages                 :   -0.00364
Mean (discounted) returns               :   -1.28073
Mean normalized returns                 :   -1.28073
Mean entropy                            :    4.75468
Variance explained by the value function:    0.20202
Std. of action_0 over agents            :    3.19947
Std. of action_0 over envs              :    3.19537
Std. of action_0 over time              :    3.19812
Std. of action_1 over agents            :    3.17769
Std. of action_1 over envs              :    3.17489
Std. of action_1 over time              :    3.17932
Current timestep                        : 9500000.00000
Gradient norm                           :    0.03554
Mean episodic reward                    : -567.83200
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    5.07376
Policy loss                             :    3.97449
Value function loss                     :  126.70538
Mean rewards                            :    0.61200
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   23.69209
Mean advantages                         :    1.29387
Mean (norm.) advantages                 :    1.29387
Mean (discounted) returns               :   24.98596
Mean normalized returns                 :   24.98596
Mean entropy                            :    3.35566
Variance explained by the value function:    0.27646
Std. of action_0 over agents            :    2.34827
Std. of action_0 over envs              :    2.56406
Std. of action_0 over time              :    2.60320
Std. of action_1 over agents            :    2.00955
Std. of action_1 over envs              :    2.09320
Std. of action_1 over time              :    2.10885
Current timestep                        : 9500000.00000
Gradient norm                           :    1.15305
Mean episodic reward                    :  590.20000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 950 / 1000
Mean training time per iter (ms)        :     131.38
Mean steps per sec (training time)      :   76114.85


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.34081
Policy loss                             :   -0.14562
Value function loss                     :    4.22721
Mean rewards                            :   -0.02945
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.25034
Mean advantages                         :   -0.02811
Mean (norm.) advantages                 :   -0.02811
Mean (discounted) returns               :   -1.27845
Mean normalized returns                 :   -1.27845
Mean entropy                            :    4.74924
Variance explained by the value function:    0.18769
Std. of action_0 over agents            :    3.18969
Std. of action_0 over envs              :    3.18522
Std. of action_0 over time              :    3.18722
Std. of action_1 over agents            :    3.14180
Std. of action_1 over envs              :    3.13875
Std. of action_1 over time              :    3.14342
Current timestep                        : 9600000.00000
Gradient norm                           :    0.03068
Mean episodic reward                    : -594.98200
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    4.25302
Policy loss                             :    3.19632
Value function loss                     :  119.53486
Mean rewards                            :    0.61180
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   23.50979
Mean advantages                         :    1.31273
Mean (norm.) advantages                 :    1.31273
Mean (discounted) returns               :   24.82252
Mean normalized returns                 :   24.82252
Mean entropy                            :    2.77310
Variance explained by the value function:    0.30261
Std. of action_0 over agents            :    2.26174
Std. of action_0 over envs              :    2.46423
Std. of action_0 over time              :    2.51926
Std. of action_1 over agents            :    1.68436
Std. of action_1 over envs              :    1.78463
Std. of action_1 over time              :    1.80277
Current timestep                        : 9600000.00000
Gradient norm                           :    1.10563
Mean episodic reward                    :  614.36000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 960 / 1000
Mean training time per iter (ms)        :     131.35
Mean steps per sec (training time)      :   76131.35


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.00258
Policy loss                             :    0.19290
Value function loss                     :    4.13975
Mean rewards                            :   -0.02909
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.29192
Mean advantages                         :    0.04022
Mean (norm.) advantages                 :    0.04022
Mean (discounted) returns               :   -1.25170
Mean normalized returns                 :   -1.25170
Mean entropy                            :    4.73767
Variance explained by the value function:    0.18618
Std. of action_0 over agents            :    3.19830
Std. of action_0 over envs              :    3.19339
Std. of action_0 over time              :    3.19447
Std. of action_1 over agents            :    3.12764
Std. of action_1 over envs              :    3.12300
Std. of action_1 over time              :    3.12900
Current timestep                        : 9700000.00000
Gradient norm                           :    0.02691
Mean episodic reward                    : -588.79600
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.81261
Policy loss                             :   -1.72055
Value function loss                     :  105.96576
Mean rewards                            :    0.60140
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   24.60137
Mean advantages                         :   -0.64857
Mean (norm.) advantages                 :   -0.64857
Mean (discounted) returns               :   23.95280
Mean normalized returns                 :   23.95280
Mean entropy                            :    3.03429
Variance explained by the value function:    0.32590
Std. of action_0 over agents            :    2.56005
Std. of action_0 over envs              :    2.81306
Std. of action_0 over time              :    2.92726
Std. of action_1 over agents            :    1.82189
Std. of action_1 over envs              :    1.92473
Std. of action_1 over time              :    1.95218
Current timestep                        : 9700000.00000
Gradient norm                           :    1.12529
Mean episodic reward                    :  609.00000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 970 / 1000
Mean training time per iter (ms)        :     131.35
Mean steps per sec (training time)      :   76132.67


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.09292
Policy loss                             :    0.10299
Value function loss                     :    4.16516
Mean rewards                            :   -0.02936
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.29036
Mean advantages                         :    0.02345
Mean (norm.) advantages                 :    0.02345
Mean (discounted) returns               :   -1.26691
Mean normalized returns                 :   -1.26691
Mean entropy                            :    4.75133
Variance explained by the value function:    0.18638
Std. of action_0 over agents            :    3.19080
Std. of action_0 over envs              :    3.18545
Std. of action_0 over time              :    3.18853
Std. of action_1 over agents            :    3.11612
Std. of action_1 over envs              :    3.11245
Std. of action_1 over time              :    3.11794
Current timestep                        : 9800000.00000
Gradient norm                           :    0.02722
Mean episodic reward                    : -592.84600
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -2.32977
Policy loss                             :   -3.38606
Value function loss                     :  121.66192
Mean rewards                            :    0.61060
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   25.73337
Mean advantages                         :   -1.20760
Mean (norm.) advantages                 :   -1.20760
Mean (discounted) returns               :   24.52577
Mean normalized returns                 :   24.52577
Mean entropy                            :    3.20664
Variance explained by the value function:    0.28089
Std. of action_0 over agents            :    2.48739
Std. of action_0 over envs              :    2.74256
Std. of action_0 over time              :    2.84721
Std. of action_1 over agents            :    2.02462
Std. of action_1 over envs              :    2.14198
Std. of action_1 over time              :    2.17311
Current timestep                        : 9800000.00000
Gradient norm                           :    1.17561
Mean episodic reward                    :  612.60000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 980 / 1000
Mean training time per iter (ms)        :     131.33
Mean steps per sec (training time)      :   76145.46


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.44418
Policy loss                             :   -0.24911
Value function loss                     :    4.19554
Mean rewards                            :   -0.02911
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.19526
Mean advantages                         :   -0.05146
Mean (norm.) advantages                 :   -0.05146
Mean (discounted) returns               :   -1.24672
Mean normalized returns                 :   -1.24672
Mean entropy                            :    4.74056
Variance explained by the value function:    0.18070
Std. of action_0 over agents            :    3.17969
Std. of action_0 over envs              :    3.17441
Std. of action_0 over time              :    3.17680
Std. of action_1 over agents            :    3.11503
Std. of action_1 over envs              :    3.11131
Std. of action_1 over time              :    3.11650
Current timestep                        : 9900000.00000
Gradient norm                           :    0.02853
Mean episodic reward                    : -578.76400
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.73092
Policy loss                             :   -1.67921
Value function loss                     :  111.90366
Mean rewards                            :    0.60440
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   24.65270
Mean advantages                         :   -0.47630
Mean (norm.) advantages                 :   -0.47630
Mean (discounted) returns               :   24.17640
Mean normalized returns                 :   24.17640
Mean entropy                            :    3.41493
Variance explained by the value function:    0.32981
Std. of action_0 over agents            :    2.35162
Std. of action_0 over envs              :    2.51744
Std. of action_0 over time              :    2.60429
Std. of action_1 over agents            :    2.05317
Std. of action_1 over envs              :    2.16243
Std. of action_1 over time              :    2.18005
Current timestep                        : 9900000.00000
Gradient norm                           :    1.11308
Mean episodic reward                    :  600.52000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 990 / 1000
Mean training time per iter (ms)        :     131.33
Mean steps per sec (training time)      :   76145.42


========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.52432
Policy loss                             :   -0.32923
Value function loss                     :    4.19812
Mean rewards                            :   -0.02944
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.17564
Mean advantages                         :   -0.07007
Mean (norm.) advantages                 :   -0.07007
Mean (discounted) returns               :   -1.24571
Mean normalized returns                 :   -1.24571
Mean entropy                            :    4.74142
Variance explained by the value function:    0.18246
Std. of action_0 over agents            :    3.19042
Std. of action_0 over envs              :    3.18554
Std. of action_0 over time              :    3.18809
Std. of action_1 over agents            :    3.12981
Std. of action_1 over envs              :    3.12630
Std. of action_1 over time              :    3.13154
Current timestep                        : 9990000.00000
Gradient norm                           :    0.02496
Mean episodic reward                    : -584.57556
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
[Device 0]: Saving the &#39;runner&#39; torch model to the file: &#39;/tmp/continuous_tag/example/1650600997/runner_9990000.state_dict&#39;.
[Device 0]: Saving the &#39;tagger&#39; torch model to the file: &#39;/tmp/continuous_tag/example/1650600997/tagger_9980000.state_dict&#39;.
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    3.98438
Policy loss                             :    3.00639
Value function loss                     :  114.82838
Mean rewards                            :    0.60620
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   23.08975
Mean advantages                         :    0.91406
Mean (norm.) advantages                 :    0.91406
Mean (discounted) returns               :   24.00381
Mean normalized returns                 :   24.00381
Mean entropy                            :    3.40591
Variance explained by the value function:    0.32785
Std. of action_0 over agents            :    2.45406
Std. of action_0 over envs              :    2.63899
Std. of action_0 over time              :    2.73929
Std. of action_1 over agents            :    2.04365
Std. of action_1 over envs              :    2.15399
Std. of action_1 over time              :    2.17432
Current timestep                        : 9990000.00000
Gradient norm                           :    1.14537
Mean episodic reward                    :  604.33333
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
[Device 0]: Saving the &#39;runner&#39; torch model to the file: &#39;/tmp/continuous_tag/example/1650600997/runner_9990000.state_dict&#39;.
[Device 0]: Saving the &#39;tagger&#39; torch model to the file: &#39;/tmp/continuous_tag/example/1650600997/tagger_9990000.state_dict&#39;.
========================================
Metrics for policy &#39;runner&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :   -0.53310
Policy loss                             :   -0.33753
Value function loss                     :    4.15815
Mean rewards                            :   -0.02936
Max. rewards                            :    1.00000
Min. rewards                            :  -10.00000
Mean value function                     :   -1.15203
Mean advantages                         :   -0.07211
Mean (norm.) advantages                 :   -0.07211
Mean (discounted) returns               :   -1.22414
Mean normalized returns                 :   -1.22414
Mean entropy                            :    4.74295
Variance explained by the value function:    0.18542
Std. of action_0 over agents            :    3.18995
Std. of action_0 over envs              :    3.18508
Std. of action_0 over time              :    3.18737
Std. of action_1 over agents            :    3.13307
Std. of action_1 over envs              :    3.12895
Std. of action_1 over time              :    3.13443
Current timestep                        : 10000000.00000
Gradient norm                           :    0.02939
Mean episodic reward                    : -587.28000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Metrics for policy &#39;tagger&#39;
========================================
VF loss coefficient                     :    0.01000
Entropy coefficient                     :    0.05000
Total loss                              :    2.22805
Policy loss                             :    1.22339
Value function loss                     :  117.49338
Mean rewards                            :    0.61080
Max. rewards                            :   20.00000
Min. rewards                            :    0.00000
Mean value function                     :   23.44936
Mean advantages                         :    0.43354
Mean (norm.) advantages                 :    0.43354
Mean (discounted) returns               :   23.88290
Mean normalized returns                 :   23.88290
Mean entropy                            :    3.40551
Variance explained by the value function:    0.32965
Std. of action_0 over agents            :    2.49148
Std. of action_0 over envs              :    2.68992
Std. of action_0 over time              :    2.78799
Std. of action_1 over agents            :    2.06527
Std. of action_1 over envs              :    2.17076
Std. of action_1 over time              :    2.18669
Current timestep                        : 10000000.00000
Gradient norm                           :    1.13207
Mean episodic reward                    :  610.80000
[Device 0]: Saving the results to the file &#39;/tmp/continuous_tag/example/1650600997/results.json&#39;
========================================
Speed performance stats
========================================
Iteration                               : 1000 / 1000
Mean training time per iter (ms)        :     131.34
Mean steps per sec (training time)      :   76137.41


Training is complete!
</pre></div></div>
</div>
</section>
<section id="Visualize-an-episode-rollout-after-training">
<h2>Visualize an episode-rollout after training<a class="headerlink" href="#Visualize-an-episode-rollout-after-training" title="Permalink to this headline">¶</a></h2>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><br/><span></span><span class="c1"># anim = generate_tag_env_rollout_animation(wd_module)</span>
<span class="c1"># HTML(anim.to_html5_video())</span>
</pre></div>
</div>
</div>
<p>Note: In the configuration above, we have set the trainer to only train on <img class="math" src="../../_images/math/ccfd94177ebd9428910e965635541a9d99052c66.png" alt="50000"/> rollout episodes, but you can increase the <code class="docutils literal notranslate"><span class="pre">num_episodes</span></code> configuration parameter to train further. As more training happens, the runners learn to escape the taggers, and the taggers learn to chase after the runner. Sometimes, the taggers also collaborate to team-tag runners. A good number of episodes to train on (for the configuration we have used) is <img class="math" src="../../_images/math/d94997a2318fec7e8e5bc4d8d79bb633675f9411.png" alt="2"/>M or higher.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Finally, close the WarpDrive module to clear up the CUDA memory heap</span>
<span class="n">wd_module</span><span class="o">.</span><span class="n">graceful_close</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[Device 0]: Trainer exits gracefully
</pre></div></div>
</div>
</section>
<section id="Congratulations---Time-to-Join-the-Community!">
<h2>Congratulations - Time to Join the Community!<a class="headerlink" href="#Congratulations---Time-to-Join-the-Community!" title="Permalink to this headline">¶</a></h2>
<p>Congratulations on completing this notebook tutorial! If you enjoyed this and would like to join the Lightning movement, you can do so in the following ways!</p>
<section id="Star-Lightning-on-GitHub">
<h3>Star <a class="reference external" href="https://github.com/PyTorchLightning/pytorch-lightning">Lightning</a> on GitHub<a class="headerlink" href="#Star-Lightning-on-GitHub" title="Permalink to this headline">¶</a></h3>
<p>The easiest way to help our community is just by starring the GitHub repos! This helps raise awareness of the cool tools we’re building.</p>
</section>
<section id="Join-our-Slack!">
<h3>Join our <a class="reference external" href="https://www.pytorchlightning.ai/community">Slack</a>!<a class="headerlink" href="#Join-our-Slack!" title="Permalink to this headline">¶</a></h3>
<p>The best way to keep up to date on the latest advancements is to join our community! Make sure to introduce yourself and share your interests in <code class="docutils literal notranslate"><span class="pre">#general</span></code> channel</p>
</section>
<section id="Contributions-!">
<h3>Contributions !<a class="headerlink" href="#Contributions-!" title="Permalink to this headline">¶</a></h3>
<p>The best way to contribute to our community is to become a code contributor! At any time you can go to <a class="reference external" href="https://github.com/PyTorchLightning/pytorch-lightning">Lightning</a> or <a class="reference external" href="https://github.com/PyTorchLightning/lightning-bolts">Bolt</a> GitHub Issues page and filter for “good first issue”.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/PyTorchLightning/pytorch-lightning/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22">Lightning good first issue</a></p></li>
<li><p><a class="reference external" href="https://github.com/PyTorchLightning/lightning-bolts/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22">Bolt good first issue</a></p></li>
<li><p>You can also contribute your own notebooks with useful examples !</p></li>
</ul>
</section>
<section id="Great-thanks-from-the-entire-Pytorch-Lightning-Team-for-your-interest-!">
<h3>Great thanks from the entire Pytorch Lightning Team for your interest !<a class="headerlink" href="#Great-thanks-from-the-entire-Pytorch-Lightning-Team-for-your-interest-!" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://pytorchlightning.ai"><img alt="Pytorch Lightning" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAByMAAAE8CAYAAABjFR0gAAAACXBIWXMAAAsSAAALEgHS3X78AAAgAElEQVR4nOzdT2ydV3rn+eP3FQLYoVJsZFEgPU4pjYDCAO5QobxLai6dypKKlFQ2Mroj2inImwnEkrwZIl2kqgOtJJWcrGTUlKikoZpFqkr+s0zGZLuzk1xiV23kDiZyKkVNLYKRIkWFALqvB7/X5zBH9+Wfe8lz3r/fD0BI5qXFe+/73vc95zzneZ7nPvvsMwNsZ3F6Y9YYM2sfvmeMWb2wPnlvmx8HAAAAAAAAAAAANhGMRMHi9Ma4MWbBfn2h8APGvGuMWb6wPnmn8AgAAAAAAAAAAABgEYzEMxanN+YVaDTGfGmId+a6ApYX1icfFB4BAAAAAAAAAABA5xGMRG5xeuOIMeaKMaY34jvyUP/fhfXJ5cIjAAAAAAAAAAAA6DSCkR1nS7IqCHlqn+/Ep8aY+Qvrk6uFRwAAAAAAAAAAANBJBCM7bHF6Y3mHvpB7tWaDkve6/v4CAAAAAAAAAAB0HcHIDlqc3pg1xqwM2Rdyr95W70n6SQIAAAAAAAAAAHQXwcgOWZzeOGSDkKP2hdwr9ZNcuLA+udL19x4AAAAAAAAAAKCLCEZ2gO0LqZKsZyp6tes2KEk/SQAAAAAAAAAAgA4hGNlyi9MbCzYQGbIv5F69a4OS9JMEAAAAAAAAAADoAIKRLWX7Ql4xxkzX7BU+tM/rCv0kAQAAAAAAAAAA2o1gZMvYvpAK9h2v+Sv7VBmb9JMEAAAAAAAAAABoL4KRLWH7Qqok61LDXtGaLd16p/AIAAAAAAAAAAAAGo1gZAssTm/M276QX2rwq7lug5KUbgUAAAAAAAAAAGgJgpENZvtCKgjZa8lLemh7SS4XHgEAAAAAAAAAAEDjEIxsIFuSVX0hT7X0Jaqf5PyF9cnVwiMAAAAAAAAAAABoDIKRDbM4vbFse0N+oQMvd80GJe8VHgEAAAAAAAAAAEDtEYxsiMXpjRM2G7LJfSH36m2Vo6WfJAAAAAAAAAAAQLMQjKy5xemNIzYI2Za+kHv10AYkrzTz6QMAAAAAAAAAAHQPwciasn0hVZL1TNffiwHrKlNLP0kAAAAAAAAAAID6IxhZQ4vTGws2ENmFvpB79a4NStJPEgAAAAAAAAAAoKYIRtbI4vTGrDFmpaN9IffqvMrY0k8SAAAAAAAAAACgfghG1sDi9MYh2xfyeNffiz361PaTXGnkswcAAAAAAAAAAGgpgpEVsn0hVZJ1qbNvQlhrNihJP0kAAAAAAAAAAIAaIBhZkcXpjXmbDUlfyPCu236SlG4FAAAAAAAAAACoEMHIktm+kApCTnfqhZfvoe0ludy1Fw4AAAAAAAAAAFAXBCNLYvtCKjB2qhMvuD4+tVmSN7v+RgAAAAAAAAAAAJSNYGRkXl/IBUqyVmrNBiXvdPg9AAAAAAAAAAAAKBXByIhsX0hlQ36ptS+yed7WMaGfJAAAAAAAAAAAQHwEIyNYnN44YvtC9lr34trhoQ1IXun6GwEAAAAAAAAAABATwciAbEnWK/SFbAz1k5y/sD652vU3AgAAAAAAAAAAIAaCkYEsTm8s0xeysd61/STvdf2NAAAAAAAAAAAACIlg5D4tTm+csNmQ9IVsvvM6lvSTBAAAAAAAAAAACINg5B7RF7K1HtosyZWuvxEAAAAAAAAAAAD7RTByRLYvpEqynmnUE8eo1m1Qkn6SAAAAAAAAAAAAe0QwcgSL0xsLNhBJX8juuK5jTj9JAAAAAAAAAACA0RGMHMLi9MasMWaFvpCd9dCW5KWfJAAAAAAAAAAAwAgIRu5gcXrjkA1C0hcS8qnNkqSfJAAAAAAAAAAAwBAIRm6BvpDYxZrtJ3ln5x8DAAAAAAAAAADoNoKRAxanN+ZtSU76QmI3121QktKtAAAAAAAAAAAAWyAYadm+kApCThceBLb30PaSXN72JwAAAAAAAAAAADqq88FI2xdSQcjjhQeB4X1qsyRv8p4BAAAAAAAAAAB8rrPBSNsXcsEYs1R4ENg79ZOcv7A+eY/3EAAAAAAAAAAAdF0ng5H0hUQJ3jbGLNNPEgAAAAAAAAAAdFmngpG2L6R6+/UKDwLhPbQBySu8twAAAAAAAAAAoIs6EYy0fSEVhDxVeBCI71NbunWV9xoAAAAAAAAAAHRJq4ORXl/IBUqyogbe1blIP0kAAAAAAAAAANAVrQ1G2r6Qyob8UuFBoFrn1bOUfpIAAAAAAAAAAKDtWheMpC8kGuKhzZJc4YABAAAAAAAAAIC2ak0wkr6QaKh1G5SknyQAAAAAAAAAAGidVgQjF6c3lukLiYajnyQAAAAAAAAAAGidRgcjF6c3Tqj3Hn0h0RIP7flMP0kAAAAAAAAAANAKjQxGLk5vHLFBG/pCoo0+Vclh+kkCAAAAAAAAAICma1QwcnF6Y9wGIekLiS5Ys0FJ+kkCAAAAAAAAAIBGakwwkr6Q6LDrtp8kpVsBAAAAAAAAAECj1D4YSV9IIPfQ9pJc5u0AAAAAAAAAAABNUdtgJH0hgS19arMkb271IAAAAAAAAAAAQJ3ULhhJX0hgKGs2KHmHtwsAAAAAAAAAANRVrYKRi9Mb6gm5TF9IYGj0kwQAAAAAAAAAALVVi2Dk4vTGrDFmhb6QwJ6on+TyhfXJK7x9AAAAAAAAAACgTioNRi5ObxyyQUj6QgL7p36S8xfWJ1d5LwEAAAAAAAAAQB1UEoy0fSFVjvVM4UEA+7Vmg5L3eCcBAAAAAAAAAECVSg9G0hcSKM3btnwr/SQBAAAAAAAAAEAlSgtG0hcSqAT9JAEAAAAAAAAAQGWiByPpCwnUAv0kAQAAAAAAAABA6aIFI+kLCdTSu8aYBfpJAgAAAAAAAACAMkQJRtIXEqi988aYK/STBAAAAAAAAAAAMQUNRtIXEmiUhzZLcoXDBgAAAAAAAAAAYggSjKQvJNBo6zYoST9JAAAAAAAAAAAQ1L6CkfSFBFqFfpIAAAAAAAAAACCoPQcjF6c35tVzjr6QQOvQTxIAAAAAAAAAAAQxcjDS9oVUEHK68CCAtvhUWc/0kwQAAAAAAAAAAPsxdDDS9oVUEPJ44UEAbbVmg5L0kwQAAAAAAAAAACPbNRhp+0IuGGOWCg8C6IrrNihJP0kAAAAAAAAAADC0HYOR9IUE4Hlorwf0kwQAAAAAAAAAAEPZMhhJX0gAO6CfJAAAAAAAAAAAGMozwUj6QgIYgfpJLlxYn7zDmwYAAAAAAAAAALayGYxcnN44YYxZoSQrgBG9TpYkAAAAAAAAAADYSh6MtGVZP9zicQAYBgFJAAAAAAAAAABQ8Nz/8es/HTfGqMzilwqPAuVYpz9p4z00xhy6sD75oOtvBAAAAAAAAAAA+DeJMeYEgUhU6PqF9ckjxphPOQiN9gV7LQEAAAAAAAAAANiUEEBAhRSInF+c3jhEQLwVZrv+BgAAAAAAAAAAgGcpGDle+C4QXx6ItL/lCO93Kxzq+hsAAAAAAAAAAACepWBkr/BdIC4/EGnIqAMAAAAAAAAAAGinhOOKkg0GIg3BSAAAAAAAAAAAgHYiGIkybRWIlOnCdwAAAAAAAAAAANB4BCNRli0DkYvTG2RFAgAAAAAAAAAAtBTBSJRhy0CkRTASAAAAAAAAAACgpQhGIradApFypPAdAAAAAAAAAAAAtALBSMS0WyDSkBkJAAAAAAAAAADQXgQjEcuugcjF6Q1lRX6h8AAAAAAAAAAAAABagWAkYhgmI9JQohUAAAAAAAAAAKDdCEYitPUhA5GGEq0AAAAAAAAAAADtRjASIa2PGGAkGAkAAAAAAAAAANBiBCMRSh6IvLA++WCYf29xemPcGPOlwgMAAAAAAAAAAABoDYKRCGGkQKRFViQAAAAAAAAAAEDLEYzEfu0lEClHCt8BAAAAAAAAAABAqxCMxH7sNRBpyIwEAAAAAAAAAABoP4KR2Kv9BCKlV/gOAAAAAAAAAAAAWoVgJPZiX4HIxekNsiIBAAAAAAAAAAA6gGAkRrXfjEhDv0gAAAAAAAAAAIBuUDByjWONIYUIRBr6RQIAAAAAAAAAAHTDAY4zhhQqEGkIRgIAAAAAdnHIfsWyygEY2nKkf3fFGHOv8F10gSpmjUd6nXeMMSHWrgA0R8wxA9cUIBCCkRhGsEDk4vSGBpxfKDwAYC/0eTphA/y9hr6DO2XnuwWiB3bwd4/Fik6Yt19ongX7WQXQzWt3rAUgghXdpfNqKeKrf67wHWwn1nFY5fPdWVcizmFfZbMB0DkxxwxcU4BACEZiN58GzIg09IsEgjrhBlv379/Pv5pkbGzMTE1N7TQBLTyWZdnjJElu24HgKgPCVjq01bFHI8Ta3Q6g/uYjXrsJVnTcBx98YDY2NoK9CceOHTMTExOF72Nnn3zyibl06dKOPzPKMZibmyt8H93zzjvvBHvNr7zyipmZmSl8H0B3hBwzcE0BwiMYiZ08VLAjYCDSUKIVCO/NN980t2/fbtU7e/DgQQUq878fPnw4D1xqIDgxMTE2MTHRswuebtfbu8aYm/aL0hkt0cbzuq1Onz6dfwGArt2hEKyA8/777wcdE9gxZeH72Nnjx4+DHYejR48WvoduChmMFAIHQLeFHjNwTQHCIhiJ7Ty0GZGhy60RjASwq0ePHm0OIN2fbqI6OTmZL2C4r4mJiePGGH1dM8Zct+XcyJgEAKBkIRd/CFYAAAAAQHsQjMRWogQiF6c3VHrvS4UHAGAEKrmhL+14E2VQKntCX2NjY6eMMaf6/f6P0zS9ZAOTAAAAAAAAAICKJLzxGBArI9LQLxIILv9MNa1XZGiuf83s7Kw5f/68+fjjj02api8rUzLLsn+wvTUBAAAAAAAAABUgGAlfzECkoUQrENy4sZmC+JyyJdW7Tj2rFJRMkuQlY8wPbNlWNkQAAAAAAAAAQMkIRsKJHYg0BCMBlEU9q/ygpDGmZ4z5oTFmmYMAAAAAAAAAAOUhGAlTRiBycXpDGVzThQcAICIXlLx8+bJ5/PixftFSv9//EVmSAAAAAABUapy2KgDQHQQjUUZGpGHhH0CVbty4YY4dO2bW1tbyfpJZln1kjJnnoAAAAAAAUCrNxW8aY/4/Y8wCbz0AdAPByG4rKxBpKNEKoGqPHj0y586dy7MkkyQZM8ZcM8ascGAAAAAAAIhKGZArWZY9snPx47zdANAtBCO7q8xApCEYCaAulCX52muvubKtp+yOzHEOEAAAAAAAwahK2pV+v/+PxpgfaP795MmTsQ8++CCfkwMAuuUAx7uTyg5ESq/wHQCoyCeffJKXbb169aqZmpo63u/3P0rT9MvGmAccEwAAAAAA9uSQsiCzLDubJMlL+gfSNDUKQK6uruZfAIBuIhjZPaUHIhenN8iKBFA7Ktv65ptvuoDkywQkAQAAAADYk3lbijUvv5okiVlbW9sMQGr+DQDoNoKR3bNQckaksWUZAKB2BgOSxphVrlkAAAAAAOxKWZDzNgvyoH74/v375rvf/W4egNzY2OAdBABsIhjZLa9fWJ9cqeAVkxkJoLYGApLTaqpvd3UCAAAAAIBnjasXpHpAGpsFqTKs77//vrl9+3bhhwEAMAQjO6WqQKQhGAmg7gYCkppQ3TPGLHPgAAAAAADYNG5bnLzssiAVhKQMKwBgN8kuj6MdKgtELk5vqNzhFwoPAEDNaPK0vLxsHj9+rCe2xEYKAAAAAACesaBApPpBHjt2zNy4cYNAJABgKAQj26/KjEjDYj6AJvnkk0/ygKRkWfaeLT8DAAAAAEDnqT+kNvC6eTMAAMMiGNluVQci5UjhOwBQY2q0r1IztgF/1ddQAAAAAADqYFbzZM2ZyYYEAIyKnpHtVYdApCEzEkATXbp0yRw9elT9I48bY04YY25yIFGmqakpc/DgwUa855OTk4XvAQAAAGidPOHg9u3bHFnU0uHDh4M9Lea5QHgEI9upFoHIxemNQ8aYLxUeAFB76v0wMTGx7dNUkGS3QZ7+/+3+jY8//rjwvbt3727urnSTmyonOQpIXr16VWVo/iJJkl8xxjwo/BCiOXfunOvfWXs6d3W+hPTWW2+ZmZkZTjAAAFALGpfcunWLgwF0W97GZGNjo+vvA2rq7NmzHBqgxghGtk9dMiINWZFAcykYGSgQsjb4jX6//8szMzMvD35/u9+nPo4KSupL5WDKot+ncq0nT55UetqCMYamGCVSZiBy5xv0NtwrfAcAALRBYUwfCJv9AAAhxFwsYp4LBEIwsl3qFIg0BCOBVnh1hxexp8FemqaF71numjFuy78ou/rQ1NRUT4GpkydPmvv375t33nnHvP/++4X/OQb9LgVmn3/++bfSNF1hEFqK5YYFfj8rfCcsguAAAKBqzO0BAHW2GjkgCSCAhDexNeoWiDRMWIBWWN3hKzT3u27aAMy8vY78O2PM7xlj3lXZ16WlpTwYOTsb/xKjsrE3btxQAPUXCQoBAAAAAAAAwOgIRrZD7QKR9IsEENADG6A8YYz5VWPMdQUlL168aJaXl/P+lTGpVKt6F/b7/T9wPTIAAAAAAAAAAMMhGNl852uYEWnIigQQyT2bMflqlmU/mZubM1evXo0akBzIjlwo/AAAAAAAAAAAYFsEI5vt+oX1ybqWDSQYCSCm1SRJft0Ys6Z+krEDkh988EH+Z7/f/1rhQQAAAAAAAADAtghGNpcCkfM1fvYEIwHE9sBea667gGQsGxsbZm1tTdmRL9pysQAAAAAAAACAIRCMbKZaByLpFwmgZPMuQ/LcuXPRfvP777/v/kowEgAAAAAAAACGRDCyeeqeEWnIigRQgRPqIXny5Elz9OjRKL99dXXVPH782GRZ9vuFBwEAAAAAAAAAWyIY2SxNCEQagpEAKvAgSZI/1K9dXo7XSlcBySRJDnKdAwAAAAAAAIDhEIxsjqYEIg2L9AAqsqpr5cTEhDl9+nSUZ3D79m33V65zAAAAAAAAADAEgpHN0KRApKFfJIAK5WmRr732WpRnQDASAAAAAAAAAEZDMLL+mhaIBIAq3dN1c2xszBw7diz409jY2DD379/XX3uFBwEAAAAAAAAABQQj641AJACM7or+j9nZOMmLn3zyifvrkcKDAAAAAAAAAIBnEIysryYHIh8WvgMA5bmTZdlPer2eOXjwYPBfevfuXffX8cKDAAAAAIA2GbdtOga/2JwKAMAIDvBm1VLTMyJvGmNOFb4LACVJkuT7xpgzyo58//33g/5SLzNSE9DVwg8AKItLfx5Mg/b/e/AzurrN94FQZu2ipVugHN9msfKOMebBwN85L+M5Yr8OuWtEv9//5TRNXx74jev2WDywx8V93WvRewEgnnHvejPuXW9+LU3TF7f5re66c89+rQ7cI1AuF3A8kmXZTJIkL+3227Mse5wkye2B+8adwg8CALC73eaS/pyxceMFgpH1824LSrMuE4wEUDFtijgzNTUV/Fk8evSo8D0AUbkBeL44tMuC3qDB/q5L7i/9fv+naZresgP41ZYEgma9YOxWE5f9WNjHwpoLArnnc8h+bWdwUvXAXtfrFhDyz8tXRjgvzRbnZs47L1e9BWmMTufXCfu15Xudpqn5+OOPN/9b/aanpqamvR857v6SZdkju9Fp1Z6LBAnQJaHvJ757DQ/2H/LuA7+z3X3g5z//ub+hcdPAdcddq/KxSr/f/3Gapn/jXXdGeU473WP3o+nHaztagzuRZdlXkiQZcz+TJMnmfeLWrVuF//Xw4cN5JZ6JiYmxiYmJnn+/8e4bN0c8fjGEPCdinFtHShiHx+nhEs7KHj5bux3X7TZltnWzQ8xrX9Pes+XCd3Z3ZIfqX1td+1d3eAzDc+MIt5Fpy3nLgKXCd4xZs8fhjre+sVezEa+Z9whG1su6HQQ12oX1yXuL0xtvKxDQ9QMKoDL5jVcTxNAGMiMHaSDx94Xv7oNdmP47+y/cswGBNkwe9F4tZ1n2+0mShK+na8yvMihutG0DCW5BTyWTtTlAf/c3Cbj/1uKQvyFhcnJSi0X5n/qamZnRguGLNuDgBvTveot+TTx/ZreZnISw3eR0K7P22OnP6S0e381Wk7DxPU6sQzthFyyfuXa5wNbGxkb+5Z+Xg+eoHD16NP/Tnaf6U/esgfPSZVt8jyDYUHSOzPf7/T/yMx7v379vbt++vXnd0H/rGO3EXS90nHRcjh49enBsbOyU3XB5zV4rrpDNio7Q4tiHkV7q+Zpc20eRj1EGrzXuPqDrjK43g/eC3eh64+4Jr7zyiu4H+rf1dcYGtr5jrzu7jU8WIq7FNPF4bUfHcSHLsjfc/fxnP/tZfr9wX7vdKwZ594xn7hvevXylovvGfMTxYQhf2Gbs1yUxEjq23ZTpZfOueoGLpo8xY57nrzZszBf6fdjq8zn4O/xgGBsqd6Zx1bydTz6Tfa95ir78DTC6Hzn+GoebP9qNTW5DjH8tWbPzx1GPx4mI44g1gpH1oUDk7IX1yVYsMFxYn1xYnN4YJ0MSQFXUN3JmZmbXsjqj2mVB4Z5doDyuxYfHjx8XfmBUU1NTL46Njbld1r1+v380TdMvN3yyoLJHH2nn8ZMnT7bcKb4XWjjWlzcQRrNsG0hYXV3dDCYMuzCkz6o/cPf/7ijgYBeM8q+JiYnjNgj0LTs2u9LEANDly5fzxdCtXvOoTp8+nX8NobCoZ2wgbqtF2Z2emwvUGbup5OzZs4WfKdkR+9q+6jImdO1yi5WaLI56HfNfv85vnyaYbjFzdnZ2zFvM/POKFzPr6pBfmUUBAR2PDz74IH9vR11MFhdU9o+TjsuxY8d0TDavFRprJEnyDXtMgFZToG2r7LC90P13bm6uaW/XCRvoyxdl3bXGjVF2uq8Nw/3/+vfeeeedfJHR3gf0pcDWGbs4uGavedvdB/IsVv0boTT0eG3nmXuG7ud6z2/cuLHvOYk7D/RviT12z9zLbcbrpSruG7ov7uWeuBWNz0NQ4D0mfQ5mZmai/o4A8s/s2tra5rkzDLcpcyuDmzIHN79tlc3rBS5WmrzWEPI8txtDCt9vAp0bly5dGvqZ7rRZz507jn9+6T3aKhhmA95/42WHd31D5bjdyHTOrXW47HuNrUYZRwzOHR23IUbHxq5tbH7G7Zzl+/bzvVtgMr8mvfnmm4UH9uPq1av5/00wsh5aFYh0VG52cTq/kBGQBFC6JEn+H2NM8GDkEG66YOTyctiNw/r35ubmXrYDiBOFH2iGQy4QqclhyIUS+/4YG0BCcxyxi3vBAgnDcgEH11vWZSPoPLJl0655AaDlpgS5QwUizbOLTdv1yd1yUc8tzu6ygWNLoZ57APP+hNF/bdtNAkNwAVz/vFQQ7NixY35gkiDYwLmnDUBaxAu5COVzizr60uLya6+9ZuymJ10nvpkkyR9u8xkBWkGLZaHGbVoka1Bwa95+xvN5he6L3/3ud6OPUXT/dPcbLfzquqPNQXZx8UMbONi2hHrIMXbDjtd2XIWFPNvDHUfda2O14fDHC/Y+7jJer/X7/T9N0/RPyryP67XWaIyFf5NXHwk5fh/clGm2CGC4DQ9u04MXuNCmzOtN3fwW+jxvajBS4+JQ78NWQW93Pvn3GjeX1p92E4bb6HutyefUPo3bTa1ntWHXVVHQear3MOT9ZzCgqY1E+mzrM97r9V6y978zdlPM/7nbxoNY9wuCkYiKgCSACilo0NONt+RJ10qWZX+mHcwa4IccXGgBVAO7qamp4zsEBupuOUYg0u3YtmVtq+7LguHM2kWhfLeeFoU0KI8VSBiGCwIpoLFVAMhOohoTlAxhh2NRWNTTZzr0pKoimwvP/oTRBQfLtlMQrIrFzBrY8twr8/i4BWbde7QRxh6PD211hPmdJvYAGkMluf9M9wJlL7ispSqCObqvuvuQ5jbKVpiZmdH46YfGmLftNZHrzvZ0LP9CC8FaoNc9Y5QMtBDc8dP4Uvfxubm5F9nMgir5Gx40xnTzac1/JiYm3Nxnt0xsYJObSzsDVUUqzw6vwLILQrpNrbr/7DC/Dkq/R/c6fblNTd6mmG95Gw+u+Bub+v3+r6k1TixJyw5yU023+UOogKQdIANAmSoLFqini0pV6GYfkiYMXqmNJvZrURbLKbdwHJLbrZ2m6cVqXyKGcMhOaLVw39P5cP78+XxgXObgfDcuAKTnpednMwRP2b6wKyP2UGwblVr+HwoGaVHPHb+Y2QUlme33+z/S4qAWnxUY14KvMlGqCkQO0iRWz+d3f/d388B9mqZuMfMfGpwxP4oT9rUWzr0q6Hql46HzxC6+5KVbO3IsgLZy45QfuHuBrrnnzp2rRVaZnoOuO97Y5Iy9LrqJR2+rLJaOGrdVa3QsDyoTUveMsgORPlc9R+eUAtw24/ZD+zy7PLZExTSm0VzMzX20Gc9uGv3QXhOPcIwwCn8+/dZbb+XnlK04c83OucIumNXHrL0vL+ne4z5XuvZXtdbhNjW5eaSek20rdcpubFp1x0Pzy5jjCIKR9XF8cXqjzQFJlQ95vfAAALRTfj0/efJk8Bfn+ubZicGhwg/Umzan5CWRQtMOY4u+XfU1bnfd/f1gELIugZ6tuIH7YFDSBhwWtvhfWsULLrrrjT7HP3QLtHU/fkMat9eODzVB1kRZ1xRNGOtazkwT2S0WM39gJ5JNuzcMY8sF5bqcezpPdM5oYm/7iv6AkuFAIy27cYruBdpoUOXi4U50/dN1R9dD26v5Q3fdCdG33uctSjZp4XjWbpw6rrGbjqUWxeuyaUrnlALcel52bHncPl8CPqicC1p4m61cJnbXN2Rij9yGSp1TXlCybRsx3HrHh5qbaY7mAn912rDrNh4oeULrG95n/MOnT5/+98L/EBjByHo51fKA5AoBSQAdoRIHa65xdGheMK9R2ZH9fv9rxk5uQtKCtDJRbYkJSlTV0+x2mXRN4hb+NHh/8uSJTrpv2V2drV048hYgD9kFiGvuGGqBtgUlWV2mXZ61rV27mqg9p3AAACAASURBVCg3JavEX8x0E8ksy37UskC5O0a1XFD26dqg52cDAWfIdAEa45BdgFvS5/fy5cv5vaDu/fVc1ZSB606U39MwK24xWBunNHar67F0m1n0PO3Goh+6DZzojNpuInPn58CGzK5U40AELrt/YCPGP7QgS/JIv9//yK13aE6pOVodNzP53PqGCxIfOHDgN83OrVr2jWBk/RCQBIB2yK/lCriEpl1lGuBkWfbVBi1ynlC5B020Qy9oeO8xWSj1dMVfEGp6Jp3OXwUcNGj3dnX+sANZktoteUrXnjqVLd2nK36mnY6prq9NtEVm3rdaFAg74x+jugcH9Px0nXNlW+3CBAFJoL60YepHWoBzmfFVlvHci4HrTu0XP0vwJbfBqAkbp/T89DwV8LFB5WsNbcmBvcmDkXUe32yRif0DsiSxH27uMpDd39Tr3nyWZR9pXUDZkLofN21O6QeJNRYiGNk9BCQBYP/ynVUVZres9Pv9n6qXoZrBh+SazdsF56bsSsx3+IYOYCj7dGZmRlmXP/abbqMWDtmswXx3oCt11oJMupzrFafsCbtw1Kbgz5ZcILIFvajG7fVi89ysa6bdqFygfCAQ1ujMXRccaNIx0vN02apamCAgCdTWvN0wNabrp+5xTQ3k6brjMuy6HIz02wA0bTHYlca048ol2k+gTvxMbJcl2YZxJqqzRXb/UgPn0wqgXtM4QusCyoZs8pzSBSVjtFZyCEbWV1cCkg8LDwJAQKEHAl7Z1XuFBwekafptfUcBydC0YGI+L316rgHnyyFXXi/0jk/XKzJN00uFB1GlPMvA3x1Y92ymvVL2hBega0XwZ5BeW4sCkUdsyeBpvZY2npt6XZrU67Onz6B26jbxnNQ9w5VKbOJ5NxiQtIsrAOrjmdLjbmzddNr4pYBkF7WhAofuGV5A8hQlW1E3LqOt6eNM1Ic7pxpYVWTFlXfXmL9pVRV2EjOgSjCy3roQkJwlIAkgkp7dsRfLrsFIVzbUBcxC0o5nr0Rk3Qf/+SQ69O6qgwcP5oHeLMsesXO4Vp7JMmj67sBhuOCPFsHaOCnXtbRFgciP/B5SbT039br02bOlj8aaeE5qMbnpk3o/IGlLHXOvAuphpYWlxzd1NTOyLa97ICB5jf58qBs3zvTaA9DrFPui63fDqoo8M45o68brGAhG1l/bA5J3CEgCiCBfcI2xcH748GH312GCkQ+MMdfHxsai9I70Fk5q3auu3+9/zUQo0ere0yRJLhceRFXm7aJJq7IMhuH6/bhJuQ3+NL0Rf06vq0WByDEFIvWaukClj/RZbGpAsg1cQNKVNGOxDqjcMwuILbi/oYV0XirYI1mW/aXrKwjUieY9Gmda1xjjYD/cmNlt8K1xQDIfR+g67WV0YkgEI5uBgCQAjCYPAMTYnTQxMeH+Okww0rjsyJMnTxYe2C8F97wSPnXdNTafpumLGlCGzkDy3lMyTeph3pU706C8bVkGw3KTcrtL+MM2TMpbkD24GYjUselKINLRZ5GAZLVcBoH5fFH5z1lUBipDIHIPRmlTgXA0l3Wb3J4+ffpfeWtbK1+7aOr1yI0zrWtt2YyJargNvq4McA0DkpuBSAVOu9yjea8IRjYHAUkAGF4+AL5161bwt8zLjLxTeHBr+rm1qakpfyIfjBfwqWvAI39eoQNTs7OzLjD8LgsjtbAZiGRx79lJuQ08EPypzqEsy/6bFvLUf7CrQXK9br1+G5B8ryF9WFpF10WvnBmbaIDyzROI3DfG3CXTfUOtOQ4cOPCbda+Gg/1p8ua/gbnP+8x9sF+uMo9tS3SlJm/oghtHvPXWW61vRRMLwchmISAJALtTtsFxlUOLscigoGK/3/+pLcE6rPzaHaNUq+vDmGXZ2cKD1dOx6Ok4hM5S9fpw1mVg2mUEIrdANloYk5OT7t8Z5ZrrjPf7/feTJDmo7Oym9x/cL71+vQ/qmWmMudnsV9NMWlT2+kdSygwozwnGKmgqV9Ehy7L/wmYi1NXA3IeNb9iXgb7rqgRWdWkbjSO+5cYRZETuHcHI5iEgCQA7yxf3XJAuJAUi1f8xTdNRUy5XFMCcm5vzF9aD0CBIJSzs4nLdSqLku3dDHwu9hzMzM5qQ/8QYs1r4AZTpiOsRyeJekZ+N9tlnn60xKR+dVxp72Gx034p203apR+RuvP6fvRpM6jvJnYv9fv9PuSYApThke+61pQcyOkbzPS+znns3aktzH2/jG1UgsC+uZKttTbRU4XrXETeOUNsFxhGj89ZB7xGMbCYFJO8sTm+0cvJKQBLAPowrQ1CDlRil+F555RX315EDYGmaXtSfCkiGVtNSrToWb+hYrK6GjRe6XpFJknyj8CDKlJe/1O/TLlgG5Vtz2WjPPffcL9meFyiHFuuO67y8dOkSb7lHZYVqMKnvLJ2TuiaonzIl94D4bIb8mII5ocekQFm0udPeu8+wkQV15m36OM44B/ulc8nLDv+LCq5/qrTzl67lR+iKX13hbTAmGNlg01oMJyAJAM9YUDk+Lf7HqN/u9Xzcy0qGsiP/xSsvGowWVlSW1pavqMt94YSOhZ5byGNx8ODBvNxtlmWPKDNYLVf+UosjXe3DN6ya9ryoPX3WvWvmKGVaNYZcop/H1pRhUfGkvvMUFDGfX0e/1vX3Aojsiu696rnnPndAE2ks45WbJ8DTIv1+/9dsoLk1BkoLH+r6Mcb+aE1Jaw4247bs7PBljSNUjazrLT9COdCOl9FZLiA5e2F9ci99dGpNAUm9Nrvo/4WuH2wAuzriFp9jlGg1Nhipcqtpmu6lXOCDNE3/amxs7JQW2EMHb/TvqUymnZxWXr6n3++fS9M0+MLP7OxsXirXGPOdPfaQQxjLbnGvTllnKqWsgLWjIH1d+jmo54U+p2NjY2dsIJ30jG3oWqvPusuCNsa8O0KZVmVl/0WSJPlCSB2O/+B56VOQuopgqSb1mlT3er2X6nLfiEVlgbzduPn7XXUmt85LZUfOzc29aKsaUMoMCE9rCWc0N1BZtaro+q/7gNvUqP8+fPhw/nddj+7evZv/XdclfdGHqho6Rjouul/oz8H79q1bt/LMRB2vqjJjNMfVfE+VgJIkGfm+rTmot7l2aK7qUMzxiu7V3jwvGP/+X1eqlNC2CjN6PVoHOH36tCstTJ9s7IvOJ10jJiYmypxLb44jqmz54cYRujfpGtn0cQTByOYjIAkAtnSCgl8aJMSYKHmTo78uPDg8jWBOaRIZOhipRU39u8qySNO06kXlWReoCj0QsgFXQ3ZZpTYD/1Ut7mkArs+kG5Srh+hO9Fw1MNdCkv6sokybrkt6v65evarP6X+1WZKtDqhrYUmLXu44ub67I3p7xEDZsnbN6ppYxXHWa1RJb/3pvnaj81MLm3q+obPJd6L7pQ2QL9lg2L0dfrwR3GKmO+92WoT0rwt6/8teXNbualu6fYFgJBCevzGl7E0fuv64wM9u94Fer/fMf7sWB7omUXkiLh0fHadhgmCDY03Nc3R8yrxv6/fYjSyKlJ4YtUrMftqFaA4Wqz+8jsHS0lLh+x3R2uoUCh7p2E5MTJyy4xw2YpajleeUrn/aBH3x4kXz9OnTPz1w4MBvFX4osKaPI9xX3RCMbIeuBCRX7GsFgEErWtiPufisSaq1n9KgWuhdm5iY6GlQEXLhcyDLYuTJaWD5zsfQCyh6z+zC8lobFs2bKnbgfyduwWhwwG0zlv/OBvf8DLr8g/vCCy8cnZmZGfMXkpQV5haRyqLPvHa1nzx58kUbYGtdmS1XSlkLXttMnNYK3ylyx/HmCBmRxt+9WmbGrgJgyuK0u3UHH17b4rx0tFhwROdnr9cb03mtxThdy3Vuxg6O6fOrhaKzZ88au8HjROGHGkLnnI7BFuecO9/8D7o2VIy/8MILMzMzMwfddUHnjQKE+oyWcW1zu5enpqambQkz7mtAOPnGFN3ry7zP61qkgM3AvWDN3gPueJ9z96SO2HvBuP277glfmZubG9N9VJuYdD/QdYmMyXA0p1DFCnf9z7JMNTL/xh4Xd5z8a7I7PrpWz2ZZ9tszMzMv6f/XMSrz3qHz2QYVR5nv7TcQNKuNSy64FPJ1agylsY+OQZIk/3mb8VKbHWnzi9M5YwPNy/QpL01rzyld/7QRZGZm5jdLqCxSp3HE+sD9yXjX9EP2a8txhMtsr9M4gmBke3QpQ5KAJACfBiDHtaAXa/FZi+u6kduAx36DfFrw7akPWuiFZg0y7OR0vsJgpAZCp1QeM3Qw0usdR1ZkdTZ7JpQ1KNfnT0EGHX+3az3Lsp8kSfJ9V6JFwdGdaEejHaBrLHGi3+//Tq/Xe1HBH52rmiiXlX1QUYmZ6LSYpInTwK77de813omdCdrv9/+8zED54GKm9a59vavDLqjZ81OTx/l+v/8H2lSi91FBSd3XYr4WLaDq8zUxMXHcfj4adT5uMWFf9xZdd3z/7ft+yL5uLbTmlQt0rdH7UkZ/OR1jGww+wb0NCOZQv99/6+c//3lpG1O2uBat2WvRzV3uff51Kh+722uTrkvzL7zwwldPnjw5puu0FhJ1XaIP8t5pTKkxgrepLT9OSZLstqD9wLs/rvj37eeff/5rp0+f/kWdA/q3Y28k0vhbi8s6N5IkGbb05WBwdeRfq3G0Wg1oDBtyzOxK8idJ8scNqRLwWeE7AWgsqUoNo9AcRl+DXNnGOpRsdK1ktBmbjVdb0rX+w60e2K+9nFMK9G1F55POqyoqiQxSlSF9ZVn2zSGu3Xulth/nnjx50oRxhH99b8Q4gmBku7Q9IPmAgCQAz7i92fY0yNaCcKybqgYG5vN+Dn9VeHB0NxVIUZ8uLd6HnBxoYKgJiV1UrmqwHyUrUu+VFg5sEKrKrM8uq2RQrh3nXums61qwT5JkxyDDNh7Ya8ZNG7zMB+gqHaQduxr8l7GIVEWJmdgUvNH7p+OUZdmjJEkuV1D2c96Vh44dKNdips5LF3i116VvDDFZ3InO6YU0TZUtO68J9tzc3Eta9NN5GfM1NXHXuo6BPkdeIPi6ff6jnnP37Lm6YjOVF1544YVz6nHk3vuYfZx0XG0wcp5gJBCMNk79ojYVxF6E1/hU14kA16JB+aaWJEny65J6BJ48efKgC3jVsexa3SlzXvcNLfTa+/Yf7nMDjrtv63hf0XhSC+S6p8bezGKzI8dK3kSkMc6ZLSpA7Ivre1ZxVZ+RuDLvIWjsbKs6ZMaY/2mM+X+H/We/+MUvHp2YmNiyvrBfRUbP1VVnqSIwOTDOpHfkFrYLLO/FwDn1o2HnJlmW/XtlfBce2KZMta5DsXvJbkXnss2OfCniNVDrDWOMI+IgGNk+LiB54sL6ZOt2nBCQBGDN2p5rL8YORBpvx2aohUK7UP8t/buhgzqaZHgLm6X3jrQ9K/PnEZJb9LfvHapR9aB8IXBmnctec4tIx7WIpKzP2Jl1JZeYicbPMOj3+/9ijLmYJMmVKnph2t2xeZA3JmVD6rptA695EDLCrtwVu+liQWXR9JouX76cZ+vFMLBrvfbZkf6Ccr/f/3Gapv8pUFm3B7Ykk87hlampqeNaQIvVI8vYEuteqdbxtveRBUqQV+hQsCD0WHSQ27Cg+8HTp0//9sCBA/97hBKT/nVp855QRuZ8m+i+oeu53dx23S7OhrrePnBVabIs+0ttZnHj2Fi0GG/nRo2raLCDxtz/dN/W2CAEjSs1/zDGfDTqhjCb/bQVV1JYf57QGEOfAa09aP6h31dmdpvGmdrEZ7N5Q8/nWkHvUahNDN459T9HKdm6w/m0bZlqrTvpfqTnXmag22VH2vWB0NfA0sYR/sbrksYRy8pwd+OImPepnWx7pqHRNJlVWdNW1om2WZ+ztgwTgG45ZBftP1QgUoMDZeTEnIjbpuvGBkNCbfJYUV8M/dtazA/JZSQqKBj0Hx6OMpNe1MAm9DHRcbbBjkYGbVqgtEG5JlAKumiCo0CDMeY37CJPrInrPVsi8VWbtZz//i16zwVlJ1B5EC3qL4pE1y69BhuIVEDoZRvYrWKBYV49PXTtiZnFpmu2XrNdzHw7SZJfj3hNemDfT52XjzXZjzlh9BZAat3H1C0ou/tymqb/IdKkXdeEr+tY6/fFvB54C4L0UgL2L79Q6j4ec36gcakW8+z94LytchCz1527J/yGxioKROl+FHoe0UZ6j7xA5PmIY8qbSZJ8+bPPPvtnHZ9QwaqtcN9opZABFdd3fdkGkf6dMeZ1XTs0v3IZvAqal0VrFNrU2uT+5A30fwV6yq5M9Yqdc/2KMeZXdT1VRRxd79577708qFbWPclVBFO7JhssDSmfC8UeR+geoYxhe2/6eknjiAV/HKHXWMU4gmBke33BZkgSkATQBrN28PP3rh+hsiHLKBXpTSRDLjg/SJLkexp4aFd1SBowaUFeQcEKyqDkvy909o4CAHqvbJlcdlJWo5TFvYFgz/lIgYbtrNrg0nUFOsoIQGh3soJoTStZ5AKR9v1RQOjLVfaAcQHdmGXRdG5qwqjAoDHm9yJk6m5H5+WX9Xs1aXRlw0PTIpE3qT9Uwusa2TYLyjFpB/HrLiAZa7LuLSq3ct4GlEjl5L8ae+OUNoZog4i9H7xaciWSO3assqZ7MAHJ3blqBl7pu5juPPfccz2dG5pDhp7nOcpA0nmeZdnRwoNA0QNbdUNBpFe1iVBBSc3rYo0rB3ktXAhGtsM9m233SzbQ/UiZt2Vs6HW8cyrkfEDjiDfKGEfoHuGNI8ps1VD5OIJgZLsRkATQVEdc6cJ+v/+Ptql3HoQ8f/58Pmguo7SIlxW5FqH8Qz4ZjrFrNtLAbDdHXP/O0JlJocvkYmTjZWRFDgR7Xq+izLBXaivPiNKEKtZCkvGyI5sWjNQEygUiI2etDuOEAroqrxurPJB/biowWEFfozsuIKnnEeuc9D7ftcyOLHlB2dFGpLf1e2NtgPLumWS4APujjI2xmD2slBGpjSHe/aCKEpluHeQ6Acmd6f7tVdooa6yle3Ye4dF4Kdax0b3DZpqFzgpCu63azZ75XEfjyjJKNep8tQH0rxQeRNO5QPfbWjsrK8itDfhWyPGz5pUHY44jtP7X9XEEwcj2cwHJVu4+UUDywvqkFsDfLjwIIITVIb+W9/B1c/Df1o4qY8xnxpgfGmOuKQCiDD8tMr/11lv5oMYLtEWlm7FKTVgxJq/aTbamAZvKUobkla3olZjhki9ehw5W6b2xQY+1EjPk8KzNjNdYg/Itgj1Vl+O9YgOifuAtOH1WbSCi15SsKC3EutKsNQlabZbSiUHXoIFzs6rrkBY31Rsx2uKm7q92oeiNwoMVq2hB2VlwWQQxAsEuiN7v93+58CCAofX7/T8yEcaijj7/LiOy4vuBM+8vJOJZ/lzO9hYu06rbyBKrXOutW7fcX8mqx15orvMbrqxwGQFJ9cy3AXQ2X7WPKwH6uts8GTsgqfGzV9UliH6/n980Yo0j9J64jMi6jSPKqDrnEIzsBgUkf7A4vdGoXfejuLA+uWBTmwGE1Rvya2kPX8cH/+2f/exnYypbqOCjSqKpFOsrr7ySTyQ1eC2TBgmuL1jE8oP5qF+L+6GVnOGymTkXOljsDWLpFVmRLMvOmmd3Hwalwa8mLObzxvl1GJQ7K36Jxlh9VeqejebTwp6ujerfmqbpsRqUTdZmi54mojGy5fV63cQsSZI/rsG5edMtbsZYNNJmA7tQdLBu2bpuMbeCBWX3e3X8/U1KQWnsY/uuAtibI/oM6bMUI0teYwB33bVZb3UZq8xrs4TGUrGuT02lyipeNn0Vx2vZlS6M3JuvlqXV0QgqK/wlXUPKCEhSlr4TVvxqLrEDkt4aYYgA9yGNI7RRONY4wt2n6ziO0KbLmL2OfQcK30GbXVuc3lDgrnULuovTG+OU7wPCGeUmpMXavWQNafE4Vkm9/VImjCaOmkAmSRJzVK5s0J/0er2XNDgJ+X4oKKjj+MILL7yRJEnsIEeUzCSdW5oY9fv9n6ZpSjCyGrMxS2C6HnDW6zXMftV5d2RsbOzMxYsXo2wc0GdVE5MXXnjhq0mS1HrjmJ6nXdi7WGWPSE9e+SPW7lXv9b5dow0RytL7Sq/Xe1nnjj4/ITeB6Dqu6659b2vxmr2S6VUtKBub5fLuxMTEcWVHlb1BCsCu8vtnrAoqWqT3+tXW6gKgvs1Zlv3DyZMnD+raVEYriybwFsGrKPsvD5IkuawNu5pXhs468Y4zwUjsxwNdQz777LNP5+bmfknnVazr6N27d91fZ1m/bTXXXuKjpaWlMR330G18nIFWB/u9N+fzylgbsL1xxNfrOo44ffr0QV0DYo8jCEZ2T+sCkjYQqQ/ydOFBANEpk6JNk14FR7ydz38YO/MnSZJv6NocepLqMlzm5uZchku0636/3/9amqbBB26uV2Sapt8uPIiy5It7sRbevUF5nYI9gxRsPzI1NdVTgN8LngZjP6tjdhJUdj/CobjNAXaTRi0WEFSST9eeGOenNqXY1/uTyJtSRmazA29OTEx8SbuOXWZxYMdt1nvV2a9+3+Cqj4PO++Na4A59zmmhRjuSAy2mAJ2TZdnvJ0kS5X7gl4lO07RW9wPrgZ2z/EDjqjJ6ddWdNo14m1iq3Dyl+8aSjkmZJfCAET147rnnVKnqh9qIp7WdGJtQvcARAfT2u2OrylzTtU8bemO0e/EC3PvOto05r3TjCNt6qI6B+FLHEZRp7aZrbSnZSiASQGi6+XqT1zKCAje1uK8bfugeYF6mYsxr/rz6eioQGXrSQonW6vX7/d8xkTINtFBUs96DO5lXuRlNpGKU2vImPbXt8e0+j3aXf+UBqtildFQm3PzbhpE6vF7fHTvp1s7ad+3ENsZX5T199Hmz1RfWa5CN66oZBL9fx+rHC3TEIVVxUInWGJ+lqstED+mmzd6OUsWhaWo0h9D44bo23oXuP+6d65S8RAgaW349VisAxwYkWb/thhV3X/I2FgblBbjH9/nvRptXujYnVp3XPDbHEQQjEcsVG8hrLAKRAELT5L2C4Ih2IX1HA//QN30NpuwArRdxB2KUslgDZfnqUA6yi9R/6UWVaI3B9Uxwvdhq7l6SJJf0OY3RS0HBSPVcVWZH4cGasKU7TY02B0QrpaOsSJcFU+PNEA/sztoTNmgY46vyLF1tWrBqcRySJPm+secIgNrILxSxshlqUCZ6WPncpayeT3Vm53M/rUmmef4cvPtZEAEX4QHniusdF2uco/kOOkUbeh/pvhSrd6633rUf0cYRA5n6nR9HGIKRnfaFyJkyURGIBBCaFhvOnj2rYMBju/O5zEyYvFRDjB1jXi+1GMFVBTh7GgCGLtVLVmQt5MGeyIt77zaoJOGysqIUlIsxmdJnKEmSg3Xc4V6z7DQnnzTeunWr8MB+uetPmqbUVKuYl0lSl+tE/jxCZ7gM9LwBMJr8c+OVawvGW5CrY3nWQfdcFl6XS7W6IEqapn9deLAa+X3j8OHDNXk6wPbcJlFXISS0gb6RaD/XOzdagCtQgDs/H2O0n2riOCJ2diTByG6rbSmwnRCIBBCaFhVdllaSJMcq2LF0z5VECL1r1su4eiPCztl8QOUFPIPQ8fCykuidVZ0yBuV1L8/6DFuyM1p2pFW7ybm3O7o2/SyzLPuKrm1eECcI1xvTZlSwGaJiXuC/LjuJ8+fxyiuvFB7YD8q0AnuXZdlvmwjjFY1H7captQZV6cjH5rFK4jVBDccs+bljzyWg7vKS9JqLx9h8yXink64oO1LrXKHbHAzYcyWwfr+f3zhCjyN0P/I2YDOOsAhGdlvjassTiAQQmhYa3nnnHaNdxMaY1ysMfuXZkaH7vGjAryCHzbgKuQllPMuyryoYELpEq3sPyEqqVpZlR+/fvx+8b4I3KG/S4p6zEmsyVeedwt4CWl0CQkeSJBmLESh3G0LSNP2rwoMonbcxpS5YVAZqxvWLDK2hVTp0jVrX/CZWSby68zaL1KYcnu5joTPqgVhcJluXNzUgqM22RKE33ptnq+TsORjp+kWGxjhiawQj0RgEIgGEpsHQQCCyykFCtF2IN27cyP/s9/vnCg/u3byCAaEDkQrw6Lgo4ENWUqUO6fhGHpRfKTzYALEmU3qvFdzv9/thU64CqOHCXr6hLkZJvrr1KES+QPBPNXsb1iIGI+n9BYwmWolW735Qm6oAQ8rvXzEWfZvAzutMnTa81fA+Buwkv4bQHxsB1fm+lD+pGOsedawuNKSox4tgZLfVvXHqoBUCkQBCUUDk4sWLdQlE5mKVgNTASl/a8RUqKz7LsrMmQolWHRcdEwV8Cg+iTFGDPerN2sBBuRNtcG4/py/WLSBRw4W9fOdrjMzIXq/nSrQ2bZyMBvMWQBpXuQaoWH4/UCWHkLQx0KviUGYf+RDy8VVXg5E2A3Gt8ACAYT1wmVERy2qy+apb7mizeU0D3Pm5GLoalFfq/d3Cg/UXdRxBMLLbGlMabXF6Qwt/xwsPAMAeqD/k0tKSC4jUIhBp3YxVAtILGobo0XdCJbHW1taCD9q8cjCNzJprkXxBPPQOQQ3KbbD5bwoPNke0yZQX/K1VQMIu7K0XHqhOlB2s7pimafrXhQeBiOihBOxZHowMvXnKu8c3sXf5PW2qUbUVANij/NoXer7jbSRk81XHJEnyf2sdoIYlq/NzMfQm18OHD7u/NnGDa9RxBMHIbmvEwNoGIk8VHgCAEWmXs0qWKuClYEKSJF+uWSk+1dO/rEGaV8oyCJVTVQlI9XkMsBNx3njlX0NRELaBDb7bKl/cC71A7pX7bGpWZC7WZMp7v/fc8yKi2mSG9Pv9XzYRzk9v0tjExefW6Wq/MwAjiZJd45VibuT9IE3TvIkW11Gg3rys7roF5/JrH71OEVAelPPmW63GOGJ7BCO7rfYfCAKRAEJ57bXX8uCZK92TJMmv1HSXUh4cjdEw8VfO0wAAIABJREFUXgFJ9QFUZmPhweEpSHJcE6fQu8ea3kuwZaKUwfQmtE0vgZmPoUJPprz3u47ByNpQyekYfT1adH62QtMn8QBKETujoamb4/L7WMT+tgAC8KoM1a1sab4JkQ0NCCgfz9fwvhSl4o63CZtxxACCkd316YX1yVp/IAhEAghBi8vvvPOOOXv2rOt7dt4OOOra/0XX5uu66Yeu0e5Ktfb7/XOFB4eXl3nVexqSJjrq1ZZl2U9YeG4vb0Lb9GAPi3wVU6Z3aC06PwEA++C1S2jqImL+vLuSgQIguHw+TjASodX1nIrYMoFxxACCkd1V64VeApEA9kuDnOXl5Twb0tY6XzPG/IYxZrkBb26eHalszpC08/Ljjz/Os4r2WApmPMuyNxQEWF0NextxmaBJknyj8CBKl2XZUa9sUDD6LKr/QAuOaD4493Y8BuG953G6xbdDtKxRe37+uPAAUBJXghgA9ikfp9iNmJ3hLXLXddMpAHQVAe5miTaOIBjZXbXd8U0gEsB+KBNSQcj33nvPzM3NucDH63ZxvynZLqtaENfCeOg+DSrVai0UHtzdiSRJDurfCLlzTLvPVaI1y7LHTe8l2BYq5xsjGGk+L7H5d4VvNk+UHY5eqSZsLw9G3rp1a9sf2I80Tf+J9x5V8DYLAaiYxt/qL89xaBavYgUVDgAAlWnRJuzgCEZ2Vy0zIwlEAtgLF8xS6VBlQvpByDRN/xeXadgkaZpeMhGyIxVIVGZjlmVfHbU3RZZl3zReuddQVI5WO66SJPkeO5mBevFK1QEA0BlPnz79R442AAAYldbc+v3+P/PGFRGM7KaHF9Yna7dTjEAkgFG4AOSlS5fMhx9+aJaWlvwSe40NQnpWtCNbgdXQwQAFJJX5pkzHwoPbm02S5KW1tbXg2VunT592f21CCV2gU7zsbHq5AgA64ZNPPjG/8Au/8L9ytAEAwKgYR2zvwLaPoM1qt5hEIBLAbrQgrubJ+vPo0aPPlC9VFmSapn+lAF6apq0py5MkyWVjzJL6KSrrMxRlNurfVKZjkiTDBmzzsq5emdcgdCxtSaW1Bjf3BgAAAOpipOonAAAAnmjjCIKR3VSrYCSBSKC7lPE32BNRwSljG1vrS4Eqr/+Hb91ez1oVgByg6+OSK0EbijIb1ZtqZmbmJdtLc7f7gnq0HVcPwdXVsLcQvTbrSuFBAAAAoDrjDW0hcMTYzAwAAFCZRo8jbt++XXhgvwhGdlNtgpEEIoF6OnfuXJ6FuB319xsMIkbmsubu2WtYV8oF6vVen5iYOKWgXcisRP1bKmtrjJkf4v3MsyJD94pUsNn190zT9GbhB1Apfc5DU++E559//tfSNOXgYq/yydxO9ygAQLdoTBmyjcCtW7fcOPlIQ+cd2khoHj16VHgAAKribZCY5SCgJvK5JeOIgkOF7wRCMLJ7atMvkkAkUF9a5LU3zp2s7fDYKAZvzHe8nUP0KPs8OzJKMFJB57GxsVM22Ljdbq3xLMveePLkSfASrQpESpqmFwsPomprU1NTvdDPQRPQmZmZFwsPNE8+gb57927QJ65JkLXd5xGf3yOC99I19vyMcd4DAKLRXKGnKiohFxFVDcRq9CJijIwGANgrNkighjS3PM44ooDMSARTiw8AgUigMZ7jUFVu9enTp387MzPzm8pGDVlu6caNG+b06dPGZkduVyb1RJIkBxWIDD15eO2119xfh+1biYbTAN9udBimPHCdRck48EpSt7X0dK0pc9c6RA9bAGiO0BtUvM1GszuMkeusR4lWAACGwzjiGSotOx1rHJEUvoO2q3xxi0AkAIzmwIED3zbPBu+C+OCDD/J/Jsuys9v9e1mWfdNEKNGqTE9bBvQ6WWC1tFmuJCRvQHuk4e9PlJ2CMbL92kilnYfI3h+ZyulYTT8/AaAr8o1NodtHaLyiDSpZln2l8GD9nTD0iwQAYBiMI4ryKlCxqisQjOyeSrMQCEQCwJ6saPFdZU1DBoeUpba2tmaSJHlpm74Ns3rs448/DlqyQk6ePOn+2sTd5l2Qb17yMvWC8II9W51vjZFl2e+bCAN0bxJEZuQO0jT9u+0f3bsWBcsBoCui9RFeXV3VGHnMBfca5IR7/gAAYEeMI4ry50swEkFcWJ+sbERKIBIA9i5N0zw70vVZDMXrAzm/xT+pXpJ5OdeQFHCxQZc1gi61lZeoDD0ob/gOQeeQC9KHRs/IoeXXjaNHjwb9R70JV9MmjK3kHV/uEwC2k18fQmc0mGeDeY26J2jDlMZaBCMBANhVlE3YprnjiPEsy74acxxBMLJb1qp6tQQiAWDf8gzC0KVaNcCwzbVP2drwjnqmHddjoQch9IpshDwYGWtxz+4Q3CoA3gTzJlLGgReMZAVxZ1GC5eoBarMjp11fUNQCwXkAO1nTImLoUue6z9tewoNj5DqbV693ApEAAAxtTesejCNyJ7RWE3McQTCyWyoZkRKIBIAgtBh7XX0W1W8xJC87csH7Z/O/v/POO0F/lwZ4yu7MsuwRwchai9I7weyejVt7/X7/awOvIxj1Qez3+z9u4vtSsmiZMK6XLtmRANAYUbLlzbPVQRYKD9aQ6/UeuqoJAAAtxjjCcuOI0OuAPoKR3VJ6MJJAJAAElWdHev0Wg3CL7y7IYkszvBGjNIN77kmSXC48iFpRUCxGsEelMG02bq+B2WfzaZq+qM+MsuhCcpOfNE3jNGdol2jBcnfNy7LsbOFBAEAd5RfuGIuI3/3ud115+XMNyGqYd2XkvR7IAABgZ4wjPpePI7TWsbGxUXgwFIKRHVJ2v0gCkQAQ3B1XQiLkQEkDDQ04FGSx2WoqzXBQmV+hAy5eVidZkTWXpunf6BnOzs4Gf6LeTrvlwoM15nYKxsiK9EqOUlttOFHK6eh6qIVcTcR0+hd+AABQN/l9M8Z4ReNgZTXY8vJXCj9QH+P9fv9P9WwuXrzICYoY8pLpoUvkA0ANRAtGNmkckWXZn5nIWZGGYGSnrJf5YglEAkA0+QAmdKlWN+BQsMUFXLSLKyQtEtnG4NddzzfUWrRBuYJ5Xq/SpgR8FlzGgbI7Q/MWUQlGDifa4vPVq1fdXxsVLAeAjlKQ5F2NMb3ey8FoPNyAMcuyNhXquZIViUjyMoahN4F5jhS+AwDleOA2unZ4HHFFCQl6rjGzIg3ByE4pbWGLQCQARHUzy7KfqO9iyIGSnw2kr7W1teCDkNdee839lazIZogW7JFLly7lf/b7/T8vPFg/4y5Iv7wcPj6lhR31i9Rnm0D90G6aSOengs26HtpSwk0Jls8G/mJREECT5GMWjY9DU1aDu/dnWfZeDcusqarJGS10xs5mAEK7deuW+xfrXr4QQLtFm1s2ZBxxqqxxBMHI7iglGEkgEgDic/0WQ/eO9LKB/EbbQShwqmCL+hCS+dUYUTMN1JtPAZ80TV9uQAbainYKanAeY6egm/QkSfL9woPYzp0syx7FyNw13vUwy7K/KDxYH8t6D4wxnxljPgz8Nc+ZB6BB8o1uoSuHONqkomwBjQX6/f5HNVpIPJJlWb6pSwudodsrACUiGAmgSnkwMsamJrP1OKIuNscR586dK2UccaDwHbRV9IVfApEAUJqVLMv+y7Fjx8YUHAk1YHDZQAo+hS5Defr06fzPNE0vFR5EnWlQflyD8hi75LRwpsD32NjYkh2r1DFQvaD3QGXPYu0U9HZgkjU8AgVvx8bGTmnxOXQfT10DlSHe6/VessHyugXM83H3kydP8pJ8CpKHCJTrvbTltDkXATSJ20B1XJtUYpRTV0UH9cubmZl52bZNqHrThhYQP1IfqvPnz0d5zUBstnShsRUZbvKGA6jIPVuqtadyrTFKnmscoTHK1NTUy3auVatxRFll3glGdsP6hfXJBzFfKYFIACjVgyRJvqdFeAUxQi7Ch17QN7YEpZ6nMniSJGGS2Sw6XtcUoIgRiFPwRAHJixcv6vx4P0mSL7ueNDVxwhjzrcePH0cpz2ps1nCv18tLtCZJUqfX3gT5+DP0ddDRMde/W8Ngef66dV5qo0fIiaPKadtrNecigKbJN1BpzBIrMKesAWXOT01NubWPBRsILdvmAuIHH3wQ5R4IlOHu3bvut1AeHkDVNMfqaT4Ua+7/5ptvDo4jqgpIbo4jlLFZ5jiCMq3dEHUxYXF6Y4FAJACULh8duYzDUDQICT0Q0aLQ2NiYspi+U9GCDfZOx+u6MqVi9Y5UuVYFOjUQrlkPBQ3Q/9LYoFSsnYKuFIwrv4yRrCqIq2BujFLCW/T3OFT4oXId6ff7P4oViPSu1ZQLBtBEK/1+/6eh+6r7dF/QQqK99p6qqGTrCT8QGWvBFCiDPksa02RZ9tu84QAqpgpkjzSO0Ib6GLYYR/yo6nGEMjbLRDCyG6Lt4l6c3lAE/1uFBwAAseVlJBQkitUzLRSvt+UVzopGitqHSRSM1EA4SZKXsiz7HzXYHT3rlyxRwDQW7by0KIu5By6IG6u/h46919/j/QqD5fM6J9VjVeVj9XkMHSCnXDCApkvT9NsmQl91n1tIdH2vsyz7B1tJIbZxO5b+gRufEIhEGyiTWeMsDUU4oACq5OaWZYwjNJfzxhFlXP9qMY4gGNkNUVbQbCDyWuEBAEBZ8pGDF8yoHQVKbf+xd20AFc0TNfvM0UDYC0h+VOGChMY3H7oBesySJS4TTdmnZA3vmTJh/kXXwVg7WLVbVOemJos2C6bMDMlxGxy89uTJk7HLly/nZQJD9Qp2/HLBNe3d2nozMzOm3+//uOvvA7BPV5TVoPtrrHuCsQuJyk53m1W0sGfLxMa6P8zb68MZ9djTIialWVE2ZR6rl1po3rlMdB1A1a7EnlsaO47Q7/DGER+WNY5QNnqV4wgFI3uF76JNPr2wPhl88ZdAJADUQilBov0g66sdkiT5holQFniQApJ2QD5mB+RlLkpsBn00QI8diDTPvp8svuzdgzRN/0pB3Zg7WF2w3O5eLSt794gNfp7S4rPOlxs3bhR+KASvXPB3Snhd2Eaapv+09SMAhqS+6pd1T9DGjdi0WUULerpGq1+lMebv7Vgi1GLivM2YuJam6YsaI2lsHasnJrCTNE3/zm6iC0pVKGy1h15JWcYAsB3NLS/Gnls6ZY8jNJ+N2Vt7GGRGtl/wnc0EIgGgPsoKEu3FQKbNTU6bRoveh8nRgPytt97K+8cYY5ZsH4XYgZ8TdqfgZtAndiBSi4k2a/g6WcP7tpklHnMHqxcs1y/5YeQg8oJfllWvLVbfUvPsxhHKaQNouiuu51MZm/W0oKeFPZWct2OXU3Yx8Y6u5SMuKI7bYEzet0rrLqoYoZKwuk5rjBQ6Mx4YQV7FI8bnyvUss/3aq27XAKDb8nGE1gQaPo74Z38coaCn5rNVjyMOFL6DtrkT8vUQiASA2rmZZdmfzc7OHtQifJ0WKNxOMld3H82WpumfaAygAWzs4Ld2SGvRTb9rZmbmZRv4uW6DPyEDd7P23+ylaZoHmjQJiP050meVrMigdE5cHxsbO6Xrjo5hLFosU1BQGTdjY2NLWZa9YTeFhMr+PmIDgr0nT57kryVWNqRDuWB0WFdKEi+EXheouQf2uvytMsYsjq7XGkeo/67uRVNTU9N6DvrSomaSJB/b47DVdVa9qv+9FgzdN3QP0HhI94CYm1GAEej8Pa5SrRsbG0HfNy3GqyrJ0tLSWL/f/+923sEGqZbxyvyyERN1tjmO0JyvjEoLZvdxxOMkSW63YRxBMLL9gk2wCEQCQC1poPSdsbGxM1pQjr1oPQo9HztookRrO6w8ffr0azMzM7+pAbIGtjFpkUMLiDqP9OfExMQpu0twzQZ+bm4zEN/NuM2EPKesM/2sdgpevXq1tHIlej02+PM2k/FglEn41ddee21M5WdCL5L5lDWrc0UT016vpwnftSzLvmk3Xqzs8byctf1KdY7n5+TFixdLmTgSGEeHdaVlzXjhO+2nnk9/pA1NZYxZHG1m0j1CX8qm0O9W//SjR48eHBsb6+10ziVJkl/77969m99jynrOwAjyTQ0KKMU4P11VknPnzv3i2NjYt/r9/ltpmn7bjvm7tKGitbwKJsx/UHf5OKLX69VlHDHWlnEEwch2e3hhfTLIDZtAJADUmnaNntHuqboEI71Mm++RadMeBw4c0C7lDxWE0QC3jExcNxj3gpJuEH7NBiZX7de9bSa2h+zXrP3KB/DKhNRgXZ+ZMgfqmkzos2qzJAj+hKONGZeUrVjGDlYFO/U7dDxV8mZmZuYlt3PVGLPuLZw9GNjBOu6VH9P5eCTLst+2pV/zfiHaFRu7TLCjz1UDygV3MZCCiOpY2j4Gvc6uvNatpGn6n1RZQdmRutaVXT1E9wmNMdzYXIvwXlbQM7TxhPKraIB8ffGVV16JVoXCbfjStWtubu5FtWywX8aO+80OSRduvLXd4wAwtDRN/1hrH3UZRxi7lrAVzSFjbsYNiWBkuwW5AROIBIDay0sUKnOszF1bO/GafRNsaRedXG9PTEyc0SKB6+9SBheU1EKeJgM6173A5NKwT0EDdX1G9G+VXa5EC5GazJjPdy7+IYH64JZVNlXZimVdC92CmXau6rqnCaItqTNd+OFtqISO/h2dk2Vev+teLtj7fB6h7zCAPVBg4m1VD9G9t6wya9vRImZZFRiASO5lWfYTbcCK2R5EC+r6zCrg6bKCNP63436zU2aQ+bzvpDb8/S5BSQD7tFqncYSxc8+mIxjZbvvOiiQQCQCNodKAp9Rnr+pgpJsw2t2rlGBpn+V+v/8HJ0+efLGK8h8KUCgIqi8FgHSu6evw4cN+6Z9NWihRqRL9f/qqcsegzezUX98luBKHDfKWvoNV55ULzrvsF7dzVRkEvlu3buX/5c7Hqnp4KHha56xIsoQABLDQ7/e/ojJrGiPXqZ0B0ERJknxfFXkUJIxdxWGrrCCNr7Ya7xs7B9XcYG5u7mCWZe8lSfJLhR8CgNEsM44Ii2Bku+1rdZBAJAA0ymq/3/+xeuNoklZlg2oFACx6RbbTgzRN/6ML+GhQXlWAT79XX03oq6TPhYI//X7/p2mazhd+AKFUvoPVZb+4nauxSpnthxbrFBy3fX0XavcEASAQlWvNsuyjs2fPjmkzSJVjZKAFNL87o3FtWSXlfTt9fv2MIQUkbTl8siMB7McDxhFhJW16MXjWhfXJPd90CUQCQPOkaZqn5Sg4VBW7GzUPuBCMbDWNMc6rL+jFixe33aGMz2mDgAuKpWk6R3nW6LSD9ce9Xq/S62GdeeWC/zPnI4CWu5Mkifo+5ZtDtuvbCGAod+wG2HzeV0feJslDHFIAATCOCIhgZHut7fWVEYgEgMZaUY8MBQOrCg7pd5vPAy7fLjyItlE0410/0IYifRY1aVHg1hjzeogy+tiV28H6+OzZs0wYBygjUouIT58+/VtjzJXCDwBA+6zYrPl8MwabqIC9cxtgvb7TteJlSBKMBBAK44hACEa2156yIglEAkCzJUly2dheYFXwSrSywN0N89odrSC0y7TCv9Ek5erVqy4QeZ1s4VKxg3ULeh9cedYDBw78x+JPoApedglZqkA8Kkl9XddB3ZtZSAT2bEVVcDT+Z3wFoEMYRwRAMLK9Rt51TyASAFohDwJWUZpQgciJiQljgy4sqHaDMtC+7DJy67pDugouEGkXafSZoE9k+djB6nFZuubzjSsK1N4r/BCGkc+zjh49GuzNsvdOQ+Y0EN08C4mju3//vvt/jjToaSOiNE3/RP/6W2+9xdsMoEsYR+yBN444RDCyvUbKjCQQCQCtoSDgdS2+e1mKpfB+H9lf3fIgSZL/TZlWCkZSsrUQiHyXQGSl2MFazNJ9m+v0vrDZBmi2ZxYSyezandeDb7zGTxPlWlG5d5V9pz83gI5hHDEiv5cvwch2Wr+wPjn0JJlAJAC0Tp4dWWapVg3ANBlVyc69lgpHo6kkpjIkH+u863LJVpVbJCOydjq/g/XSpUt+cHyh8AMA0C2b9wVljIfMdAa6QuXe6c8NoKMYR+wRwch2GnoRmEAkALSSyrytaWBU1qDI7YhN0/RS4UF0hQtI/kQlWxX86FrQR5+5GzduDAYiyaKqh87uYNXmAG+zCMFxAPjcvCvlrftCW7K7NPby+tACMd3z+3Nz3gHomNaOI2Ku4xCMbKeheo0QiASAVsuzI8so1aqByuzsrFHfQEr/dZ4Ckr+uoEev1+tU0EeTDwUivTKYBH3qp1M7WHVt1jmpzQH6TKq/K8FxAHiGMsVfd9ldTQ+o6L7mrvtASVZci5CLFy/SPw1A1xTGEU2+Dmoc8f7770ddwyEY2U67ZkYSiASA1rvpMtRiL6oo4KkJaJIk3yk8iC56YIMem0GfsvuXlkmTDWWBavKhSYgmI5TBrDWNgc+3bQfrIL9cMIFIANjRiio76FqpLHIF85p2b9BYRD27dd2fmJgoPA5EtrnZy6sQAgBd8cw4QsG8Jo8jNE8+fPhw4WdCIRjZPp9eWJ+8t9OrIhAJAN2QJMllU0LvSO/fv1J4EF31wC5MfP3555//l6WlpVaWbVVGsCYbygLV5EOTELKDG2HZ38HatnNT56VfLphAJADs6k6apv/BbVbRvUHX0SZk0GvDl56rxuO2SgnlMlGFPCCpYLg2ImosAgAdUhhHNKUSj8YRWtPw1w1ttacoCEa2z44lWglEAkCn5EGRmJNB/dt2B/a76htS+AF03ZU0TX/LlW1t4i7BrWiRTwEslaNyZVltwGfHcRhqZXMHqzs3m57B63a0euflefqWAsBItFnlV13vdWUI1HUxUc9Jz00bvuxY/O0kSX7FEIxEdfKNiK5kq3pWU7YVQMdoHPEbGkcoS7Ip4wiv1cyr+guZkRjFtiVaCUQCQOdoAXpNCxSxFiW8xXuyIrGdzV2CypLULkEFfprYr08LKqdPnzbvvffeZjakHbAvEPBppGd2sGoipglZE8uLuWxILzPmVTsZBgCMRpvrtJPvVVdyTYuJusbWYdOKWzzUc9Jze/r06d8yFkGNaE74qmsX0paNiAAwgjt2HPF7uhb6Qck6jiO0ZuiNI/K4UsyNJAQj22fLYCSBSADorPy+EKN/jAKcCshogLXd/QfwLKdp+rKyaHU+1nmX4CAXhNSCiv60wZ6v20AW537z5TtYtaDr+oVpN38TMkvcZFIZCPY6f91mxnBeRkbmEdB6q/Y+/3suU1KbVlZXV/N7RJllKDUOUUBH4xC3eGg3RP3egQMHfotrPmpmNUmSX1eWjStX6MbQ3DsBdMhNOy973c0z3ThC1WzK3AC7wzjiVRs4LW0ccaDwHTTZwwvrk4XyYAQigdbKd77qpvLo0SOOMkrnasq73pTAEJRtcMIOeJdnZmZ6Ggzfv38/D6hoYF6n65kmCBq0a2e3qMegMeZSkiRXyD5onTt2QXc+y7Jvzs3NvaTj/sEHH+STttu3b9fq9SoI+eabb7rdrPlkMk3TP27TgrRXHqhun7Wbxpglt0EBQOvdtF9HlDXwwgsvfHVubm5M94jHjx/nYxfdI/S1sbER7L1Q0EbXegU9tfnPs2Y3eBGARJ09sFk2GjMvT0xMnNJ9U19ra2v550WfnZCfGQCoqZUDBw6s2DWQeY0jTp48Oab1NK2DuDFEV8YRBCPbhUAk0C26cRzXgF69y4AyKQiuEhMKziRJssKbjxGt2sG4vhYmJiaOa5egvhT80eKEvqqgQbsG7JocuIxiZUIq6E4QshNW7DXtmaDkJ598snluVrVwts25+ZMkSb6RpmnrrsNe1lHdFtzv/Ou//uvPJiYmvqj7IAHJ+GL2rWkzMpCC03rLfJIk83Zj1Qk/MClaVNT94u7du/mf2mA1zGYWbX7S2FqLhjrf9d9+VRN7rf+O7QdPj3b4n++6j0vv2V6Sy3Zs9Uav13tJC+PKmPQ/M/qs6DOj/waAFtKcRpnjC24c8cUvfvEroccRLghZ13EEwch2eSYYuTi9cYRAJNBquoksnzx58gu73aB0Q9tu8VQ3qsESnrZ5Mdph1p0DIWmR2J4n3+t6cOb/b+8OYuO6z8SA//2egMCKhAjoIRDdrBWgUC5pydC6tVvS2T0astLsxQK2ktvAuuzWiuUTDyuqB56sgEZPNnYbCwXknrqyrWsRCW1vllaD7iWCgUgbhEYOxcorI7l4xsU3+j9mxCEpinqPfDPv9wPGkvmGFN/jzPCb//f/vi+eP7EpoO6v2RE38u1YJCX7/f6fvfLKKy9U1QbVrum6dwmOqoL26rahXcrNkeTUxIhqU57ZelIybsePH1+IRbO4VYnJTz/9tPEFs3g8njhxYlidu8ljc7Uoimtjn7TP4jn1NG2HNsYvVUVyNQvtwIEDYxsu99s3vvGN/55SejOSkZu9Nm0Xd42q2lRLtm0vnnfsv6qqiaFruf3a2ZHNVYvf/va3Xzp69OihDRUI6+J3RsQ3Kb/WPeE9183878TC5U5fB4+MfaTl4ndpVzR0rq37HbmFWPxeLopiOVcZV8+ZP9nsORPPkyrGGr1u1eJ8kLhsXrxG1TlSYy/inUl8H1Qlj+rQtZgy3idlk9Qt4EFez433m2k0jjh69OhCrAVtFUfcvn17/e8NxRGNi2Tk/ZTSi235hngm65ntnIjUtgOm24O8m+YXFgbYQixILOx0UfRpVC1a8y7XTmsiGdlBEcOcL8tyfZfg888//6cLCwsvVIF4tSgRCxLVY3p0QWInNu4YjP/fJGlyc6Ql26RVH9zL339TX7uLPsi3SJif6vf7//H48ePfr5Ijo4/L+LN6XO5G9biM15R4Y73xDWZuxfo3bX5s1rnQGhXJBw4c+IuxA+2w3O/3/2R+fv77kv+N8n62Hs/6evHA75ZtVRurUl5UPJaTLXM5Fp+rPjk2tlR/7/f7v0kpfZb/90FOKt3Lf+520XD4b0VFRZ0idmpAkwujbXuEY2AnAAAWmUlEQVRcdelcd6J6jK9u8pwZPm8OHjz40vz8/DAIqlrSP0mV7H/C4vwzi++nK0n0iEUnKM5p8n1Qo5uvY9NhVRk37Zp4/uSqv7GPT5AbG2LeuZHXxfU4ot/v/7N471Hdqck4ou7ROSNrLg8O5G9SMnI6DB9oI4nIb3X9gkAHxHP9u7lyYztz2+yUvdP1yrYpFomd2lvIjVSP3dQqajjwm3oNE4FlWabRndPPP//8ifn5+Re2WpAY3SU4KhI7T6g07Y28Abgx4a+HVeKM+sVr3WpZlqtVYjIelwcPHvzh/Pz84c0el1s9JkfFgtkmCfGh/Abz02pHa1mWbX693fiGd6dJpMcW6kdcy5WpbX0+PijL8o9z/LVZfLVd3DXq3obfo5Jvj9u4OMP+uFN12mBHquf1tpXrOc5pRIOLiHU+H8+PfWR6delcd2PsOTOSWBiNEzbGDMfybWg02d+gppJdbaxwvTT2kfo08bt9Et8H3WvwOrcxfmrk+dOmqr+aVMnEx+KIjXFDQ3HE8P1Lg5ua7hzIJ7cXL9g0745EJPvIQsH+uac6jc30+/2fRIAS7QTrFG3pMkkPr31NW985nYPtIxuqDYaLoxt3CY6KuabRCTJ/6N6Gm58fuzFMTG6xo3+4MNbv9/9FJM93+LWrN+bV5qB4XN5refJxoy4utD7IjwOAthjGRVpXMkUebIjXt03yZ9X7hSbiqC5tzLDG1LyureXZ2NR+ww0fdY95GnUgv5C/OXaESfNFXvyQiGS/TNtOFJh0Z8uyfCESkXW2aI0WgtFCJCp2yrKUjGSvPdisUma7XYET3rKFyTC2o3+7xyQANGS4iFj3eIaRGWRd74jCZNiYwARgZxqJI0Zmot4pVnozNxosM2fvPJCIZB/dX+nN7GSHGrBHvvrqq5+kBlq0VrMMyrL867GDAADAvhgMBj+sZhnXaWQGn2QkAEypfr//p3Fmt27dqvUER9q0DmdGptxWRyJrspn7yX4yAwHaZfHAgQP/OuaV1R1EjLRo1ZoOAIA2GauGGgwGLxVFcehJ32O/3//7siz/39iBR1XvbZ5hW5kriuJw3bF/OHHiRPVXyUgAptXcdutcT4gntiv0q9rxtv136Fx0V7t5c7tT2Z2RDgvDmZFppTcTswYXJSSBXXhdVSS0ztnUQFVkJCKPHj0af70yAQsyAAB0yGYLhZ999lmKasEn2Wr2dEppYTAY/OeiKP79DufV7ZfhBuEbN8bysc/s+PHjwxncRVFIRgIwrWL82MJW5/bb3/5201mK0T3g+PHjW35ejiN+XBTFX+bNTW01XEdsMI54WBTFemXkaEIyLsrs2GcBPO5+vFDlVs9Ae8T84DMRJDWRjMzMigQAoFWKooiywIWTJ0/WMu8oYt8LFy7EQmP0F/vb2Ijb0jj4SCx0/u53v6s9/o958blNa/0llwDQIrlLwvdHOgI8s9OnT6e33norfpH+PFdHtnEdPeKI/xBxRN3JyJE44nb8pxg9GAnJld5MlKT+NKX0xdhnAzx6bbi00ps5JhEJrdRIVWTsZJqfnx8GZy0NngAA6LZhjBpxax0ino6E5Icffjj8aoPB4L/kNm5tcz4qQq9evVr7tzVyLcX/AEy1siyHG29eeuml2k4zfjdfunRp+PfBYPBJLiBom4gjDsf3+vDhw1q/tZFrOYwjirF7PEpKruYL8+7YQaDLojVjJCGXPQqgnQaDwVvRiqpaNKlL7OZKj4Kzy370AAC0ULRYq3URMRblLl++nH72s59F5eWhwWDwcVQQjN1x/xzr9/tvNxH/p8ev5Z2xgwAwXW6kmuOIlDc3vf/++8M4ot/vf9K2OCKldHEP4oitk5HpUULywUpvJnrOf/cJQziB6RevAT9Y6c1EW1Zz4qC9zsZupmirUOdupsOHD6fFxcVhj3ctWgEAaKlGFhFTrmy4fv16LCR+p9/v/6+2LCTGomZZlt+MRc66qxlCvAfIVEYCMO2Gv+tGfvfVJn5P3759Ozb4x4zq1bZcx5wcTU3FETtORlZWejP3Vnoz8RN4Oc+IA7ojnvM/iteAaOPs5w7t1u/3L6QcRNQp2lNFj/eiKP6rhwAAAC0VG2d70Vo0ZhTVbXl5eZiQjIXEliQkP4jvJRY3m2jRGtfw6NGjKW9OtikZgGl3L0YTRRwRm/LrFnOo7969G1/1TN7oP9VxRFzHHEd8VH3sicnISsyGixlx5klCJ4zOhbzmRw4TYbEKItbW1mr9fl977bXqr63ZvQUAAJsYdvFooqohbUhIDgaDf9jHGZJxnmc+//zz4eJmE1555ZXqq1oTAKATyrL8n6mhOCIqD8+dO7eekNznjU3DOCK+l6biiChsyNbjiB0nIysj8ySvjB0EpoG5kDCZzqbcQqpO0VJhZCfTPY8NAABabLjgNZJIq10kJGOuUoxHSCn9XXxoDy/HkXyOZ2K+UywgNtFWLW2xiAgAU67RTU1VQrJq2ToYDH6dUjo1dsfmHMktU4eJyPhemoojRq7h7pOR6Q/zJGPR8wfmScLUMBcSJtexamd0zIus0+nTp6uvZlYkAABtt95irYlWrZXLly+nS5cupUgIppQu9vv9/xvrbmN3rNfZXI35aiwgRrIwV1fUzoZEADrqTiQIFxYWGosjIvn3xhtvDEcsFUVxKKX0tzlBeGzszvWq4oiFphORG+KI9TzDrpKRlZghl+dJ/sg8SZhY5kLC5BtWRX7yySe1nkgEXhGA5Z1adkQDANB6ZVn+TWq4OjLl2Ds27lXVDSmlX+TFxLqTkme/+uqr/51S+nlUY0ZVZpMLiOnxqkgbEgHolKIofpb2II6IZORI29aFlNKv8u/dxuOIiF+ajCO2Kmx47uuvvx67824tza5Fa4rzKaVvNXYmQF1iLuSqdqww+QaDwT9FQPHyyy/XGkxE26c8L/Kn5kUCADAhogXZP0bVYlNt1jaKCoCInaMiMz2Kz39dFMX/yItwu9n0G9/4qX6//2dlWb4QH4ik53vvvZdu3bo1duc6xYbEjz/+uDqHP2r0HwOA9tnzOCI2AUW1ZK4mTNHlIW+uulFnHPHOO+801lWhsl0cUWsyMj1KSB7JC5Znxg4CbRFzIc9rxwpTIaoif379+vXh/Jo6RcvXgwcPflkUxXdG2yoAAEDLRRLwTLRSrbt7yHYiKRnVANFdpDIYDB4WRXE7Lyje26Lt6Vxe/FzM1RHrIs6Pc2g6CVmJ9xS5GuR1lZEAdNS+xBGRlIzb/Pz8+scGg0Gsy93KccSDLZKTizmOmGtzHFF7MrKyNLs2l5OSC2MHgf0ScyGXV3oz9Q6VA/ZNzKeJtlCx6FHn7qYIfi5evJjy5oWzY3cAAID2irlLv4qZ6iMtR/fM4cOHh9UUcYsE5aFDh3b8T8f3HAuGcYvNgU22Uduoqmbo9/u/Kcvyn4/dAQC6YV/jiPh9XMUQkxRHRIeIq1evbhlHNJaMrCzNrp3KSckXxw4Ce+V+TkLa1QjTJTb+/F20Woh2DnWK4CG3mfrBFruuAACgzYZVDTGTKW77KRYVo/Xa9773vU0XFGPhcG1tbbi5cC8XDTeK65SrMcaqGQCgY1oXR1R/bjQpcUTjyciKeZKwL77ImwFWtWSFqdRI24hqJ1Oupt6bBvkAAFCvYzFz6fe///03o4tILNKxtajAiFlSeU7Vv9zyjgDQDeKIp1B1WPvqq6/+z4EDB/7NZp9ZjH2kISu9meVc3nqlFVcHpl881+biuScRCVMpesGfiYHadfevjyArG9vFBAAAE+JeWZbvRCXihQsX/My2EW1lq/nzZVn++db3BIDOEEfsUFRsVtfowIEDf7HVZ+1ZMjI9Skg+WOnNnM0t326O3QGoQzy3Xo7n2kpvZrPB+MB0iG4DVQVjbWIhIgZNR393yUgAACbcclQ1LCwsjG64Y4PLly9X7WMvGdEAAOvEETsQG5p2EkfsaTKystKbubPSm4m2bz/Ks+yAZxfPpdfjubXSm7nhesJ06/f7P4kTvH79eq3n+dprrw3/LMvyr8cOAgDAhIlKv8Fg8OVbb71VzURnRFQyxHynWGyN9UTXBgD+oOoY8MYbb4gjNvE0ccS+JCMrK72Zayu9mWM5Y/rF2B2AnfgiP4eiJasqJuiGU2VZvhCJyLp71keP98zrCQAA0+BOURR/Gefx/vvvD1uJ8UjE/rEZcTAYPCzL8o9dFgAYE5V+r0flX8QR0VGMR542jtjXZGTFPEnYNXMhoZuGLVrrnhW5uLiYjh49mvJrizbPAABMi9hodyUWEt955x0LiXkB8eLFi7GA+GVRFP82pWRNAQA2tx5HvPfee+KIXcYRz3399ddjH9xPS7Nri7mcc6FV3xi0S8yFXNaOFTopNu/86vPPPx+tYqxF7PCK1goxdzal5PUFAIBpE4uJZ+7evZvefvvt2ruMTIpqATF7XVcUANiR9Tji3Llz6eHDh528ahviiBjFeG3sTptoXTKysjS7djYnJV8cOwjdFS1Zz2vHCp02DHwuXbpUa2VktKv6+OOPY0fTr4ui+KOxOwAAwHQYxtNffvnlcP5TLCh2iUQkADwTccQu44hWtGndTE62zJknCeviuXBMIhI67chgMPhxBDw3btRbuBg93kNRFH81dhAAAKbH2arV2tWrV9Pp06c786NdXl5eb6mWKxmsLwDA01mPI6LDWN1dy9rsWeOI1lZGjlqaXTuWqyTPjB2E6fdRroY0vw2IgOfnH374Ybp8+XJtFyN63UeV5cGDB6PP+3fMiwEAoAOGsXWc5s2bN4fx9bS2bY0uKDEr8/jx4+nrr7/+p+eeey5GI90ZuyMAsFOPxRGRqJvWtq11xRETkYysmCdJx/RyEtLcNmBoMBj8QyQLT548WetCyUiLhSs5mAIAgC6Y6/f7/60sy+9H95GolIyNf9O0mBiVn9FGLio4Yr00pXTK5kMAqMXcYDD4ONbqIo6ISsmIJaZJnXHERCUjK+ZJMuXMhQQ2ExtyfnH79u1hEFCnCJRid1NK6bspJVXYAAB0zXK/33+7LMtvfv7558OEZHQOmeSk5EsvvZQuXLhQxfnhpyml1bE7AgDP4kis5aeUhrv8I46IpGSMV5qyOOJSzsnt2kQmI9OjhGT1Q47bt8buAJMpntSrK70ZuxSBjYYDst9+++1a50VGcPHee++lvLtpcewOAADQDY+NCIoKh0hIRmJyktq3Rnx/7ty5ND8/X33oZu5+YtMhADRHHPEEE5uMrOR5krGz69WxgzA5zIUEthMbcP4xdlfVPRh7pCry5ZSSttAAAHTdcDFxMBj8uCiKYU+yu3fvpuvXr6dPP/10+Pc2WlxcHLZSqxYPB4PBr4ui+E8ppWtd/4ECwB56LCmZxBHrJj4ZWcnzJCMpOTt2ENrrfuwsMBcSeILhUOzYTXX58uXt7/kUYrj2K6+8klRFAgDAmCN5LtL50bWm2CAYnUriduvWrbFP2kuxqTA2K8YC4tGjR4f/cr/f//uyLC/nzioAwP6YyDgiJyH/qok4YmqSkZU8T3JV61ZaLuZCLq/0ZsxrAHbiTgQuJ0+eHLZ2iJYJOxVBzsZ2EKO7nQaDwcOiKP6Vtk0AALClY3nz3qnRzlzRgi0WEuO2F9UOMzMzw/cC1a1aOMyu5IVDm50BoF22jSMiMfnLX/5y6uOIqUtGpsfnSV4cOwj7792ciDQXEtiJCFh+1cSVyrum/zwnOwEAgCerKh0WB4PBvyuK4nD1GbGoGAuJkZiMv8fC4sOHD596cTEWC2OBsPrzxIkTw+qFQ4cObbzrR7l9WtysMQBA+z1VHBGetoJyszgi/tyQfEx7HUdMZTKyYp4kLXMzt2RVfQQ8jbP5NurOUwQJcznQGXVvJNgAAAB2by5XOyz2+/0TZVm+sN1XikXGWGDcaItk47rBYPBlURS3csXCDRWQADAV1uOIwWDww9Hk5GaiA1rcNpqEOGKqk5EV8yTZZ+ZCAgAAQDccyQuL1abAY/lWWdjsKuTxCbdHPnRj5M97xioAQCfsKo7o9/u/Kcvys5EPtS6O6EQysrI0uxatW5fNk2SPmAsJAAAAAAB0WqeSkekP8yQjIfnm2EGoTwx8PW8uJAAAAAAA0GWdS0ZW8jzJD7Yqa4VdupmTkHdcQAAAAAAAoOs6m4ys5HmSkZR8cewg7Nz9nIS85poBAAAAAAA80vlkZMU8SXYp5kKurvRmll1AAAAAAACAx0lGjsjzJFdTSmfGDsI4cyEBAAAAAAC2IRm5iaXZtbmclDRPks3EXMjlld7MjU2OAQAAAAAAkElGbmNpdu1UTkqaJ0nKcyEjCfmBqwEAAAAAAPBkkpE7sDS7FvMAz5sn2Vlf5KT0qpasAAAAAAAAOycZuUPmSXbWR3ku5L2uXwgAAAAAAICnJRn5lJZm1xajVad5klOvl5OQ5kICAAAAAADskmTkLi3Nrp3NSUnzJKfLFzkJaS4kAAAAAADAM5KMfAa5det58ySnxiVzIQEAAAAAAOojGVmDpdm1Y3me5KsTfzLddDOldNZcSAAAAAAAgHpJRtYoz5OMpOTs1JzUdLufk5DmQgIAAAAAADRAMrIBeZ7kqtatrRVzIZdXejOrXb8QAAAAAAAATSpc3fqt9GY+SCkdyzMIaZd342cjEQkAAAAAANA8lZENy/MkIzm5MNUn2n4xF/L8Sm/mTtcvBAAAAAAAwF6RjNwjeZ5kJCVf7MQJt8f9nIS81vULAQAAAAAAsNckI/fY0uza+ZhXaJ5k42Iu5OpKb2Z5ys8TAAAAAACgtSQj98HS7NqRnJB8s3Mnvzeu5GrIB104WQAAAAAAgLaSjNxHS7Nrc1G9Z55kbWIu5PJKb+bGlJwPAAAAAADARJOMbIGl2bVTOSlpnuTu3M9JyA8m8ZsHAAAAAACYVpKRLbI0uxatW8+bJ/lULuXZkFqyAgAAAAAAtIxkZMvkeZJRJXmm69fiCT7KcyHvbX83AAAAAAAA9otkZEstza4tRutR8yTH9HIS0lxIAAAAAACAlpOMbLml2bWzOSnZ9XmSX+S5kKtjRwAAAAAAAGglycgJkFu3xizJix29BO/mRKS5kAAAAAAAABNEMnKCLM2uHcvzJF/tyCnfTCmdNRcSAAAAAABgMklGTqA8TzKSkrNTeor381zIa2NHAAAAAAAAmBiSkRNsaXbtfJ4n+a0pOaWYC7m60ptZHjsCAAAAAADAxJGMnHB5nmQk796c8FO5kqshzYUEAAAAAACYEpKRUyLPk/wgpbQwYWcUcyGXV3ozN8aOAAAAAAAAMNEkI6dMnicZSckXW35m93MS8oOxIwAAAAAAAEwFycgptTS7Fq1bz7d0nuSlPBtSS1YAAAAAAIApJhk5xfI8ydWU0pmWnOVHeS7kvbEjAAAAAAAATB3JyA5Yml2by0nJ/Zon2ctJSHMhAQAAAAAAOkQyskOWZtfOxpzGPZwn+UWeC7k6dgQAAAAAAICpJxnZMbl16/k9mCf5bk5EmgsJAAAAAADQUZKRHbU0u3Yst259teYrcDOldNZcSAAAAAAAACQjO25pdm0xJyVnn/FK3M9zIa+NHQEAAAAAAKCTJCMZyvMkV3fRujXmQq6u9GaWx44AAAAAAADQaUXXLwCPrPRmPkgpHcuzHnfqSkppTiISAAAAAACAzaiMZEyeJxkJxlNbVEpeydWQd8aOAAAAAAAAQCYZybaWZtfmUkpHqvus9GZubHd/AAAAAAAAGEop/X/v3BJVuTshzwAAAABJRU5ErkJggg==" style="width: 240px; height: 60px;" /></a></p>
<script type="application/vnd.jupyter.widget-state+json">
{"state": {"506cafd9063b4655b538097b800b5c7c": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_55b1a93b963c49aab91141adbf5e8275", "placeholder": "\u200b", "style": "IPY_MODEL_d03e0513e62c46108363287a2a0e77a8", "value": "Epoch 999: 100%"}}, "55b1a93b963c49aab91141adbf5e8275": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "68eabba3edea405fb5cf18d0d4570adf": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "7d9c33be4337451fa0f668005e6ceaf8": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_9a6ea0c5bee84047bd405c4559ac83f4", "max": 1.0, "min": 0.0, "orientation": "horizontal", "style": "IPY_MODEL_dc6bf3f0e6bb433ba62346801a81fc77", "value": 1.0}}, "9a6ea0c5bee84047bd405c4559ac83f4": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": "2", "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "a904bd1f2550454bbe04bac868ab813c": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_ab358b7366a84a88bfd37f925668a9b6", "placeholder": "\u200b", "style": "IPY_MODEL_68eabba3edea405fb5cf18d0d4570adf", "value": " 1/1 [14:35&lt;00:00, 875.19s/it, loss=0.68, v_num=0, loss_runner=-.533, loss_tagger=2.230]"}}, "ab358b7366a84a88bfd37f925668a9b6": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "c3ebd76328474e80a5fc3214fdbc80d5": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": "inline-flex", "flex": null, "flex_flow": "row wrap", "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": "100%"}}, "d03e0513e62c46108363287a2a0e77a8": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "dc6bf3f0e6bb433ba62346801a81fc77": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "ec750af199a94f8091c03c862ee4ce11": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_506cafd9063b4655b538097b800b5c7c", "IPY_MODEL_7d9c33be4337451fa0f668005e6ceaf8", "IPY_MODEL_a904bd1f2550454bbe04bac868ab813c"], "layout": "IPY_MODEL_c3ebd76328474e80a5fc3214fdbc80d5"}}}, "version_major": 2, "version_minor": 0}
</script></section>
</section>
</section>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../templates/img-classify.html" class="btn btn-neutral float-right" title="Simple image classification with Lightning Flash" accesskey="n" rel="next">Next <img src="../../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="text-transformers.html" class="btn btn-neutral" title="Finetune Transformers Models with PyTorch Lightning" accesskey="p" rel="prev"><img src="../../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright Copyright (c) 2020-2021, PytorchLightning team..

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Multi-agent Reinforcement Learning With WarpDrive</a><ul>
<li><a class="reference internal" href="#Setup">Setup</a></li>
<li><a class="reference internal" href="#Introduction">Introduction</a></li>
<li><a class="reference internal" href="#Specify-a-set-of-run-configurations-for-your-experiments">Specify a set of run configurations for your experiments</a></li>
<li><a class="reference internal" href="#Instantiate-the-WarpDrive-Module">Instantiate the WarpDrive Module</a></li>
<li><a class="reference internal" href="#Visualizing-an-episode-roll-out-before-training">Visualizing an episode roll-out before training</a></li>
<li><a class="reference internal" href="#Create-the-Lightning-Trainer">Create the Lightning Trainer</a></li>
<li><a class="reference internal" href="#Train-the-WarpDrive-Module">Train the WarpDrive Module</a></li>
<li><a class="reference internal" href="#Visualize-an-episode-rollout-after-training">Visualize an episode-rollout after training</a></li>
<li><a class="reference internal" href="#Congratulations---Time-to-Join-the-Community!">Congratulations - Time to Join the Community!</a><ul>
<li><a class="reference internal" href="#Star-Lightning-on-GitHub">Star Lightning on GitHub</a></li>
<li><a class="reference internal" href="#Join-our-Slack!">Join our Slack!</a></li>
<li><a class="reference internal" href="#Contributions-!">Contributions !</a></li>
<li><a class="reference internal" href="#Great-thanks-from-the-entire-Pytorch-Lightning-Team-for-your-interest-!">Great thanks from the entire Pytorch Lightning Team for your interest !</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
         <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
         <script src="../../_static/jquery.js"></script>
         <script src="../../_static/underscore.js"></script>
         <script src="../../_static/doctools.js"></script>
         <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
     

  

  <script type="text/javascript" src="../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <!--
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://lightning-sandbox.rtfd.io/en/latest">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pt-lightning-sandbox.readthedocs.io/en/latest/#tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pt-lightning-sandbox.readthedocs.io/en/latest/#community-examples">View Resources</a>
        </div>
        -->
      </div>
    </div>
  </div>

  <!--
  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pt-lightning-sandbox.rtfd.io/en/latest/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pt-lightning-sandbox.rtfd.io/en/latest/">PyTorch</a></li>
            <li><a href="https://pt-lightning-sandbox.readthedocs.io/en/latest/introduction_guide.html">Get Started</a></li>
            <li><a href="https://pt-lightning-sandbox.rtfd.io/en/latest/">Features</a></li>
            <li><a href="">Ecosystem</a></li>
            <li><a href="https://www.pytorchlightning.ai/blog">Blog</a></li>
            <li><a href="https://github.com/PytorchLightning/pytorch-lightning/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pt-lightning-sandbox.readthedocs.io/en/latest/#community-examples">Resources</a></li>
            <li><a href="https://pt-lightning-sandbox.readthedocs.io/en/latest/#tutorials">Tutorials</a></li>
            <li><a href="https://lightning-sandbox.rtfd.io/en/latest">Docs</a></li>
            <li><a href="https://pytorch-lightning.slack.com" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/PytorchLightning/lightning-sandbox/issues" target="_blank">Github Issues</a></li>
            <li><a href="" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Stay Connected</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/PyTorchLightnin" target="_blank" class="twitter"></a>
            <a href="" target="_blank" class="youtube"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>
  -->

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. Read PyTorch Lightning's <a href="https://pytorchlightning.ai/privacy-policy">Privacy Policy</a>.</p>
    <img class="close-button" src="../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pt-lightning-sandbox.rtfd.io/en/latest/" aria-label="PyTorch Lightning"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pt-lightning-sandbox.readthedocs.io/en/latest/introduction_guide.html">Get Started</a>
          </li>

          <li>
            <a href="https://www.pytorchlightning.ai/blog">Blog</a>
          </li>

          <li class="resources-mobile-menu-title">
            Ecosystem
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch-lightning.readthedocs.io/en/stable/">PyTorch Lightning</a>
            </li>

            <li>
              <a href="https://torchmetrics.readthedocs.io/en/stable/">TorchMetrics</a>
            </li>

            <li>
              <a href="https://lightning-flash.readthedocs.io/en/stable/">Lightning Flash</a>
            </li>

            <li>
              <a href="https://lightning-transformers.readthedocs.io/en/stable/">Lightning Transformers</a>
            </li>

            <li>
              <a href="https://lightning-bolts.readthedocs.io/en/stable/">Lightning Bolts</a>
            </li>
          </ul>

          <!--<li class="resources-mobile-menu-title">
            Resources
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pt-lightning-sandbox.readthedocs.io/en/latest/#community-examples">Developer Resources</a>
            </li>

            <li>
              <a href="https://pt-lightning-sandbox.rtfd.io/en/latest/">About</a>
            </li>

            <li>
              <a href="">Models (Beta)</a>
            </li>

            <li>
              <a href="">Community</a>
            </li>

            <li>
              <a href="">Forums</a>
            </li>
          </ul>-->

          <li>
            <a href="https://github.com/PytorchLightning/lightning-sandbox">Github</a>
          </li>

          <li>
            <a href="https://www.grid.ai/">Grid.ai</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })                                                                                         
  </script>

  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PQBQ3CV"
  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->
 </body>
</html>