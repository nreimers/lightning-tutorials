{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28b270f4",
   "metadata": {
    "papermill": {
     "duration": 0.188189,
     "end_time": "2022-04-22T04:14:55.596739",
     "exception": false,
     "start_time": "2022-04-22T04:14:55.408550",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "# Multi-agent Reinforcement Learning With WarpDrive\n",
    "\n",
    "* **Author:** Sunil Srinivasa (sunil.srinivasa@salesforce.com), Tian Lan (tian.lan@salesforce.com), Huan Wang (huan.wang@salesforce.com) and Stephan Zheng(stephan.zheng@salesforce.com)\n",
    "* **License:** BSD 3-Clause \"New\" or \"Revised\" License\n",
    "* **Generated:** 2022-04-22T04:11:41.774547\n",
    "\n",
    "This notebook introduces multi-agent reinforcement learning (MARL) with WarpDrive (Lan et al. https://arxiv.org/abs/2108.13976). WarpDrive is a flexible, lightweight, and easy-to-use open-source framework that implements end-to-end deep MARL on GPUs. WarpDrive enables orders-of-magnitude speedups compared to CPU-GPU implementations, using the parallelization capability of GPUs and several design choices to minimize communication overhead. WarpDrive also prioritizes user-friendliness - it has utility functions to easily build MARL environments in CUDA and quality-of-life tools to run end-to-end MARL using just a few lines of code, and is compatible with PyTorch.\n",
    "WarpDrive includes the following resources. code - https://github.com/salesforce/warp-drive documentation - http://opensource.salesforce.com/warp-drive/, and white paper - https://arxiv.org/abs/2108.13976.\n",
    "\n",
    "---\n",
    "Open in [![Open In Colab](data:image/png;base64,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){height=\"20px\" width=\"117px\"}](https://colab.research.google.com/github/PytorchLightning/lightning-tutorials/blob/publication/.notebooks/lightning_examples/warp-drive.ipynb)\n",
    "\n",
    "Give us a ⭐ [on Github](https://www.github.com/PytorchLightning/pytorch-lightning/)\n",
    "| Check out [the documentation](https://pytorch-lightning.readthedocs.io/en/stable/)\n",
    "| Join us [on Slack](https://www.pytorchlightning.ai/community)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe7f739",
   "metadata": {
    "papermill": {
     "duration": 0.453122,
     "end_time": "2022-04-22T04:14:56.502329",
     "exception": false,
     "start_time": "2022-04-22T04:14:56.049207",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup\n",
    "This notebook requires some packages besides pytorch-lightning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39c16775",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2022-04-22T04:14:57.355607Z",
     "iopub.status.busy": "2022-04-22T04:14:57.355041Z",
     "iopub.status.idle": "2022-04-22T04:16:27.521729Z",
     "shell.execute_reply": "2022-04-22T04:16:27.522133Z"
    },
    "id": "LfrJLKPFyhsK",
    "lines_to_next_cell": 0,
    "papermill": {
     "duration": 90.617034,
     "end_time": "2022-04-22T04:16:27.522450",
     "exception": false,
     "start_time": "2022-04-22T04:14:56.905416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Cannot install rl-warp-drive==1.5.1, rl-warp-drive==1.6, rl-warp-drive==1.6.1, rl-warp-drive==1.6.2, rl-warp-drive==1.6.3, torch<1.9 and >=1.6 and torchmetrics==0.6.0 because these package versions have conflicting dependencies.\u001b[0m\r\n",
      "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/user_guide/#fixing-conflicting-dependencies\u001b[0m\r\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\r\n",
      "You should consider upgrading via the '/usr/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "! pip install --quiet \"ffmpeg-python\" \"torch>=1.6, <1.9\" \"torchmetrics>=0.6\" \"ipython[notebook]\" \"rl-warp-drive>=1.5.1\" \"pytorch-lightning>=1.4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e368bb",
   "metadata": {
    "papermill": {
     "duration": 0.043212,
     "end_time": "2022-04-22T04:16:27.610660",
     "exception": false,
     "start_time": "2022-04-22T04:16:27.567448",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**⚠️ PLEASE NOTE:**\n",
    "This notebook runs on a GPU runtime. If running on Colab, choose Runtime > Change runtime type from the menu, then select `GPU` in the 'Hardware accelerator' dropdown menu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b395f606",
   "metadata": {
    "papermill": {
     "duration": 0.043961,
     "end_time": "2022-04-22T04:16:27.697948",
     "exception": false,
     "start_time": "2022-04-22T04:16:27.653987",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad90056",
   "metadata": {
    "lines_to_next_cell": 2,
    "papermill": {
     "duration": 0.043582,
     "end_time": "2022-04-22T04:16:27.785582",
     "exception": false,
     "start_time": "2022-04-22T04:16:27.742000",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This tutorial provides a demonstration of a multi-agent Reinforcement Learning (RL) training loop with [WarpDrive](https://github.com/salesforce/warp-drive). WarpDrive is a flexible, lightweight, and easy-to-use RL framework that implements end-to-end deep multi-agent RL on a single GPU (Graphics Processing Unit). Using the extreme parallelization capability of GPUs, it enables [orders-of-magnitude faster RL](https://arxiv.org/abs/2108.13976) compared to common implementations that blend CPU simulations and GPU models. WarpDrive is extremely efficient as it runs simulations across multiple agents and multiple environment replicas in parallel and completely eliminates the back-and-forth data copying between the CPU and the GPU.\n",
    "\n",
    "We have integrated WarpDrive with the [Pytorch Lightning](https://www.pytorchlightning.ai/) framework, which greatly reduces the trainer boilerplate code, and improves training flexibility.\n",
    "\n",
    "Below, we demonstrate how to use WarpDrive and PytorchLightning together to train a game of [Tag](https://github.com/salesforce/warp-drive/blob/master/example_envs/tag_continuous/tag_continuous.py) where multiple *tagger* agents are trying to run after and tag multiple other *runner* agents. As such, the Warpdrive framework comprises several utility functions that help easily implement any (OpenAI-)*gym-style* RL environment, and furthermore, provides quality-of-life tools to train it end-to-end using just a few lines of code. You may familiarize yourself with WarpDrive with the help of these [tutorials](https://github.com/salesforce/warp-drive/tree/master/tutorials).\n",
    "\n",
    "We invite everyone to **contribute to WarpDrive**, including adding new multi-agent environments, proposing new features and reporting issues on our open source [repository](https://github.com/salesforce/warp-drive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53800c80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T04:16:27.878283Z",
     "iopub.status.busy": "2022-04-22T04:16:27.877743Z",
     "iopub.status.idle": "2022-04-22T04:16:31.121994Z",
     "shell.execute_reply": "2022-04-22T04:16:31.122420Z"
    },
    "papermill": {
     "duration": 3.293662,
     "end_time": "2022-04-22T04:16:31.122613",
     "exception": false,
     "start_time": "2022-04-22T04:16:27.828951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits.mplot3d.art3d as art3d\n",
    "import numpy as np\n",
    "import torch\n",
    "from example_envs.tag_continuous.tag_continuous import TagContinuous\n",
    "\n",
    "# from IPython.display import HTML\n",
    "from matplotlib import animation\n",
    "from matplotlib.patches import Polygon\n",
    "from pytorch_lightning import Trainer\n",
    "from warp_drive.env_wrapper import EnvWrapper\n",
    "from warp_drive.training.pytorch_lightning_trainer import CudaCallback, PerfStatsCallback, WarpDriveModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef8c20a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T04:16:31.218000Z",
     "iopub.status.busy": "2022-04-22T04:16:31.217492Z",
     "iopub.status.idle": "2022-04-22T04:16:31.219029Z",
     "shell.execute_reply": "2022-04-22T04:16:31.219492Z"
    },
    "papermill": {
     "duration": 0.053665,
     "end_time": "2022-04-22T04:16:31.219645",
     "exception": false,
     "start_time": "2022-04-22T04:16:31.165980",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_NUM_AVAILABLE_GPUS = torch.cuda.device_count()\n",
    "assert _NUM_AVAILABLE_GPUS > 0, \"This notebook needs a GPU to run!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df055b4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T04:16:31.311880Z",
     "iopub.status.busy": "2022-04-22T04:16:31.311382Z",
     "iopub.status.idle": "2022-04-22T04:16:31.313874Z",
     "shell.execute_reply": "2022-04-22T04:16:31.313436Z"
    },
    "papermill": {
     "duration": 0.050822,
     "end_time": "2022-04-22T04:16:31.314001",
     "exception": false,
     "start_time": "2022-04-22T04:16:31.263179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set logger level e.g., DEBUG, INFO, WARNING, ERROR.\n",
    "logging.getLogger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876b4e1f",
   "metadata": {
    "papermill": {
     "duration": 0.044499,
     "end_time": "2022-04-22T04:16:31.402024",
     "exception": false,
     "start_time": "2022-04-22T04:16:31.357525",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Specify a set of run configurations for your experiments\n",
    "\n",
    "The run configuration is a dictionary comprising the environment parameters, the trainer and the policy network settings, as well as configurations for saving.\n",
    "\n",
    "For our experiment, we consider an environment wherein $5$ taggers and $100$ runners play the game of [Tag](https://github.com/salesforce/warp-drive/blob/master/example_envs/tag_continuous/tag_continuous.py) on a $20 \\times 20$ plane. The game lasts $200$ timesteps. Each agent chooses it's own acceleration and turn actions at every timestep, and we use mechanics to determine how the agents move over the grid. When a tagger gets close to a runner, the runner is tagged, and is eliminated from the game. For the configuration below, the runners and taggers have the same unit skill levels, or top speeds.\n",
    "\n",
    "We train the agents using $50$ environments or simulations running in parallel. With WarpDrive, each simulation runs on sepate GPU blocks.\n",
    "\n",
    "There are two separate policy networks used for the tagger and runner agents. Each network is a fully-connected model with two layers each of $256$ dimensions. We use the Advantage Actor Critic (A2C) algorithm for training. WarpDrive also currently provides the option to use the Proximal Policy Optimization (PPO) algorithm instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a08c801b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T04:16:31.498415Z",
     "iopub.status.busy": "2022-04-22T04:16:31.497873Z",
     "iopub.status.idle": "2022-04-22T04:16:31.499760Z",
     "shell.execute_reply": "2022-04-22T04:16:31.500149Z"
    },
    "papermill": {
     "duration": 0.054793,
     "end_time": "2022-04-22T04:16:31.500314",
     "exception": false,
     "start_time": "2022-04-22T04:16:31.445521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_config = dict(\n",
    "    name=\"tag_continuous\",\n",
    "    # Environment settings.\n",
    "    env=dict(\n",
    "        # number of taggers in the environment\n",
    "        num_taggers=5,\n",
    "        # number of runners in the environment\n",
    "        num_runners=100,\n",
    "        # length of the (square) grid on which the game is played\n",
    "        grid_length=20.0,\n",
    "        # episode length in timesteps\n",
    "        episode_length=200,\n",
    "        # maximum acceleration\n",
    "        max_acceleration=0.1,\n",
    "        # minimum acceleration\n",
    "        min_acceleration=-0.1,\n",
    "        # 3*pi/4 radians\n",
    "        max_turn=2.35,\n",
    "        # -3*pi/4 radians\n",
    "        min_turn=-2.35,\n",
    "        # number of discretized accelerate actions\n",
    "        num_acceleration_levels=10,\n",
    "        # number of discretized turn actions\n",
    "        num_turn_levels=10,\n",
    "        # skill level for the tagger\n",
    "        skill_level_tagger=1.0,\n",
    "        # skill level for the runner\n",
    "        skill_level_runner=1.0,\n",
    "        # each agent only sees full or partial information\n",
    "        use_full_observation=False,\n",
    "        # flag to indicate if a runner stays in the game after getting tagged\n",
    "        runner_exits_game_after_tagged=True,\n",
    "        # number of other agents each agent can see\n",
    "        num_other_agents_observed=10,\n",
    "        # positive reward for the tagger upon tagging a runner\n",
    "        tag_reward_for_tagger=10.0,\n",
    "        # negative reward for the runner upon getting tagged\n",
    "        tag_penalty_for_runner=-10.0,\n",
    "        # reward at the end of the game for a runner that isn't tagged\n",
    "        end_of_game_reward_for_runner=1.0,\n",
    "        # margin between a tagger and runner to consider the runner as 'tagged'.\n",
    "        tagging_distance=0.02,\n",
    "    ),\n",
    "    # Trainer settings.\n",
    "    trainer=dict(\n",
    "        # number of environment replicas (number of GPU blocks used)\n",
    "        num_envs=50,\n",
    "        # total batch size used for training per iteration (across all the environments)\n",
    "        train_batch_size=10000,\n",
    "        # total number of episodes to run the training for (can be arbitrarily high!)\n",
    "        num_episodes=50000,\n",
    "    ),\n",
    "    # Policy network settings.\n",
    "    policy=dict(\n",
    "        runner=dict(\n",
    "            # flag indicating whether the model needs to be trained\n",
    "            to_train=True,\n",
    "            # algorithm used to train the policy\n",
    "            algorithm=\"A2C\",\n",
    "            # discount rate\n",
    "            gamma=0.98,\n",
    "            # learning rate\n",
    "            lr=0.005,\n",
    "            # policy model settings\n",
    "            model=dict(type=\"fully_connected\", fc_dims=[256, 256], model_ckpt_filepath=\"\"),\n",
    "        ),\n",
    "        tagger=dict(\n",
    "            to_train=True,\n",
    "            algorithm=\"A2C\",\n",
    "            gamma=0.98,\n",
    "            lr=0.002,\n",
    "            model=dict(type=\"fully_connected\", fc_dims=[256, 256], model_ckpt_filepath=\"\"),\n",
    "        ),\n",
    "    ),\n",
    "    # Checkpoint saving setting.\n",
    "    saving=dict(\n",
    "        # how often (in iterations) to print the metrics\n",
    "        metrics_log_freq=10,\n",
    "        # how often (in iterations) to save the model parameters\n",
    "        model_params_save_freq=5000,\n",
    "        # base folder used for saving\n",
    "        basedir=\"/tmp\",\n",
    "        # experiment name\n",
    "        name=\"continuous_tag\",\n",
    "        # experiment tag\n",
    "        tag=\"example\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d75b7f1",
   "metadata": {
    "papermill": {
     "duration": 0.043377,
     "end_time": "2022-04-22T04:16:31.588395",
     "exception": false,
     "start_time": "2022-04-22T04:16:31.545018",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Instantiate the WarpDrive Module\n",
    "\n",
    "In order to instantiate the WarpDrive module, we first use an environment wrapper to specify that the environment needs to be run on the GPU (via the `use_cuda` flag). Also, agents in the environment can share policy models; so we specify a dictionary to map each policy network model to the list of agent ids using that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "471bd41c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T04:16:31.685376Z",
     "iopub.status.busy": "2022-04-22T04:16:31.684837Z",
     "iopub.status.idle": "2022-04-22T04:16:43.208951Z",
     "shell.execute_reply": "2022-04-22T04:16:43.209400Z"
    },
    "papermill": {
     "duration": 11.576766,
     "end_time": "2022-04-22T04:16:43.209588",
     "exception": false,
     "start_time": "2022-04-22T04:16:31.632822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1650601003\n"
     ]
    }
   ],
   "source": [
    "# Create a wrapped environment object via the EnvWrapper.\n",
    "# Ensure that use_cuda is set to True (in order to run on the GPU).\n",
    "env_wrapper = EnvWrapper(\n",
    "    TagContinuous(**run_config[\"env\"]),\n",
    "    num_envs=run_config[\"trainer\"][\"num_envs\"],\n",
    "    use_cuda=True,\n",
    ")\n",
    "\n",
    "# Agents can share policy models: this dictionary maps policy model names to agent ids.\n",
    "policy_tag_to_agent_id_map = {\n",
    "    \"tagger\": list(env_wrapper.env.taggers),\n",
    "    \"runner\": list(env_wrapper.env.runners),\n",
    "}\n",
    "\n",
    "wd_module = WarpDriveModule(\n",
    "    env_wrapper=env_wrapper,\n",
    "    config=run_config,\n",
    "    policy_tag_to_agent_id_map=policy_tag_to_agent_id_map,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a09c2de",
   "metadata": {
    "papermill": {
     "duration": 0.044721,
     "end_time": "2022-04-22T04:16:43.299151",
     "exception": false,
     "start_time": "2022-04-22T04:16:43.254430",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Visualizing an episode roll-out before training\n",
    "\n",
    "We have created a helper function (see below) to visualize an episode rollout. Internally, this function uses the WarpDrive module's `fetch_episode_states` API to fetch the data arrays on the GPU for the duration of an entire episode. Specifically, we fetch the state arrays pertaining to agents' x and y locations on the plane and indicators on which agents are still active in the game. Note that this function may be invoked at any time during training, and it will use the state of the policy models at that time to sample actions and generate the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f082b0fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T04:16:43.406191Z",
     "iopub.status.busy": "2022-04-22T04:16:43.391366Z",
     "iopub.status.idle": "2022-04-22T04:16:43.408058Z",
     "shell.execute_reply": "2022-04-22T04:16:43.407612Z"
    },
    "papermill": {
     "duration": 0.064535,
     "end_time": "2022-04-22T04:16:43.408206",
     "exception": false,
     "start_time": "2022-04-22T04:16:43.343671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_tag_env_rollout_animation(\n",
    "    warp_drive_module,\n",
    "    fps=25,\n",
    "    tagger_color=\"#C843C3\",\n",
    "    runner_color=\"#245EB6\",\n",
    "    runner_not_in_game_color=\"#666666\",\n",
    "    fig_width=6,\n",
    "    fig_height=6,\n",
    "):\n",
    "    assert warp_drive_module is not None\n",
    "    episode_states = warp_drive_module.fetch_episode_states([\"loc_x\", \"loc_y\", \"still_in_the_game\"])\n",
    "    assert isinstance(episode_states, dict)\n",
    "    env = warp_drive_module.cuda_envs.env\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(fig_width, fig_height))  # , constrained_layout=True\n",
    "    ax.remove()\n",
    "    ax = fig.add_subplot(1, 1, 1, projection=\"3d\")\n",
    "\n",
    "    # Bounds\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_zlim(-0.01, 0.01)\n",
    "\n",
    "    # Surface\n",
    "    corner_points = [(0, 0), (0, 1), (1, 1), (1, 0)]\n",
    "\n",
    "    poly = Polygon(corner_points, color=(0.1, 0.2, 0.5, 0.15))\n",
    "    ax.add_patch(poly)\n",
    "    art3d.pathpatch_2d_to_3d(poly, z=0, zdir=\"z\")\n",
    "\n",
    "    # \"Hide\" side panes\n",
    "    ax.xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "    ax.yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "    ax.zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "\n",
    "    # Hide grid lines\n",
    "    ax.grid(False)\n",
    "\n",
    "    # Hide axes ticks\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_zticks([])\n",
    "\n",
    "    # Hide axes\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    # Set camera\n",
    "    ax.elev = 40\n",
    "    ax.azim = -55\n",
    "    ax.dist = 10\n",
    "\n",
    "    # Try to reduce whitespace\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=-0.2, top=1)\n",
    "\n",
    "    # Plot init data\n",
    "    lines = [None for _ in range(env.num_agents)]\n",
    "\n",
    "    for idx in range(len(lines)):\n",
    "        if idx in env.taggers:\n",
    "            lines[idx] = ax.plot3D(\n",
    "                episode_states[\"loc_x\"][:1, idx] / env.grid_length,\n",
    "                episode_states[\"loc_y\"][:1, idx] / env.grid_length,\n",
    "                0,\n",
    "                color=tagger_color,\n",
    "                marker=\"o\",\n",
    "                markersize=10,\n",
    "            )[0]\n",
    "        else:  # runners\n",
    "            lines[idx] = ax.plot3D(\n",
    "                episode_states[\"loc_x\"][:1, idx] / env.grid_length,\n",
    "                episode_states[\"loc_y\"][:1, idx] / env.grid_length,\n",
    "                [0],\n",
    "                color=runner_color,\n",
    "                marker=\"o\",\n",
    "                markersize=5,\n",
    "            )[0]\n",
    "\n",
    "    init_num_runners = env.num_agents - env.num_taggers\n",
    "\n",
    "    def _get_label(timestep, n_runners_alive, init_n_runners):\n",
    "        line1 = \"Continuous Tag\\n\"\n",
    "        line2 = \"Time Step:\".ljust(14) + f\"{timestep:4.0f}\\n\"\n",
    "        frac_runners_alive = n_runners_alive / init_n_runners\n",
    "        pct_runners_alive = f\"{n_runners_alive:4} ({frac_runners_alive * 100:.0f}%)\"\n",
    "        line3 = \"Runners Left:\".ljust(14) + pct_runners_alive\n",
    "        return line1 + line2 + line3\n",
    "\n",
    "    label = ax.text(\n",
    "        0,\n",
    "        0,\n",
    "        0.02,\n",
    "        _get_label(0, init_num_runners, init_num_runners).lower(),\n",
    "    )\n",
    "\n",
    "    label.set_fontsize(14)\n",
    "    label.set_fontweight(\"normal\")\n",
    "    label.set_color(\"#666666\")\n",
    "\n",
    "    def animate(i):\n",
    "        for idx, line in enumerate(lines):\n",
    "            line.set_data_3d(\n",
    "                episode_states[\"loc_x\"][i : i + 1, idx] / env.grid_length,\n",
    "                episode_states[\"loc_y\"][i : i + 1, idx] / env.grid_length,\n",
    "                np.zeros(1),\n",
    "            )\n",
    "\n",
    "            still_in_game = episode_states[\"still_in_the_game\"][i, idx]\n",
    "\n",
    "            if still_in_game:\n",
    "                pass\n",
    "            else:\n",
    "                line.set_color(runner_not_in_game_color)\n",
    "                line.set_marker(\"\")\n",
    "\n",
    "        n_runners_alive = episode_states[\"still_in_the_game\"][i].sum() - env.num_taggers\n",
    "        label.set_text(_get_label(i, n_runners_alive, init_num_runners).lower())\n",
    "\n",
    "    ani = animation.FuncAnimation(fig, animate, np.arange(0, env.episode_length + 1), interval=1000.0 / fps)\n",
    "    plt.close()\n",
    "\n",
    "    return ani"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b01f86e",
   "metadata": {
    "papermill": {
     "duration": 0.044623,
     "end_time": "2022-04-22T04:16:43.497741",
     "exception": false,
     "start_time": "2022-04-22T04:16:43.453118",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The animation below shows a sample realization of the game episode before training, i.e., with randomly chosen agent actions. The $5$ taggers are marked in pink, while the $100$ blue agents are the runners. Both the taggers and runners move around randomly and about half the runners remain at the end of the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6fbc469",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T04:16:43.592942Z",
     "iopub.status.busy": "2022-04-22T04:16:43.592450Z",
     "iopub.status.idle": "2022-04-22T04:16:43.594514Z",
     "shell.execute_reply": "2022-04-22T04:16:43.594021Z"
    },
    "papermill": {
     "duration": 0.051088,
     "end_time": "2022-04-22T04:16:43.594633",
     "exception": false,
     "start_time": "2022-04-22T04:16:43.543545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# anim = generate_tag_env_rollout_animation(wd_module)\n",
    "# HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051710d9",
   "metadata": {
    "papermill": {
     "duration": 0.045043,
     "end_time": "2022-04-22T04:16:43.685651",
     "exception": false,
     "start_time": "2022-04-22T04:16:43.640608",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Create the Lightning Trainer\n",
    "\n",
    "Next, we create the trainer for training the WarpDrive model. We add the `performance stats` callbacks to the trainer to view the throughput performance of WarpDrive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8d46605",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T04:16:43.784435Z",
     "iopub.status.busy": "2022-04-22T04:16:43.783408Z",
     "iopub.status.idle": "2022-04-22T04:16:43.788918Z",
     "shell.execute_reply": "2022-04-22T04:16:43.789340Z"
    },
    "papermill": {
     "duration": 0.059385,
     "end_time": "2022-04-22T04:16:43.789518",
     "exception": false,
     "start_time": "2022-04-22T04:16:43.730133",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "log_freq = run_config[\"saving\"][\"metrics_log_freq\"]\n",
    "\n",
    "# Define callbacks.\n",
    "cuda_callback = CudaCallback(module=wd_module)\n",
    "perf_stats_callback = PerfStatsCallback(\n",
    "    batch_size=wd_module.training_batch_size,\n",
    "    num_iters=wd_module.num_iters,\n",
    "    log_freq=log_freq,\n",
    ")\n",
    "\n",
    "# Instantiate the PytorchLightning trainer with the callbacks.\n",
    "# Also, set the number of gpus to 1, since this notebook uses just a single GPU.\n",
    "num_gpus = 1\n",
    "num_episodes = run_config[\"trainer\"][\"num_episodes\"]\n",
    "episode_length = run_config[\"env\"][\"episode_length\"]\n",
    "training_batch_size = run_config[\"trainer\"][\"train_batch_size\"]\n",
    "num_epochs = num_episodes * episode_length / training_batch_size\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=num_gpus,\n",
    "    callbacks=[cuda_callback, perf_stats_callback],\n",
    "    max_epochs=num_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c50bebe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T04:16:43.890815Z",
     "iopub.status.busy": "2022-04-22T04:16:43.890306Z",
     "iopub.status.idle": "2022-04-22T04:16:45.448158Z",
     "shell.execute_reply": "2022-04-22T04:16:45.448567Z"
    },
    "papermill": {
     "duration": 1.611231,
     "end_time": "2022-04-22T04:16:45.448746",
     "exception": false,
     "start_time": "2022-04-22T04:16:43.837515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-98f939bd39fd4834\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-98f939bd39fd4834\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start tensorboard.\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bfc5ef",
   "metadata": {
    "papermill": {
     "duration": 0.049087,
     "end_time": "2022-04-22T04:16:45.548033",
     "exception": false,
     "start_time": "2022-04-22T04:16:45.498946",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train the WarpDrive Module\n",
    "\n",
    "Finally, we invoke training.\n",
    "\n",
    "Note: please scroll up to the tensorboard cell to visualize the curves during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebd10af5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T04:16:45.661952Z",
     "iopub.status.busy": "2022-04-22T04:16:45.661457Z",
     "iopub.status.idle": "2022-04-22T04:31:20.916060Z",
     "shell.execute_reply": "2022-04-22T04:31:20.916445Z"
    },
    "papermill": {
     "duration": 875.309707,
     "end_time": "2022-04-22T04:31:20.916630",
     "exception": false,
     "start_time": "2022-04-22T04:16:45.606923",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/AzDevOps_azpcontainer/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "  rank_zero_deprecation(\n",
      "/home/AzDevOps_azpcontainer/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "  rank_zero_deprecation(\n",
      "Missing logger folder: /__w/1/s/lightning_logs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n",
      "------------------------------\n",
      "0         Trainable params\n",
      "0         Non-trainable params\n",
      "0         Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/AzDevOps_azpcontainer/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/AzDevOps_azpcontainer/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1927: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec750af199a94f8091c03c862ee4ce11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.29561\n",
      "Policy loss                             :   -0.09944\n",
      "Value function loss                     :    4.22670\n",
      "Mean rewards                            :   -0.02825\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.26719\n",
      "Mean advantages                         :   -0.02164\n",
      "Mean (norm.) advantages                 :   -0.02164\n",
      "Mean (discounted) returns               :   -1.28883\n",
      "Mean normalized returns                 :   -1.28883\n",
      "Mean entropy                            :    4.76890\n",
      "Variance explained by the value function:    0.16321\n",
      "Std. of action_0 over agents            :    3.10107\n",
      "Std. of action_0 over envs              :    3.09726\n",
      "Std. of action_0 over time              :    3.10287\n",
      "Std. of action_1 over agents            :    3.03511\n",
      "Std. of action_1 over envs              :    3.03153\n",
      "Std. of action_1 over time              :    3.03523\n",
      "Current timestep                        : 100000.00000\n",
      "Gradient norm                           :    0.04779\n",
      "Mean episodic reward                    : -458.86800\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   71.48376\n",
      "Policy loss                             :   66.68478\n",
      "Value function loss                     :  499.21271\n",
      "Mean rewards                            :    0.58700\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :    4.72709\n",
      "Mean advantages                         :   17.19324\n",
      "Mean (norm.) advantages                 :   17.19324\n",
      "Mean (discounted) returns               :   21.92033\n",
      "Mean normalized returns                 :   21.92033\n",
      "Mean entropy                            :    3.86286\n",
      "Variance explained by the value function:   -0.01202\n",
      "Std. of action_0 over agents            :    2.20117\n",
      "Std. of action_0 over envs              :    2.34950\n",
      "Std. of action_0 over time              :    2.35609\n",
      "Std. of action_1 over agents            :    2.13672\n",
      "Std. of action_1 over envs              :    2.26055\n",
      "Std. of action_1 over time              :    2.26863\n",
      "Current timestep                        : 100000.00000\n",
      "Gradient norm                           :    1.10622\n",
      "Mean episodic reward                    :  496.12000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 10 / 1000    \n",
      "Mean training time per iter (ms)        :     139.64\n",
      "Mean steps per sec (training time)      :   71614.70\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/AzDevOps_azpcontainer/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:229: UserWarning: You called `self.log('Current timestep_runner', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n",
      "/home/AzDevOps_azpcontainer/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:229: UserWarning: You called `self.log('Current timestep_tagger', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.13941\n",
      "Policy loss                             :    0.05974\n",
      "Value function loss                     :    3.92918\n",
      "Mean rewards                            :   -0.02666\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.17205\n",
      "Mean advantages                         :    0.01279\n",
      "Mean (norm.) advantages                 :    0.01279\n",
      "Mean (discounted) returns               :   -1.15927\n",
      "Mean normalized returns                 :   -1.15927\n",
      "Mean entropy                            :    4.76878\n",
      "Variance explained by the value function:    0.18872\n",
      "Std. of action_0 over agents            :    3.16038\n",
      "Std. of action_0 over envs              :    3.15714\n",
      "Std. of action_0 over time              :    3.16244\n",
      "Std. of action_1 over agents            :    3.30826\n",
      "Std. of action_1 over envs              :    3.30508\n",
      "Std. of action_1 over time              :    3.30887\n",
      "Current timestep                        : 200000.00000\n",
      "Gradient norm                           :    0.03368\n",
      "Mean episodic reward                    : -563.87200\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    0.28682\n",
      "Policy loss                             :   -1.30073\n",
      "Value function loss                     :  165.07660\n",
      "Mean rewards                            :    0.55720\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   26.87305\n",
      "Mean advantages                         :   -1.25896\n",
      "Mean (norm.) advantages                 :   -1.25896\n",
      "Mean (discounted) returns               :   25.61409\n",
      "Mean normalized returns                 :   25.61409\n",
      "Mean entropy                            :    1.26443\n",
      "Variance explained by the value function:   -0.09630\n",
      "Std. of action_0 over agents            :    2.01996\n",
      "Std. of action_0 over envs              :    2.10392\n",
      "Std. of action_0 over time              :    2.10761\n",
      "Std. of action_1 over agents            :    0.03658\n",
      "Std. of action_1 over envs              :    0.10289\n",
      "Std. of action_1 over time              :    0.16429\n",
      "Current timestep                        : 200000.00000\n",
      "Gradient norm                           :    0.77949\n",
      "Mean episodic reward                    :  585.78000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 20 / 1000    \n",
      "Mean training time per iter (ms)        :     134.83\n",
      "Mean steps per sec (training time)      :   74165.85\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.36544\n",
      "Policy loss                             :   -0.16869\n",
      "Value function loss                     :    4.23149\n",
      "Mean rewards                            :   -0.02958\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.26441\n",
      "Mean advantages                         :   -0.03540\n",
      "Mean (norm.) advantages                 :   -0.03540\n",
      "Mean (discounted) returns               :   -1.29981\n",
      "Mean normalized returns                 :   -1.29981\n",
      "Mean entropy                            :    4.78127\n",
      "Variance explained by the value function:    0.19381\n",
      "Std. of action_0 over agents            :    3.17005\n",
      "Std. of action_0 over envs              :    3.16643\n",
      "Std. of action_0 over time              :    3.17118\n",
      "Std. of action_1 over agents            :    3.12740\n",
      "Std. of action_1 over envs              :    3.12359\n",
      "Std. of action_1 over time              :    3.12819\n",
      "Current timestep                        : 300000.00000\n",
      "Gradient norm                           :    0.05014\n",
      "Mean episodic reward                    : -568.40400\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    1.72281\n",
      "Policy loss                             :    0.04230\n",
      "Value function loss                     :  177.68925\n",
      "Mean rewards                            :    0.60780\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   28.67383\n",
      "Mean advantages                         :   -0.02816\n",
      "Mean (norm.) advantages                 :   -0.02816\n",
      "Mean (discounted) returns               :   28.64566\n",
      "Mean normalized returns                 :   28.64566\n",
      "Mean entropy                            :    1.92779\n",
      "Variance explained by the value function:   -0.02407\n",
      "Std. of action_0 over agents            :    0.52569\n",
      "Std. of action_0 over envs              :    0.58831\n",
      "Std. of action_0 over time              :    0.59848\n",
      "Std. of action_1 over agents            :    1.45080\n",
      "Std. of action_1 over envs              :    1.51973\n",
      "Std. of action_1 over time              :    1.52414\n",
      "Current timestep                        : 300000.00000\n",
      "Gradient norm                           :    0.98253\n",
      "Mean episodic reward                    :  588.54000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 30 / 1000    \n",
      "Mean training time per iter (ms)        :     134.78\n",
      "Mean steps per sec (training time)      :   74196.02\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.51885\n",
      "Policy loss                             :   -0.32013\n",
      "Value function loss                     :    4.05303\n",
      "Mean rewards                            :   -0.02816\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.12684\n",
      "Mean advantages                         :   -0.06705\n",
      "Mean (norm.) advantages                 :   -0.06705\n",
      "Mean (discounted) returns               :   -1.19390\n",
      "Mean normalized returns                 :   -1.19390\n",
      "Mean entropy                            :    4.78491\n",
      "Variance explained by the value function:    0.19311\n",
      "Std. of action_0 over agents            :    3.15052\n",
      "Std. of action_0 over envs              :    3.14774\n",
      "Std. of action_0 over time              :    3.15191\n",
      "Std. of action_1 over agents            :    3.16333\n",
      "Std. of action_1 over envs              :    3.16009\n",
      "Std. of action_1 over time              :    3.16444\n",
      "Current timestep                        : 400000.00000\n",
      "Gradient norm                           :    0.02620\n",
      "Mean episodic reward                    : -561.13800\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    3.69027\n",
      "Policy loss                             :    2.25955\n",
      "Value function loss                     :  152.33397\n",
      "Mean rewards                            :    0.58580\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   24.85278\n",
      "Mean advantages                         :    1.14076\n",
      "Mean (norm.) advantages                 :    1.14076\n",
      "Mean (discounted) returns               :   25.99354\n",
      "Mean normalized returns                 :   25.99354\n",
      "Mean entropy                            :    1.85237\n",
      "Variance explained by the value function:   -0.01995\n",
      "Std. of action_0 over agents            :    0.56588\n",
      "Std. of action_0 over envs              :    0.64696\n",
      "Std. of action_0 over time              :    0.65732\n",
      "Std. of action_1 over agents            :    1.34437\n",
      "Std. of action_1 over envs              :    1.43722\n",
      "Std. of action_1 over time              :    1.44228\n",
      "Current timestep                        : 400000.00000\n",
      "Gradient norm                           :    0.83519\n",
      "Mean episodic reward                    :  584.16000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 40 / 1000    \n",
      "Mean training time per iter (ms)        :     133.89\n",
      "Mean steps per sec (training time)      :   74686.90\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.20911\n",
      "Policy loss                             :   -0.00956\n",
      "Value function loss                     :    3.96707\n",
      "Mean rewards                            :   -0.02621\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.13329\n",
      "Mean advantages                         :   -0.00148\n",
      "Mean (norm.) advantages                 :   -0.00148\n",
      "Mean (discounted) returns               :   -1.13477\n",
      "Mean normalized returns                 :   -1.13477\n",
      "Mean entropy                            :    4.78451\n",
      "Variance explained by the value function:    0.17726\n",
      "Std. of action_0 over agents            :    3.18450\n",
      "Std. of action_0 over envs              :    3.18058\n",
      "Std. of action_0 over time              :    3.18559\n",
      "Std. of action_1 over agents            :    3.24211\n",
      "Std. of action_1 over envs              :    3.23863\n",
      "Std. of action_1 over time              :    3.24272\n",
      "Current timestep                        : 500000.00000\n",
      "Gradient norm                           :    0.02311\n",
      "Mean episodic reward                    : -545.78200\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.63752\n",
      "Policy loss                             :   -2.03540\n",
      "Value function loss                     :  151.13307\n",
      "Mean rewards                            :    0.55380\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   25.83076\n",
      "Mean advantages                         :   -0.89841\n",
      "Mean (norm.) advantages                 :   -0.89841\n",
      "Mean (discounted) returns               :   24.93235\n",
      "Mean normalized returns                 :   24.93235\n",
      "Mean entropy                            :    2.26906\n",
      "Variance explained by the value function:    0.04017\n",
      "Std. of action_0 over agents            :    0.87775\n",
      "Std. of action_0 over envs              :    1.06517\n",
      "Std. of action_0 over time              :    1.07649\n",
      "Std. of action_1 over agents            :    2.67889\n",
      "Std. of action_1 over envs              :    2.77255\n",
      "Std. of action_1 over time              :    2.77483\n",
      "Current timestep                        : 500000.00000\n",
      "Gradient norm                           :    1.02381\n",
      "Mean episodic reward                    :  571.20000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 50 / 1000    \n",
      "Mean training time per iter (ms)        :     133.29\n",
      "Mean steps per sec (training time)      :   75025.85\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.44279\n",
      "Policy loss                             :   -0.24555\n",
      "Value function loss                     :    4.19776\n",
      "Mean rewards                            :   -0.02866\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.17080\n",
      "Mean advantages                         :   -0.05098\n",
      "Mean (norm.) advantages                 :   -0.05098\n",
      "Mean (discounted) returns               :   -1.22179\n",
      "Mean normalized returns                 :   -1.22179\n",
      "Mean entropy                            :    4.78432\n",
      "Variance explained by the value function:    0.17778\n",
      "Std. of action_0 over agents            :    3.17138\n",
      "Std. of action_0 over envs              :    3.16814\n",
      "Std. of action_0 over time              :    3.17295\n",
      "Std. of action_1 over agents            :    3.18664\n",
      "Std. of action_1 over envs              :    3.18346\n",
      "Std. of action_1 over time              :    3.18842\n",
      "Current timestep                        : 600000.00000\n",
      "Gradient norm                           :    0.01910\n",
      "Mean episodic reward                    : -569.85800\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    0.19026\n",
      "Policy loss                             :   -1.14752\n",
      "Value function loss                     :  141.49873\n",
      "Mean rewards                            :    0.59960\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   27.95154\n",
      "Mean advantages                         :   -0.79166\n",
      "Mean (norm.) advantages                 :   -0.79166\n",
      "Mean (discounted) returns               :   27.15989\n",
      "Mean normalized returns                 :   27.15989\n",
      "Mean entropy                            :    1.54421\n",
      "Variance explained by the value function:    0.03882\n",
      "Std. of action_0 over agents            :    0.68900\n",
      "Std. of action_0 over envs              :    0.82649\n",
      "Std. of action_0 over time              :    0.84436\n",
      "Std. of action_1 over agents            :    1.53829\n",
      "Std. of action_1 over envs              :    1.79476\n",
      "Std. of action_1 over time              :    1.80652\n",
      "Current timestep                        : 600000.00000\n",
      "Gradient norm                           :    0.96701\n",
      "Mean episodic reward                    :  592.64000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 60 / 1000    \n",
      "Mean training time per iter (ms)        :     132.84\n",
      "Mean steps per sec (training time)      :   75276.37\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.17643\n",
      "Policy loss                             :    0.02091\n",
      "Value function loss                     :    4.20877\n",
      "Mean rewards                            :   -0.02961\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.26399\n",
      "Mean advantages                         :    0.00456\n",
      "Mean (norm.) advantages                 :    0.00456\n",
      "Mean (discounted) returns               :   -1.25943\n",
      "Mean normalized returns                 :   -1.25943\n",
      "Mean entropy                            :    4.78850\n",
      "Variance explained by the value function:    0.19216\n",
      "Std. of action_0 over agents            :    3.19524\n",
      "Std. of action_0 over envs              :    3.19180\n",
      "Std. of action_0 over time              :    3.19651\n",
      "Std. of action_1 over agents            :    3.19286\n",
      "Std. of action_1 over envs              :    3.18921\n",
      "Std. of action_1 over time              :    3.19416\n",
      "Current timestep                        : 700000.00000\n",
      "Gradient norm                           :    0.02438\n",
      "Mean episodic reward                    : -586.91200\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    1.17608\n",
      "Policy loss                             :   -0.22491\n",
      "Value function loss                     :  147.56619\n",
      "Mean rewards                            :    0.61340\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   27.25769\n",
      "Mean advantages                         :   -0.16004\n",
      "Mean (norm.) advantages                 :   -0.16004\n",
      "Mean (discounted) returns               :   27.09766\n",
      "Mean normalized returns                 :   27.09766\n",
      "Mean entropy                            :    1.49357\n",
      "Variance explained by the value function:    0.08838\n",
      "Std. of action_0 over agents            :    0.45946\n",
      "Std. of action_0 over envs              :    0.49755\n",
      "Std. of action_0 over time              :    0.50076\n",
      "Std. of action_1 over agents            :    1.86805\n",
      "Std. of action_1 over envs              :    2.02685\n",
      "Std. of action_1 over time              :    2.03246\n",
      "Current timestep                        : 700000.00000\n",
      "Gradient norm                           :    0.84228\n",
      "Mean episodic reward                    :  607.84000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 70 / 1000    \n",
      "Mean training time per iter (ms)        :     132.56\n",
      "Mean steps per sec (training time)      :   75434.75\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.37541\n",
      "Policy loss                             :   -0.17783\n",
      "Value function loss                     :    4.18084\n",
      "Mean rewards                            :   -0.02972\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.21098\n",
      "Mean advantages                         :   -0.03708\n",
      "Mean (norm.) advantages                 :   -0.03708\n",
      "Mean (discounted) returns               :   -1.24806\n",
      "Mean normalized returns                 :   -1.24806\n",
      "Mean entropy                            :    4.78777\n",
      "Variance explained by the value function:    0.19617\n",
      "Std. of action_0 over agents            :    3.16429\n",
      "Std. of action_0 over envs              :    3.16138\n",
      "Std. of action_0 over time              :    3.16669\n",
      "Std. of action_1 over agents            :    3.19662\n",
      "Std. of action_1 over envs              :    3.19409\n",
      "Std. of action_1 over time              :    3.19849\n",
      "Current timestep                        : 800000.00000\n",
      "Gradient norm                           :    0.02269\n",
      "Mean episodic reward                    : -583.45200\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    0.97793\n",
      "Policy loss                             :   -0.35443\n",
      "Value function loss                     :  140.94687\n",
      "Mean rewards                            :    0.61060\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   26.67221\n",
      "Mean advantages                         :   -0.16298\n",
      "Mean (norm.) advantages                 :   -0.16298\n",
      "Mean (discounted) returns               :   26.50922\n",
      "Mean normalized returns                 :   26.50922\n",
      "Mean entropy                            :    1.54200\n",
      "Variance explained by the value function:    0.10822\n",
      "Std. of action_0 over agents            :    0.17347\n",
      "Std. of action_0 over envs              :    0.28422\n",
      "Std. of action_0 over time              :    0.29403\n",
      "Std. of action_1 over agents            :    2.31733\n",
      "Std. of action_1 over envs              :    2.39203\n",
      "Std. of action_1 over time              :    2.39413\n",
      "Current timestep                        : 800000.00000\n",
      "Gradient norm                           :    0.76039\n",
      "Mean episodic reward                    :  604.00000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 80 / 1000    \n",
      "Mean training time per iter (ms)        :     132.37\n",
      "Mean steps per sec (training time)      :   75543.18\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.35127\n",
      "Policy loss                             :   -0.15415\n",
      "Value function loss                     :    4.22053\n",
      "Mean rewards                            :   -0.03042\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.21477\n",
      "Mean advantages                         :   -0.03194\n",
      "Mean (norm.) advantages                 :   -0.03194\n",
      "Mean (discounted) returns               :   -1.24672\n",
      "Mean normalized returns                 :   -1.24672\n",
      "Mean entropy                            :    4.78653\n",
      "Variance explained by the value function:    0.19839\n",
      "Std. of action_0 over agents            :    3.15758\n",
      "Std. of action_0 over envs              :    3.15373\n",
      "Std. of action_0 over time              :    3.15893\n",
      "Std. of action_1 over agents            :    3.19264\n",
      "Std. of action_1 over envs              :    3.18938\n",
      "Std. of action_1 over time              :    3.19414\n",
      "Current timestep                        : 900000.00000\n",
      "Gradient norm                           :    0.02952\n",
      "Mean episodic reward                    : -597.38200\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    1.64220\n",
      "Policy loss                             :    0.34608\n",
      "Value function loss                     :  138.46170\n",
      "Mean rewards                            :    0.62180\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   25.70090\n",
      "Mean advantages                         :    0.34612\n",
      "Mean (norm.) advantages                 :    0.34612\n",
      "Mean (discounted) returns               :   26.04702\n",
      "Mean normalized returns                 :   26.04702\n",
      "Mean entropy                            :    1.76990\n",
      "Variance explained by the value function:    0.16364\n",
      "Std. of action_0 over agents            :    0.44804\n",
      "Std. of action_0 over envs              :    0.49295\n",
      "Std. of action_0 over time              :    0.49739\n",
      "Std. of action_1 over agents            :    2.28808\n",
      "Std. of action_1 over envs              :    2.38469\n",
      "Std. of action_1 over time              :    2.38598\n",
      "Current timestep                        : 900000.00000\n",
      "Gradient norm                           :    0.87399\n",
      "Mean episodic reward                    :  616.32000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 90 / 1000    \n",
      "Mean training time per iter (ms)        :     132.34\n",
      "Mean steps per sec (training time)      :   75563.67\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    0.10690\n",
      "Policy loss                             :    0.30437\n",
      "Value function loss                     :    4.17746\n",
      "Mean rewards                            :   -0.03019\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.31399\n",
      "Mean advantages                         :    0.06380\n",
      "Mean (norm.) advantages                 :    0.06380\n",
      "Mean (discounted) returns               :   -1.25019\n",
      "Mean normalized returns                 :   -1.25019\n",
      "Mean entropy                            :    4.78486\n",
      "Variance explained by the value function:    0.20020\n",
      "Std. of action_0 over agents            :    3.20304\n",
      "Std. of action_0 over envs              :    3.19988\n",
      "Std. of action_0 over time              :    3.20410\n",
      "Std. of action_1 over agents            :    3.16478\n",
      "Std. of action_1 over envs              :    3.16108\n",
      "Std. of action_1 over time              :    3.16590\n",
      "Current timestep                        : 1000000.00000\n",
      "Gradient norm                           :    0.01797\n",
      "Mean episodic reward                    : -609.90000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    1.83317\n",
      "Policy loss                             :    0.55187\n",
      "Value function loss                     :  135.31894\n",
      "Mean rewards                            :    0.62240\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   25.07043\n",
      "Mean advantages                         :    0.60529\n",
      "Mean (norm.) advantages                 :    0.60529\n",
      "Mean (discounted) returns               :   25.67571\n",
      "Mean normalized returns                 :   25.67571\n",
      "Mean entropy                            :    1.43778\n",
      "Variance explained by the value function:    0.19644\n",
      "Std. of action_0 over agents            :    0.41113\n",
      "Std. of action_0 over envs              :    0.47021\n",
      "Std. of action_0 over time              :    0.47753\n",
      "Std. of action_1 over agents            :    1.64128\n",
      "Std. of action_1 over envs              :    1.94565\n",
      "Std. of action_1 over time              :    1.96274\n",
      "Current timestep                        : 1000000.00000\n",
      "Gradient norm                           :    0.85278\n",
      "Mean episodic reward                    :  627.50000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 100 / 1000   \n",
      "Mean training time per iter (ms)        :     132.41\n",
      "Mean steps per sec (training time)      :   75521.56\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.39392\n",
      "Policy loss                             :   -0.19717\n",
      "Value function loss                     :    4.26200\n",
      "Mean rewards                            :   -0.03067\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.22065\n",
      "Mean advantages                         :   -0.04117\n",
      "Mean (norm.) advantages                 :   -0.04117\n",
      "Mean (discounted) returns               :   -1.26182\n",
      "Mean normalized returns                 :   -1.26182\n",
      "Mean entropy                            :    4.78732\n",
      "Variance explained by the value function:    0.19860\n",
      "Std. of action_0 over agents            :    3.18279\n",
      "Std. of action_0 over envs              :    3.17968\n",
      "Std. of action_0 over time              :    3.18411\n",
      "Std. of action_1 over agents            :    3.16197\n",
      "Std. of action_1 over envs              :    3.15870\n",
      "Std. of action_1 over time              :    3.16361\n",
      "Current timestep                        : 1100000.00000\n",
      "Gradient norm                           :    0.02074\n",
      "Mean episodic reward                    : -604.74600\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    2.75310\n",
      "Policy loss                             :    1.46008\n",
      "Value function loss                     :  139.07147\n",
      "Mean rewards                            :    0.62660\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   24.75457\n",
      "Mean advantages                         :    0.96995\n",
      "Mean (norm.) advantages                 :    0.96995\n",
      "Mean (discounted) returns               :   25.72451\n",
      "Mean normalized returns                 :   25.72451\n",
      "Mean entropy                            :    1.95383\n",
      "Variance explained by the value function:    0.23910\n",
      "Std. of action_0 over agents            :    0.33641\n",
      "Std. of action_0 over envs              :    0.44263\n",
      "Std. of action_0 over time              :    0.46271\n",
      "Std. of action_1 over agents            :    2.21386\n",
      "Std. of action_1 over envs              :    2.32202\n",
      "Std. of action_1 over time              :    2.32489\n",
      "Current timestep                        : 1100000.00000\n",
      "Gradient norm                           :    0.92750\n",
      "Mean episodic reward                    :  622.28000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 110 / 1000   \n",
      "Mean training time per iter (ms)        :     132.54\n",
      "Mean steps per sec (training time)      :   75450.48\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.39002\n",
      "Policy loss                             :   -0.19326\n",
      "Value function loss                     :    4.27940\n",
      "Mean rewards                            :   -0.03079\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.22350\n",
      "Mean advantages                         :   -0.04023\n",
      "Mean (norm.) advantages                 :   -0.04023\n",
      "Mean (discounted) returns               :   -1.26373\n",
      "Mean normalized returns                 :   -1.26373\n",
      "Mean entropy                            :    4.79115\n",
      "Variance explained by the value function:    0.19672\n",
      "Std. of action_0 over agents            :    3.17452\n",
      "Std. of action_0 over envs              :    3.17079\n",
      "Std. of action_0 over time              :    3.17553\n",
      "Std. of action_1 over agents            :    3.19096\n",
      "Std. of action_1 over envs              :    3.18752\n",
      "Std. of action_1 over time              :    3.19235\n",
      "Current timestep                        : 1200000.00000\n",
      "Gradient norm                           :    0.02688\n",
      "Mean episodic reward                    : -605.87600\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    1.38344\n",
      "Policy loss                             :    0.15169\n",
      "Value function loss                     :  132.42581\n",
      "Mean rewards                            :    0.63200\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   25.21257\n",
      "Mean advantages                         :    0.38275\n",
      "Mean (norm.) advantages                 :    0.38275\n",
      "Mean (discounted) returns               :   25.59531\n",
      "Mean normalized returns                 :   25.59531\n",
      "Mean entropy                            :    1.85001\n",
      "Variance explained by the value function:    0.26496\n",
      "Std. of action_0 over agents            :    0.54090\n",
      "Std. of action_0 over envs              :    0.59882\n",
      "Std. of action_0 over time              :    0.61370\n",
      "Std. of action_1 over agents            :    1.77144\n",
      "Std. of action_1 over envs              :    2.02314\n",
      "Std. of action_1 over time              :    2.04445\n",
      "Current timestep                        : 1200000.00000\n",
      "Gradient norm                           :    1.05419\n",
      "Mean episodic reward                    :  624.70000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 120 / 1000   \n",
      "Mean training time per iter (ms)        :     132.37\n",
      "Mean steps per sec (training time)      :   75547.62\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    0.07034\n",
      "Policy loss                             :    0.26758\n",
      "Value function loss                     :    4.21671\n",
      "Mean rewards                            :   -0.03039\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.32277\n",
      "Mean advantages                         :    0.05596\n",
      "Mean (norm.) advantages                 :    0.05596\n",
      "Mean (discounted) returns               :   -1.26681\n",
      "Mean normalized returns                 :   -1.26681\n",
      "Mean entropy                            :    4.78819\n",
      "Variance explained by the value function:    0.20097\n",
      "Std. of action_0 over agents            :    3.18787\n",
      "Std. of action_0 over envs              :    3.18447\n",
      "Std. of action_0 over time              :    3.18937\n",
      "Std. of action_1 over agents            :    3.18469\n",
      "Std. of action_1 over envs              :    3.18079\n",
      "Std. of action_1 over time              :    3.18619\n",
      "Current timestep                        : 1300000.00000\n",
      "Gradient norm                           :    0.02152\n",
      "Mean episodic reward                    : -614.82200\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -1.28483\n",
      "Policy loss                             :   -2.41039\n",
      "Value function loss                     :  120.68580\n",
      "Mean rewards                            :    0.62540\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   26.84370\n",
      "Mean advantages                         :   -1.51105\n",
      "Mean (norm.) advantages                 :   -1.51105\n",
      "Mean (discounted) returns               :   25.33265\n",
      "Mean normalized returns                 :   25.33265\n",
      "Mean entropy                            :    1.62589\n",
      "Variance explained by the value function:    0.29816\n",
      "Std. of action_0 over agents            :    0.50313\n",
      "Std. of action_0 over envs              :    0.59231\n",
      "Std. of action_0 over time              :    0.61607\n",
      "Std. of action_1 over agents            :    1.22392\n",
      "Std. of action_1 over envs              :    1.56280\n",
      "Std. of action_1 over time              :    1.62526\n",
      "Current timestep                        : 1300000.00000\n",
      "Gradient norm                           :    0.96978\n",
      "Mean episodic reward                    :  631.48000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 130 / 1000   \n",
      "Mean training time per iter (ms)        :     132.22\n",
      "Mean steps per sec (training time)      :   75632.89\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    0.00544\n",
      "Policy loss                             :    0.20296\n",
      "Value function loss                     :    4.18451\n",
      "Mean rewards                            :   -0.02998\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.27892\n",
      "Mean advantages                         :    0.04237\n",
      "Mean (norm.) advantages                 :    0.04237\n",
      "Mean (discounted) returns               :   -1.23654\n",
      "Mean normalized returns                 :   -1.23654\n",
      "Mean entropy                            :    4.78746\n",
      "Variance explained by the value function:    0.19364\n",
      "Std. of action_0 over agents            :    3.16542\n",
      "Std. of action_0 over envs              :    3.16184\n",
      "Std. of action_0 over time              :    3.16614\n",
      "Std. of action_1 over agents            :    3.19429\n",
      "Std. of action_1 over envs              :    3.19155\n",
      "Std. of action_1 over time              :    3.19567\n",
      "Current timestep                        : 1400000.00000\n",
      "Gradient norm                           :    0.02590\n",
      "Mean episodic reward                    : -608.21800\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.18401\n",
      "Policy loss                             :   -1.24549\n",
      "Value function loss                     :  117.24991\n",
      "Mean rewards                            :    0.62040\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   24.58486\n",
      "Mean advantages                         :   -0.49179\n",
      "Mean (norm.) advantages                 :   -0.49179\n",
      "Mean (discounted) returns               :   24.09308\n",
      "Mean normalized returns                 :   24.09308\n",
      "Mean entropy                            :    2.22046\n",
      "Variance explained by the value function:    0.33025\n",
      "Std. of action_0 over agents            :    1.10082\n",
      "Std. of action_0 over envs              :    1.21927\n",
      "Std. of action_0 over time              :    1.27917\n",
      "Std. of action_1 over agents            :    1.50083\n",
      "Std. of action_1 over envs              :    1.78938\n",
      "Std. of action_1 over time              :    1.83957\n",
      "Current timestep                        : 1400000.00000\n",
      "Gradient norm                           :    0.86178\n",
      "Mean episodic reward                    :  626.04000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 140 / 1000   \n",
      "Mean training time per iter (ms)        :     132.06\n",
      "Mean steps per sec (training time)      :   75722.11\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.41031\n",
      "Policy loss                             :   -0.21337\n",
      "Value function loss                     :    4.25794\n",
      "Mean rewards                            :   -0.03000\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.20284\n",
      "Mean advantages                         :   -0.04444\n",
      "Mean (norm.) advantages                 :   -0.04444\n",
      "Mean (discounted) returns               :   -1.24728\n",
      "Mean normalized returns                 :   -1.24728\n",
      "Mean entropy                            :    4.79041\n",
      "Variance explained by the value function:    0.19141\n",
      "Std. of action_0 over agents            :    3.17172\n",
      "Std. of action_0 over envs              :    3.16820\n",
      "Std. of action_0 over time              :    3.17362\n",
      "Std. of action_1 over agents            :    3.18974\n",
      "Std. of action_1 over envs              :    3.18661\n",
      "Std. of action_1 over time              :    3.19123\n",
      "Current timestep                        : 1500000.00000\n",
      "Gradient norm                           :    0.01839\n",
      "Mean episodic reward                    : -598.17600\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    1.32612\n",
      "Policy loss                             :    0.24926\n",
      "Value function loss                     :  121.56894\n",
      "Mean rewards                            :    0.61660\n",
      "Max. rewards                            :   30.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   23.76665\n",
      "Mean advantages                         :    0.15925\n",
      "Mean (norm.) advantages                 :    0.15925\n",
      "Mean (discounted) returns               :   23.92591\n",
      "Mean normalized returns                 :   23.92591\n",
      "Mean entropy                            :    2.77672\n",
      "Variance explained by the value function:    0.37911\n",
      "Std. of action_0 over agents            :    1.59717\n",
      "Std. of action_0 over envs              :    1.64885\n",
      "Std. of action_0 over time              :    1.65476\n",
      "Std. of action_1 over agents            :    2.29025\n",
      "Std. of action_1 over envs              :    2.40091\n",
      "Std. of action_1 over time              :    2.41089\n",
      "Current timestep                        : 1500000.00000\n",
      "Gradient norm                           :    1.04956\n",
      "Mean episodic reward                    :  617.52000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 150 / 1000   \n",
      "Mean training time per iter (ms)        :     131.96\n",
      "Mean steps per sec (training time)      :   75778.85\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.30629\n",
      "Policy loss                             :   -0.10944\n",
      "Value function loss                     :    4.25746\n",
      "Mean rewards                            :   -0.02960\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.22184\n",
      "Mean advantages                         :   -0.02271\n",
      "Mean (norm.) advantages                 :   -0.02271\n",
      "Mean (discounted) returns               :   -1.24455\n",
      "Mean normalized returns                 :   -1.24455\n",
      "Mean entropy                            :    4.78840\n",
      "Variance explained by the value function:    0.18661\n",
      "Std. of action_0 over agents            :    3.18294\n",
      "Std. of action_0 over envs              :    3.17974\n",
      "Std. of action_0 over time              :    3.18427\n",
      "Std. of action_1 over agents            :    3.18077\n",
      "Std. of action_1 over envs              :    3.17743\n",
      "Std. of action_1 over time              :    3.18236\n",
      "Current timestep                        : 1600000.00000\n",
      "Gradient norm                           :    0.02283\n",
      "Mean episodic reward                    : -594.66800\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    0.11075\n",
      "Policy loss                             :   -0.99814\n",
      "Value function loss                     :  124.14204\n",
      "Mean rewards                            :    0.61020\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   24.20610\n",
      "Mean advantages                         :   -0.38055\n",
      "Mean (norm.) advantages                 :   -0.38055\n",
      "Mean (discounted) returns               :   23.82555\n",
      "Mean normalized returns                 :   23.82555\n",
      "Mean entropy                            :    2.65065\n",
      "Variance explained by the value function:    0.36260\n",
      "Std. of action_0 over agents            :    1.31017\n",
      "Std. of action_0 over envs              :    1.38641\n",
      "Std. of action_0 over time              :    1.39297\n",
      "Std. of action_1 over agents            :    2.24370\n",
      "Std. of action_1 over envs              :    2.36572\n",
      "Std. of action_1 over time              :    2.37353\n",
      "Current timestep                        : 1600000.00000\n",
      "Gradient norm                           :    0.86174\n",
      "Mean episodic reward                    :  614.16000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 160 / 1000   \n",
      "Mean training time per iter (ms)        :     131.89\n",
      "Mean steps per sec (training time)      :   75822.87\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    0.02414\n",
      "Policy loss                             :    0.22181\n",
      "Value function loss                     :    4.18004\n",
      "Mean rewards                            :   -0.03014\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.30193\n",
      "Mean advantages                         :    0.04642\n",
      "Mean (norm.) advantages                 :    0.04642\n",
      "Mean (discounted) returns               :   -1.25551\n",
      "Mean normalized returns                 :   -1.25551\n",
      "Mean entropy                            :    4.78947\n",
      "Variance explained by the value function:    0.20049\n",
      "Std. of action_0 over agents            :    3.19362\n",
      "Std. of action_0 over envs              :    3.19033\n",
      "Std. of action_0 over time              :    3.19515\n",
      "Std. of action_1 over agents            :    3.13914\n",
      "Std. of action_1 over envs              :    3.13572\n",
      "Std. of action_1 over time              :    3.14080\n",
      "Current timestep                        : 1700000.00000\n",
      "Gradient norm                           :    0.01649\n",
      "Mean episodic reward                    : -610.47400\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.32916\n",
      "Policy loss                             :   -1.36375\n",
      "Value function loss                     :  115.13750\n",
      "Mean rewards                            :    0.62180\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   24.30268\n",
      "Mean advantages                         :   -0.59685\n",
      "Mean (norm.) advantages                 :   -0.59685\n",
      "Mean (discounted) returns               :   23.70583\n",
      "Mean normalized returns                 :   23.70583\n",
      "Mean entropy                            :    2.33580\n",
      "Variance explained by the value function:    0.39727\n",
      "Std. of action_0 over agents            :    0.94601\n",
      "Std. of action_0 over envs              :    1.08593\n",
      "Std. of action_0 over time              :    1.21527\n",
      "Std. of action_1 over agents            :    2.09410\n",
      "Std. of action_1 over envs              :    2.25233\n",
      "Std. of action_1 over time              :    2.26746\n",
      "Current timestep                        : 1700000.00000\n",
      "Gradient norm                           :    1.10716\n",
      "Mean episodic reward                    :  628.58000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 170 / 1000   \n",
      "Mean training time per iter (ms)        :     131.79\n",
      "Mean steps per sec (training time)      :   75876.73\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.20829\n",
      "Policy loss                             :   -0.01122\n",
      "Value function loss                     :    4.24434\n",
      "Mean rewards                            :   -0.03009\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.24558\n",
      "Mean advantages                         :   -0.00225\n",
      "Mean (norm.) advantages                 :   -0.00225\n",
      "Mean (discounted) returns               :   -1.24783\n",
      "Mean normalized returns                 :   -1.24783\n",
      "Mean entropy                            :    4.79017\n",
      "Variance explained by the value function:    0.19038\n",
      "Std. of action_0 over agents            :    3.15255\n",
      "Std. of action_0 over envs              :    3.14831\n",
      "Std. of action_0 over time              :    3.15344\n",
      "Std. of action_1 over agents            :    3.16293\n",
      "Std. of action_1 over envs              :    3.15896\n",
      "Std. of action_1 over time              :    3.16376\n",
      "Current timestep                        : 1800000.00000\n",
      "Gradient norm                           :    0.01817\n",
      "Mean episodic reward                    : -606.14600\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    0.19917\n",
      "Policy loss                             :   -0.77462\n",
      "Value function loss                     :  109.51952\n",
      "Mean rewards                            :    0.62060\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   23.86355\n",
      "Mean advantages                         :   -0.33225\n",
      "Mean (norm.) advantages                 :   -0.33225\n",
      "Mean (discounted) returns               :   23.53131\n",
      "Mean normalized returns                 :   23.53131\n",
      "Mean entropy                            :    2.42816\n",
      "Variance explained by the value function:    0.42407\n",
      "Std. of action_0 over agents            :    1.38931\n",
      "Std. of action_0 over envs              :    1.44448\n",
      "Std. of action_0 over time              :    1.47183\n",
      "Std. of action_1 over agents            :    2.31901\n",
      "Std. of action_1 over envs              :    2.42196\n",
      "Std. of action_1 over time              :    2.43184\n",
      "Current timestep                        : 1800000.00000\n",
      "Gradient norm                           :    1.05716\n",
      "Mean episodic reward                    :  623.88000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 180 / 1000   \n",
      "Mean training time per iter (ms)        :     131.91\n",
      "Mean steps per sec (training time)      :   75809.11\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.42851\n",
      "Policy loss                             :   -0.23205\n",
      "Value function loss                     :    4.29204\n",
      "Mean rewards                            :   -0.03097\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.20935\n",
      "Mean advantages                         :   -0.04841\n",
      "Mean (norm.) advantages                 :   -0.04841\n",
      "Mean (discounted) returns               :   -1.25776\n",
      "Mean normalized returns                 :   -1.25776\n",
      "Mean entropy                            :    4.78749\n",
      "Variance explained by the value function:    0.19585\n",
      "Std. of action_0 over agents            :    3.18641\n",
      "Std. of action_0 over envs              :    3.18215\n",
      "Std. of action_0 over time              :    3.18681\n",
      "Std. of action_1 over agents            :    3.19706\n",
      "Std. of action_1 over envs              :    3.19332\n",
      "Std. of action_1 over time              :    3.19855\n",
      "Current timestep                        : 1900000.00000\n",
      "Gradient norm                           :    0.02202\n",
      "Mean episodic reward                    : -615.55000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    0.98144\n",
      "Policy loss                             :   -0.16142\n",
      "Value function loss                     :  124.75267\n",
      "Mean rewards                            :    0.63580\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   24.12560\n",
      "Mean advantages                         :   -0.11957\n",
      "Mean (norm.) advantages                 :   -0.11957\n",
      "Mean (discounted) returns               :   24.00603\n",
      "Mean normalized returns                 :   24.00603\n",
      "Mean entropy                            :    2.09317\n",
      "Variance explained by the value function:    0.38898\n",
      "Std. of action_0 over agents            :    1.16746\n",
      "Std. of action_0 over envs              :    1.26150\n",
      "Std. of action_0 over time              :    1.32636\n",
      "Std. of action_1 over agents            :    1.95041\n",
      "Std. of action_1 over envs              :    2.18977\n",
      "Std. of action_1 over time              :    2.27316\n",
      "Current timestep                        : 1900000.00000\n",
      "Gradient norm                           :    1.04704\n",
      "Mean episodic reward                    :  633.00000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 190 / 1000   \n",
      "Mean training time per iter (ms)        :     131.84\n",
      "Mean steps per sec (training time)      :   75852.18\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.08543\n",
      "Policy loss                             :    0.11124\n",
      "Value function loss                     :    4.27781\n",
      "Mean rewards                            :   -0.03067\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.29198\n",
      "Mean advantages                         :    0.02349\n",
      "Mean (norm.) advantages                 :    0.02349\n",
      "Mean (discounted) returns               :   -1.26849\n",
      "Mean normalized returns                 :   -1.26849\n",
      "Mean entropy                            :    4.78886\n",
      "Variance explained by the value function:    0.19626\n",
      "Std. of action_0 over agents            :    3.17970\n",
      "Std. of action_0 over envs              :    3.17575\n",
      "Std. of action_0 over time              :    3.18013\n",
      "Std. of action_1 over agents            :    3.17027\n",
      "Std. of action_1 over envs              :    3.16672\n",
      "Std. of action_1 over time              :    3.17144\n",
      "Current timestep                        : 2000000.00000\n",
      "Gradient norm                           :    0.02273\n",
      "Mean episodic reward                    : -616.72000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    1.64687\n",
      "Policy loss                             :    0.58691\n",
      "Value function loss                     :  116.93488\n",
      "Mean rewards                            :    0.63000\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   23.57109\n",
      "Mean advantages                         :    0.33452\n",
      "Mean (norm.) advantages                 :    0.33452\n",
      "Mean (discounted) returns               :   23.90561\n",
      "Mean normalized returns                 :   23.90561\n",
      "Mean entropy                            :    2.18783\n",
      "Variance explained by the value function:    0.43835\n",
      "Std. of action_0 over agents            :    1.66334\n",
      "Std. of action_0 over envs              :    1.77110\n",
      "Std. of action_0 over time              :    1.80041\n",
      "Std. of action_1 over agents            :    2.40191\n",
      "Std. of action_1 over envs              :    2.50231\n",
      "Std. of action_1 over time              :    2.53251\n",
      "Current timestep                        : 2000000.00000\n",
      "Gradient norm                           :    0.77135\n",
      "Mean episodic reward                    :  632.76000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 200 / 1000   \n",
      "Mean training time per iter (ms)        :     131.79\n",
      "Mean steps per sec (training time)      :   75880.73\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.13355\n",
      "Policy loss                             :    0.06291\n",
      "Value function loss                     :    4.27997\n",
      "Mean rewards                            :   -0.03061\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.28207\n",
      "Mean advantages                         :    0.01329\n",
      "Mean (norm.) advantages                 :    0.01329\n",
      "Mean (discounted) returns               :   -1.26878\n",
      "Mean normalized returns                 :   -1.26878\n",
      "Mean entropy                            :    4.78521\n",
      "Variance explained by the value function:    0.19319\n",
      "Std. of action_0 over agents            :    3.16152\n",
      "Std. of action_0 over envs              :    3.15819\n",
      "Std. of action_0 over time              :    3.16267\n",
      "Std. of action_1 over agents            :    3.17672\n",
      "Std. of action_1 over envs              :    3.17344\n",
      "Std. of action_1 over time              :    3.17818\n",
      "Current timestep                        : 2100000.00000\n",
      "Gradient norm                           :    0.02232\n",
      "Mean episodic reward                    : -621.98600\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    1.42578\n",
      "Policy loss                             :    0.31507\n",
      "Value function loss                     :  122.08124\n",
      "Mean rewards                            :    0.62960\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   23.93601\n",
      "Mean advantages                         :    0.13993\n",
      "Mean (norm.) advantages                 :    0.13993\n",
      "Mean (discounted) returns               :   24.07594\n",
      "Mean normalized returns                 :   24.07594\n",
      "Mean entropy                            :    2.20208\n",
      "Variance explained by the value function:    0.38437\n",
      "Std. of action_0 over agents            :    1.76383\n",
      "Std. of action_0 over envs              :    1.86312\n",
      "Std. of action_0 over time              :    1.88367\n",
      "Std. of action_1 over agents            :    2.17340\n",
      "Std. of action_1 over envs              :    2.34448\n",
      "Std. of action_1 over time              :    2.41432\n",
      "Current timestep                        : 2100000.00000\n",
      "Gradient norm                           :    1.08965\n",
      "Mean episodic reward                    :  635.40000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 210 / 1000   \n",
      "Mean training time per iter (ms)        :     131.77\n",
      "Mean steps per sec (training time)      :   75891.75\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.55194\n",
      "Policy loss                             :   -0.35554\n",
      "Value function loss                     :    4.29110\n",
      "Mean rewards                            :   -0.03062\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.18174\n",
      "Mean advantages                         :   -0.07408\n",
      "Mean (norm.) advantages                 :   -0.07408\n",
      "Mean (discounted) returns               :   -1.25582\n",
      "Mean normalized returns                 :   -1.25582\n",
      "Mean entropy                            :    4.78629\n",
      "Variance explained by the value function:    0.19081\n",
      "Std. of action_0 over agents            :    3.18329\n",
      "Std. of action_0 over envs              :    3.17954\n",
      "Std. of action_0 over time              :    3.18449\n",
      "Std. of action_1 over agents            :    3.19019\n",
      "Std. of action_1 over envs              :    3.18702\n",
      "Std. of action_1 over time              :    3.19155\n",
      "Current timestep                        : 2200000.00000\n",
      "Gradient norm                           :    0.01847\n",
      "Mean episodic reward                    : -615.82000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    4.05776\n",
      "Policy loss                             :    2.98207\n",
      "Value function loss                     :  119.69363\n",
      "Mean rewards                            :    0.62780\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   22.43901\n",
      "Mean advantages                         :    1.34366\n",
      "Mean (norm.) advantages                 :    1.34366\n",
      "Mean (discounted) returns               :   23.78267\n",
      "Mean normalized returns                 :   23.78267\n",
      "Mean entropy                            :    2.42494\n",
      "Variance explained by the value function:    0.40296\n",
      "Std. of action_0 over agents            :    1.55592\n",
      "Std. of action_0 over envs              :    1.75418\n",
      "Std. of action_0 over time              :    1.88715\n",
      "Std. of action_1 over agents            :    2.20406\n",
      "Std. of action_1 over envs              :    2.36238\n",
      "Std. of action_1 over time              :    2.42003\n",
      "Current timestep                        : 2200000.00000\n",
      "Gradient norm                           :    1.11617\n",
      "Mean episodic reward                    :  631.42000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 220 / 1000   \n",
      "Mean training time per iter (ms)        :     131.79\n",
      "Mean steps per sec (training time)      :   75877.68\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.31791\n",
      "Policy loss                             :   -0.12191\n",
      "Value function loss                     :    4.29101\n",
      "Mean rewards                            :   -0.03028\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.24447\n",
      "Mean advantages                         :   -0.02517\n",
      "Mean (norm.) advantages                 :   -0.02517\n",
      "Mean (discounted) returns               :   -1.26964\n",
      "Mean normalized returns                 :   -1.26964\n",
      "Mean entropy                            :    4.77825\n",
      "Variance explained by the value function:    0.19055\n",
      "Std. of action_0 over agents            :    3.23531\n",
      "Std. of action_0 over envs              :    3.23149\n",
      "Std. of action_0 over time              :    3.23582\n",
      "Std. of action_1 over agents            :    3.23770\n",
      "Std. of action_1 over envs              :    3.23386\n",
      "Std. of action_1 over time              :    3.23866\n",
      "Current timestep                        : 2300000.00000\n",
      "Gradient norm                           :    0.03656\n",
      "Mean episodic reward                    : -620.32200\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    4.11510\n",
      "Policy loss                             :    2.96850\n",
      "Value function loss                     :  127.32042\n",
      "Mean rewards                            :    0.62220\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   22.82997\n",
      "Mean advantages                         :    1.24553\n",
      "Mean (norm.) advantages                 :    1.24553\n",
      "Mean (discounted) returns               :   24.07550\n",
      "Mean normalized returns                 :   24.07550\n",
      "Mean entropy                            :    2.53215\n",
      "Variance explained by the value function:    0.36879\n",
      "Std. of action_0 over agents            :    3.15088\n",
      "Std. of action_0 over envs              :    3.35648\n",
      "Std. of action_0 over time              :    3.37690\n",
      "Std. of action_1 over agents            :    2.19397\n",
      "Std. of action_1 over envs              :    2.34609\n",
      "Std. of action_1 over time              :    2.40001\n",
      "Current timestep                        : 2300000.00000\n",
      "Gradient norm                           :    1.12607\n",
      "Mean episodic reward                    :  637.84000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 230 / 1000   \n",
      "Mean training time per iter (ms)        :     131.74\n",
      "Mean steps per sec (training time)      :   75909.50\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.12808\n",
      "Policy loss                             :    0.06856\n",
      "Value function loss                     :    4.27281\n",
      "Mean rewards                            :   -0.03011\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.29624\n",
      "Mean advantages                         :    0.01435\n",
      "Mean (norm.) advantages                 :    0.01435\n",
      "Mean (discounted) returns               :   -1.28189\n",
      "Mean normalized returns                 :   -1.28189\n",
      "Mean entropy                            :    4.78748\n",
      "Variance explained by the value function:    0.19714\n",
      "Std. of action_0 over agents            :    3.19865\n",
      "Std. of action_0 over envs              :    3.19564\n",
      "Std. of action_0 over time              :    3.20017\n",
      "Std. of action_1 over agents            :    3.17714\n",
      "Std. of action_1 over envs              :    3.17289\n",
      "Std. of action_1 over time              :    3.17825\n",
      "Current timestep                        : 2400000.00000\n",
      "Gradient norm                           :    0.02217\n",
      "Mean episodic reward                    : -594.38400\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    1.22460\n",
      "Policy loss                             :    0.09114\n",
      "Value function loss                     :  123.45505\n",
      "Mean rewards                            :    0.61940\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   23.95072\n",
      "Mean advantages                         :    0.15197\n",
      "Mean (norm.) advantages                 :    0.15197\n",
      "Mean (discounted) returns               :   24.10269\n",
      "Mean normalized returns                 :   24.10269\n",
      "Mean entropy                            :    2.02174\n",
      "Variance explained by the value function:    0.40530\n",
      "Std. of action_0 over agents            :    1.24395\n",
      "Std. of action_0 over envs              :    1.61606\n",
      "Std. of action_0 over time              :    1.74416\n",
      "Std. of action_1 over agents            :    2.19025\n",
      "Std. of action_1 over envs              :    2.32380\n",
      "Std. of action_1 over time              :    2.38768\n",
      "Current timestep                        : 2400000.00000\n",
      "Gradient norm                           :    0.82868\n",
      "Mean episodic reward                    :  613.96000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 240 / 1000   \n",
      "Mean training time per iter (ms)        :     131.65\n",
      "Mean steps per sec (training time)      :   75958.06\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.30096\n",
      "Policy loss                             :   -0.10410\n",
      "Value function loss                     :    4.26636\n",
      "Mean rewards                            :   -0.03140\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.26093\n",
      "Mean advantages                         :   -0.02166\n",
      "Mean (norm.) advantages                 :   -0.02166\n",
      "Mean (discounted) returns               :   -1.28259\n",
      "Mean normalized returns                 :   -1.28259\n",
      "Mean entropy                            :    4.79065\n",
      "Variance explained by the value function:    0.20772\n",
      "Std. of action_0 over agents            :    3.16324\n",
      "Std. of action_0 over envs              :    3.16009\n",
      "Std. of action_0 over time              :    3.16499\n",
      "Std. of action_1 over agents            :    3.14587\n",
      "Std. of action_1 over envs              :    3.14183\n",
      "Std. of action_1 over time              :    3.14709\n",
      "Current timestep                        : 2500000.00000\n",
      "Gradient norm                           :    0.01473\n",
      "Mean episodic reward                    : -615.28600\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    2.87641\n",
      "Policy loss                             :    1.84401\n",
      "Value function loss                     :  115.04574\n",
      "Mean rewards                            :    0.64240\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   23.24988\n",
      "Mean advantages                         :    0.87736\n",
      "Mean (norm.) advantages                 :    0.87736\n",
      "Mean (discounted) returns               :   24.12724\n",
      "Mean normalized returns                 :   24.12724\n",
      "Mean entropy                            :    2.36099\n",
      "Variance explained by the value function:    0.42961\n",
      "Std. of action_0 over agents            :    1.85476\n",
      "Std. of action_0 over envs              :    1.97942\n",
      "Std. of action_0 over time              :    2.03621\n",
      "Std. of action_1 over agents            :    1.88634\n",
      "Std. of action_1 over envs              :    2.13732\n",
      "Std. of action_1 over time              :    2.25540\n",
      "Current timestep                        : 2500000.00000\n",
      "Gradient norm                           :    0.95807\n",
      "Mean episodic reward                    :  630.76000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 250 / 1000   \n",
      "Mean training time per iter (ms)        :     131.64\n",
      "Mean steps per sec (training time)      :   75964.50\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.38766\n",
      "Policy loss                             :   -0.19114\n",
      "Value function loss                     :    4.29504\n",
      "Mean rewards                            :   -0.03145\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.24266\n",
      "Mean advantages                         :   -0.04000\n",
      "Mean (norm.) advantages                 :   -0.04000\n",
      "Mean (discounted) returns               :   -1.28266\n",
      "Mean normalized returns                 :   -1.28266\n",
      "Mean entropy                            :    4.78925\n",
      "Variance explained by the value function:    0.20545\n",
      "Std. of action_0 over agents            :    3.16964\n",
      "Std. of action_0 over envs              :    3.16577\n",
      "Std. of action_0 over time              :    3.17155\n",
      "Std. of action_1 over agents            :    3.19146\n",
      "Std. of action_1 over envs              :    3.18798\n",
      "Std. of action_1 over time              :    3.19313\n",
      "Current timestep                        : 2600000.00000\n",
      "Gradient norm                           :    0.02372\n",
      "Mean episodic reward                    : -621.43600\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    4.27217\n",
      "Policy loss                             :    3.13305\n",
      "Value function loss                     :  125.62384\n",
      "Mean rewards                            :    0.63980\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   22.81732\n",
      "Mean advantages                         :    1.42910\n",
      "Mean (norm.) advantages                 :    1.42910\n",
      "Mean (discounted) returns               :   24.24642\n",
      "Mean normalized returns                 :   24.24642\n",
      "Mean entropy                            :    2.34247\n",
      "Variance explained by the value function:    0.39670\n",
      "Std. of action_0 over agents            :    1.66779\n",
      "Std. of action_0 over envs              :    1.81548\n",
      "Std. of action_0 over time              :    1.87217\n",
      "Std. of action_1 over agents            :    2.11618\n",
      "Std. of action_1 over envs              :    2.30114\n",
      "Std. of action_1 over time              :    2.37297\n",
      "Current timestep                        : 2600000.00000\n",
      "Gradient norm                           :    1.16582\n",
      "Mean episodic reward                    :  637.22000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 260 / 1000   \n",
      "Mean training time per iter (ms)        :     131.64\n",
      "Mean steps per sec (training time)      :   75966.86\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    0.18943\n",
      "Policy loss                             :    0.38655\n",
      "Value function loss                     :    4.21191\n",
      "Mean rewards                            :   -0.02945\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.33643\n",
      "Mean advantages                         :    0.08120\n",
      "Mean (norm.) advantages                 :    0.08120\n",
      "Mean (discounted) returns               :   -1.25523\n",
      "Mean normalized returns                 :   -1.25523\n",
      "Mean entropy                            :    4.78478\n",
      "Variance explained by the value function:    0.19130\n",
      "Std. of action_0 over agents            :    3.16777\n",
      "Std. of action_0 over envs              :    3.16463\n",
      "Std. of action_0 over time              :    3.16860\n",
      "Std. of action_1 over agents            :    3.17843\n",
      "Std. of action_1 over envs              :    3.17478\n",
      "Std. of action_1 over time              :    3.17978\n",
      "Current timestep                        : 2700000.00000\n",
      "Gradient norm                           :    0.01877\n",
      "Mean episodic reward                    : -602.01000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    2.00235\n",
      "Policy loss                             :    0.90322\n",
      "Value function loss                     :  123.10245\n",
      "Mean rewards                            :    0.60860\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   23.18112\n",
      "Mean advantages                         :    0.37243\n",
      "Mean (norm.) advantages                 :    0.37243\n",
      "Mean (discounted) returns               :   23.55355\n",
      "Mean normalized returns                 :   23.55355\n",
      "Mean entropy                            :    2.63800\n",
      "Variance explained by the value function:    0.38624\n",
      "Std. of action_0 over agents            :    3.19479\n",
      "Std. of action_0 over envs              :    3.35118\n",
      "Std. of action_0 over time              :    3.35966\n",
      "Std. of action_1 over agents            :    1.39973\n",
      "Std. of action_1 over envs              :    1.70670\n",
      "Std. of action_1 over time              :    1.85653\n",
      "Current timestep                        : 2700000.00000\n",
      "Gradient norm                           :    1.10109\n",
      "Mean episodic reward                    :  619.46000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 270 / 1000   \n",
      "Mean training time per iter (ms)        :     131.60\n",
      "Mean steps per sec (training time)      :   75988.60\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.13503\n",
      "Policy loss                             :    0.06095\n",
      "Value function loss                     :    4.27977\n",
      "Mean rewards                            :   -0.02966\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.25697\n",
      "Mean advantages                         :    0.01310\n",
      "Mean (norm.) advantages                 :    0.01310\n",
      "Mean (discounted) returns               :   -1.24387\n",
      "Mean normalized returns                 :   -1.24387\n",
      "Mean entropy                            :    4.77557\n",
      "Variance explained by the value function:    0.17858\n",
      "Std. of action_0 over agents            :    3.18454\n",
      "Std. of action_0 over envs              :    3.18118\n",
      "Std. of action_0 over time              :    3.18539\n",
      "Std. of action_1 over agents            :    3.25448\n",
      "Std. of action_1 over envs              :    3.25142\n",
      "Std. of action_1 over time              :    3.25563\n",
      "Current timestep                        : 2800000.00000\n",
      "Gradient norm                           :    0.03724\n",
      "Mean episodic reward                    : -597.61600\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.25420\n",
      "Policy loss                             :   -1.37512\n",
      "Value function loss                     :  126.51085\n",
      "Mean rewards                            :    0.61000\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   24.40313\n",
      "Mean advantages                         :   -0.50740\n",
      "Mean (norm.) advantages                 :   -0.50740\n",
      "Mean (discounted) returns               :   23.89573\n",
      "Mean normalized returns                 :   23.89573\n",
      "Mean entropy                            :    2.88382\n",
      "Variance explained by the value function:    0.34452\n",
      "Std. of action_0 over agents            :    2.32531\n",
      "Std. of action_0 over envs              :    2.68173\n",
      "Std. of action_0 over time              :    2.76141\n",
      "Std. of action_1 over agents            :    2.44716\n",
      "Std. of action_1 over envs              :    2.54633\n",
      "Std. of action_1 over time              :    2.55902\n",
      "Current timestep                        : 2800000.00000\n",
      "Gradient norm                           :    0.77421\n",
      "Mean episodic reward                    :  614.42000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 280 / 1000   \n",
      "Mean training time per iter (ms)        :     131.68\n",
      "Mean steps per sec (training time)      :   75943.75\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.31438\n",
      "Policy loss                             :   -0.11753\n",
      "Value function loss                     :    4.26448\n",
      "Mean rewards                            :   -0.03045\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.21748\n",
      "Mean advantages                         :   -0.02442\n",
      "Mean (norm.) advantages                 :   -0.02442\n",
      "Mean (discounted) returns               :   -1.24190\n",
      "Mean normalized returns                 :   -1.24190\n",
      "Mean entropy                            :    4.78994\n",
      "Variance explained by the value function:    0.19299\n",
      "Std. of action_0 over agents            :    3.17974\n",
      "Std. of action_0 over envs              :    3.17640\n",
      "Std. of action_0 over time              :    3.18113\n",
      "Std. of action_1 over agents            :    3.18217\n",
      "Std. of action_1 over envs              :    3.17814\n",
      "Std. of action_1 over time              :    3.18369\n",
      "Current timestep                        : 2900000.00000\n",
      "Gradient norm                           :    0.01567\n",
      "Mean episodic reward                    : -612.18400\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.98043\n",
      "Policy loss                             :   -2.14283\n",
      "Value function loss                     :  128.53186\n",
      "Mean rewards                            :    0.62820\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   24.90007\n",
      "Mean advantages                         :   -0.76071\n",
      "Mean (norm.) advantages                 :   -0.76071\n",
      "Mean (discounted) returns               :   24.13935\n",
      "Mean normalized returns                 :   24.13935\n",
      "Mean entropy                            :    2.45836\n",
      "Variance explained by the value function:    0.38491\n",
      "Std. of action_0 over agents            :    1.84017\n",
      "Std. of action_0 over envs              :    2.07527\n",
      "Std. of action_0 over time              :    2.17418\n",
      "Std. of action_1 over agents            :    2.30525\n",
      "Std. of action_1 over envs              :    2.42692\n",
      "Std. of action_1 over time              :    2.47542\n",
      "Current timestep                        : 2900000.00000\n",
      "Gradient norm                           :    0.94925\n",
      "Mean episodic reward                    :  629.04000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 290 / 1000   \n",
      "Mean training time per iter (ms)        :     131.64\n",
      "Mean steps per sec (training time)      :   75963.11\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.45456\n",
      "Policy loss                             :   -0.25884\n",
      "Value function loss                     :    4.35929\n",
      "Mean rewards                            :   -0.03154\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.25759\n",
      "Mean advantages                         :   -0.05392\n",
      "Mean (norm.) advantages                 :   -0.05392\n",
      "Mean (discounted) returns               :   -1.31151\n",
      "Mean normalized returns                 :   -1.31151\n",
      "Mean entropy                            :    4.78627\n",
      "Variance explained by the value function:    0.20286\n",
      "Std. of action_0 over agents            :    3.18172\n",
      "Std. of action_0 over envs              :    3.17854\n",
      "Std. of action_0 over time              :    3.18299\n",
      "Std. of action_1 over agents            :    3.19275\n",
      "Std. of action_1 over envs              :    3.18950\n",
      "Std. of action_1 over time              :    3.19387\n",
      "Current timestep                        : 3000000.00000\n",
      "Gradient norm                           :    0.02393\n",
      "Mean episodic reward                    : -610.13600\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    2.42917\n",
      "Policy loss                             :    1.30917\n",
      "Value function loss                     :  124.75227\n",
      "Mean rewards                            :    0.64600\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   24.75241\n",
      "Mean advantages                         :    0.43349\n",
      "Mean (norm.) advantages                 :    0.43349\n",
      "Mean (discounted) returns               :   25.18590\n",
      "Mean normalized returns                 :   25.18590\n",
      "Mean entropy                            :    2.55037\n",
      "Variance explained by the value function:    0.39099\n",
      "Std. of action_0 over agents            :    2.04294\n",
      "Std. of action_0 over envs              :    2.32819\n",
      "Std. of action_0 over time              :    2.38869\n",
      "Std. of action_1 over agents            :    1.97610\n",
      "Std. of action_1 over envs              :    2.19584\n",
      "Std. of action_1 over time              :    2.28606\n",
      "Current timestep                        : 3000000.00000\n",
      "Gradient norm                           :    0.91051\n",
      "Mean episodic reward                    :  627.40000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 300 / 1000   \n",
      "Mean training time per iter (ms)        :     131.69\n",
      "Mean steps per sec (training time)      :   75936.21\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.15840\n",
      "Policy loss                             :    0.03821\n",
      "Value function loss                     :    4.27348\n",
      "Mean rewards                            :   -0.02991\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.26943\n",
      "Mean advantages                         :    0.00830\n",
      "Mean (norm.) advantages                 :    0.00830\n",
      "Mean (discounted) returns               :   -1.26113\n",
      "Mean normalized returns                 :   -1.26113\n",
      "Mean entropy                            :    4.78688\n",
      "Variance explained by the value function:    0.18601\n",
      "Std. of action_0 over agents            :    3.16144\n",
      "Std. of action_0 over envs              :    3.15830\n",
      "Std. of action_0 over time              :    3.16259\n",
      "Std. of action_1 over agents            :    3.16965\n",
      "Std. of action_1 over envs              :    3.16622\n",
      "Std. of action_1 over time              :    3.17115\n",
      "Current timestep                        : 3100000.00000\n",
      "Gradient norm                           :    0.01593\n",
      "Mean episodic reward                    : -609.42800\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -1.26713\n",
      "Policy loss                             :   -2.42429\n",
      "Value function loss                     :  129.74275\n",
      "Mean rewards                            :    0.61660\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   24.99495\n",
      "Mean advantages                         :   -0.96598\n",
      "Mean (norm.) advantages                 :   -0.96598\n",
      "Mean (discounted) returns               :   24.02897\n",
      "Mean normalized returns                 :   24.02897\n",
      "Mean entropy                            :    2.80518\n",
      "Variance explained by the value function:    0.34719\n",
      "Std. of action_0 over agents            :    3.25251\n",
      "Std. of action_0 over envs              :    3.42312\n",
      "Std. of action_0 over time              :    3.43372\n",
      "Std. of action_1 over agents            :    2.22983\n",
      "Std. of action_1 over envs              :    2.37029\n",
      "Std. of action_1 over time              :    2.41865\n",
      "Current timestep                        : 3100000.00000\n",
      "Gradient norm                           :    1.10009\n",
      "Mean episodic reward                    :  626.56000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 310 / 1000   \n",
      "Mean training time per iter (ms)        :     131.64\n",
      "Mean steps per sec (training time)      :   75964.74\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.17846\n",
      "Policy loss                             :    0.01729\n",
      "Value function loss                     :    4.35013\n",
      "Mean rewards                            :   -0.03173\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.32387\n",
      "Mean advantages                         :    0.00398\n",
      "Mean (norm.) advantages                 :    0.00398\n",
      "Mean (discounted) returns               :   -1.31989\n",
      "Mean normalized returns                 :   -1.31989\n",
      "Mean entropy                            :    4.78514\n",
      "Variance explained by the value function:    0.20404\n",
      "Std. of action_0 over agents            :    3.18521\n",
      "Std. of action_0 over envs              :    3.18170\n",
      "Std. of action_0 over time              :    3.18635\n",
      "Std. of action_1 over agents            :    3.16628\n",
      "Std. of action_1 over envs              :    3.16292\n",
      "Std. of action_1 over time              :    3.16696\n",
      "Current timestep                        : 3200000.00000\n",
      "Gradient norm                           :    0.02498\n",
      "Mean episodic reward                    : -610.14200\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.11171\n",
      "Policy loss                             :   -1.31491\n",
      "Value function loss                     :  132.98608\n",
      "Mean rewards                            :    0.64780\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   26.15527\n",
      "Mean advantages                         :   -0.61356\n",
      "Mean (norm.) advantages                 :   -0.61356\n",
      "Mean (discounted) returns               :   25.54171\n",
      "Mean normalized returns                 :   25.54171\n",
      "Mean entropy                            :    2.53319\n",
      "Variance explained by the value function:    0.34439\n",
      "Std. of action_0 over agents            :    1.60258\n",
      "Std. of action_0 over envs              :    1.73586\n",
      "Std. of action_0 over time              :    1.78980\n",
      "Std. of action_1 over agents            :    2.26054\n",
      "Std. of action_1 over envs              :    2.39555\n",
      "Std. of action_1 over time              :    2.46336\n",
      "Current timestep                        : 3200000.00000\n",
      "Gradient norm                           :    1.01753\n",
      "Mean episodic reward                    :  627.02000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 320 / 1000   \n",
      "Mean training time per iter (ms)        :     131.63\n",
      "Mean steps per sec (training time)      :   75969.76\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.46290\n",
      "Policy loss                             :   -0.26678\n",
      "Value function loss                     :    4.30847\n",
      "Mean rewards                            :   -0.03139\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.22942\n",
      "Mean advantages                         :   -0.05558\n",
      "Mean (norm.) advantages                 :   -0.05558\n",
      "Mean (discounted) returns               :   -1.28500\n",
      "Mean normalized returns                 :   -1.28500\n",
      "Mean entropy                            :    4.78396\n",
      "Variance explained by the value function:    0.20248\n",
      "Std. of action_0 over agents            :    3.22506\n",
      "Std. of action_0 over envs              :    3.22058\n",
      "Std. of action_0 over time              :    3.22621\n",
      "Std. of action_1 over agents            :    3.19499\n",
      "Std. of action_1 over envs              :    3.19075\n",
      "Std. of action_1 over time              :    3.19572\n",
      "Current timestep                        : 3300000.00000\n",
      "Gradient norm                           :    0.01744\n",
      "Mean episodic reward                    : -625.75200\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    1.06215\n",
      "Policy loss                             :    0.01200\n",
      "Value function loss                     :  117.38914\n",
      "Mean rewards                            :    0.64280\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   24.93075\n",
      "Mean advantages                         :    0.02592\n",
      "Mean (norm.) advantages                 :    0.02592\n",
      "Mean (discounted) returns               :   24.95667\n",
      "Mean normalized returns                 :   24.95667\n",
      "Mean entropy                            :    2.47481\n",
      "Variance explained by the value function:    0.39445\n",
      "Std. of action_0 over agents            :    1.94186\n",
      "Std. of action_0 over envs              :    2.04290\n",
      "Std. of action_0 over time              :    2.08669\n",
      "Std. of action_1 over agents            :    1.84323\n",
      "Std. of action_1 over envs              :    2.08935\n",
      "Std. of action_1 over time              :    2.22527\n",
      "Current timestep                        : 3300000.00000\n",
      "Gradient norm                           :    1.10526\n",
      "Mean episodic reward                    :  641.36000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 330 / 1000   \n",
      "Mean training time per iter (ms)        :     131.62\n",
      "Mean steps per sec (training time)      :   75973.41\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.16995\n",
      "Policy loss                             :    0.02676\n",
      "Value function loss                     :    4.26990\n",
      "Mean rewards                            :   -0.03074\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.27002\n",
      "Mean advantages                         :    0.00586\n",
      "Mean (norm.) advantages                 :    0.00586\n",
      "Mean (discounted) returns               :   -1.26416\n",
      "Mean normalized returns                 :   -1.26416\n",
      "Mean entropy                            :    4.78805\n",
      "Variance explained by the value function:    0.19528\n",
      "Std. of action_0 over agents            :    3.20214\n",
      "Std. of action_0 over envs              :    3.19784\n",
      "Std. of action_0 over time              :    3.20232\n",
      "Std. of action_1 over agents            :    3.15333\n",
      "Std. of action_1 over envs              :    3.14942\n",
      "Std. of action_1 over time              :    3.15518\n",
      "Current timestep                        : 3400000.00000\n",
      "Gradient norm                           :    0.01288\n",
      "Mean episodic reward                    : -601.46200\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    0.71623\n",
      "Policy loss                             :   -0.37041\n",
      "Value function loss                     :  120.23585\n",
      "Mean rewards                            :    0.62880\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   24.40251\n",
      "Mean advantages                         :   -0.18308\n",
      "Mean (norm.) advantages                 :   -0.18308\n",
      "Mean (discounted) returns               :   24.21943\n",
      "Mean normalized returns                 :   24.21943\n",
      "Mean entropy                            :    2.31428\n",
      "Variance explained by the value function:    0.38382\n",
      "Std. of action_0 over agents            :    1.43672\n",
      "Std. of action_0 over envs              :    1.54338\n",
      "Std. of action_0 over time              :    1.58719\n",
      "Std. of action_1 over agents            :    2.04946\n",
      "Std. of action_1 over envs              :    2.24324\n",
      "Std. of action_1 over time              :    2.34314\n",
      "Current timestep                        : 3400000.00000\n",
      "Gradient norm                           :    1.12347\n",
      "Mean episodic reward                    :  619.60000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 340 / 1000   \n",
      "Mean training time per iter (ms)        :     131.68\n",
      "Mean steps per sec (training time)      :   75942.90\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.13966\n",
      "Policy loss                             :    0.05720\n",
      "Value function loss                     :    4.23574\n",
      "Mean rewards                            :   -0.03018\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.25712\n",
      "Mean advantages                         :    0.01206\n",
      "Mean (norm.) advantages                 :    0.01206\n",
      "Mean (discounted) returns               :   -1.24506\n",
      "Mean normalized returns                 :   -1.24506\n",
      "Mean entropy                            :    4.78433\n",
      "Variance explained by the value function:    0.19050\n",
      "Std. of action_0 over agents            :    3.16613\n",
      "Std. of action_0 over envs              :    3.16289\n",
      "Std. of action_0 over time              :    3.16717\n",
      "Std. of action_1 over agents            :    3.16475\n",
      "Std. of action_1 over envs              :    3.16101\n",
      "Std. of action_1 over time              :    3.16588\n",
      "Current timestep                        : 3500000.00000\n",
      "Gradient norm                           :    0.01915\n",
      "Mean episodic reward                    : -610.17800\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.14786\n",
      "Policy loss                             :   -1.23002\n",
      "Value function loss                     :  121.78609\n",
      "Mean rewards                            :    0.61880\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   23.97610\n",
      "Mean advantages                         :   -0.48854\n",
      "Mean (norm.) advantages                 :   -0.48854\n",
      "Mean (discounted) returns               :   23.48756\n",
      "Mean normalized returns                 :   23.48756\n",
      "Mean entropy                            :    2.71398\n",
      "Variance explained by the value function:    0.40043\n",
      "Std. of action_0 over agents            :    1.67529\n",
      "Std. of action_0 over envs              :    1.79216\n",
      "Std. of action_0 over time              :    1.89752\n",
      "Std. of action_1 over agents            :    2.03197\n",
      "Std. of action_1 over envs              :    2.23206\n",
      "Std. of action_1 over time              :    2.30676\n",
      "Current timestep                        : 3500000.00000\n",
      "Gradient norm                           :    0.90047\n",
      "Mean episodic reward                    :  626.12000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 350 / 1000   \n",
      "Mean training time per iter (ms)        :     131.62\n",
      "Mean steps per sec (training time)      :   75976.63\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.33324\n",
      "Policy loss                             :   -0.13489\n",
      "Value function loss                     :    4.11399\n",
      "Mean rewards                            :   -0.02933\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.16604\n",
      "Mean advantages                         :   -0.02844\n",
      "Mean (norm.) advantages                 :   -0.02844\n",
      "Mean (discounted) returns               :   -1.19448\n",
      "Mean normalized returns                 :   -1.19448\n",
      "Mean entropy                            :    4.78983\n",
      "Variance explained by the value function:    0.19314\n",
      "Std. of action_0 over agents            :    3.16849\n",
      "Std. of action_0 over envs              :    3.16473\n",
      "Std. of action_0 over time              :    3.17010\n",
      "Std. of action_1 over agents            :    3.16927\n",
      "Std. of action_1 over envs              :    3.16522\n",
      "Std. of action_1 over time              :    3.17072\n",
      "Current timestep                        : 3600000.00000\n",
      "Gradient norm                           :    0.01620\n",
      "Mean episodic reward                    : -605.11200\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    5.07056\n",
      "Policy loss                             :    3.97148\n",
      "Value function loss                     :  124.88232\n",
      "Mean rewards                            :    0.60600\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   20.94012\n",
      "Mean advantages                         :    1.41260\n",
      "Mean (norm.) advantages                 :    1.41260\n",
      "Mean (discounted) returns               :   22.35272\n",
      "Mean normalized returns                 :   22.35272\n",
      "Mean entropy                            :    2.99488\n",
      "Variance explained by the value function:    0.38770\n",
      "Std. of action_0 over agents            :    1.48510\n",
      "Std. of action_0 over envs              :    1.61645\n",
      "Std. of action_0 over time              :    1.74834\n",
      "Std. of action_1 over agents            :    1.84253\n",
      "Std. of action_1 over envs              :    2.03710\n",
      "Std. of action_1 over time              :    2.09422\n",
      "Current timestep                        : 3600000.00000\n",
      "Gradient norm                           :    0.83337\n",
      "Mean episodic reward                    :  623.70000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 360 / 1000   \n",
      "Mean training time per iter (ms)        :     131.58\n",
      "Mean steps per sec (training time)      :   76001.51\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.62326\n",
      "Policy loss                             :   -0.42676\n",
      "Value function loss                     :    4.26578\n",
      "Mean rewards                            :   -0.02989\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.13312\n",
      "Mean advantages                         :   -0.08850\n",
      "Mean (norm.) advantages                 :   -0.08850\n",
      "Mean (discounted) returns               :   -1.22162\n",
      "Mean normalized returns                 :   -1.22162\n",
      "Mean entropy                            :    4.78316\n",
      "Variance explained by the value function:    0.18194\n",
      "Std. of action_0 over agents            :    3.19093\n",
      "Std. of action_0 over envs              :    3.18740\n",
      "Std. of action_0 over time              :    3.19163\n",
      "Std. of action_1 over agents            :    3.18128\n",
      "Std. of action_1 over envs              :    3.17857\n",
      "Std. of action_1 over time              :    3.18246\n",
      "Current timestep                        : 3700000.00000\n",
      "Gradient norm                           :    0.02691\n",
      "Mean episodic reward                    : -594.08600\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    4.58368\n",
      "Policy loss                             :    3.41873\n",
      "Value function loss                     :  130.18330\n",
      "Mean rewards                            :    0.62100\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   22.15666\n",
      "Mean advantages                         :    1.35365\n",
      "Mean (norm.) advantages                 :    1.35365\n",
      "Mean (discounted) returns               :   23.51031\n",
      "Mean normalized returns                 :   23.51031\n",
      "Mean entropy                            :    2.73768\n",
      "Variance explained by the value function:    0.36659\n",
      "Std. of action_0 over agents            :    1.42152\n",
      "Std. of action_0 over envs              :    1.55856\n",
      "Std. of action_0 over time              :    1.68198\n",
      "Std. of action_1 over agents            :    1.83403\n",
      "Std. of action_1 over envs              :    2.07478\n",
      "Std. of action_1 over time              :    2.16052\n",
      "Current timestep                        : 3700000.00000\n",
      "Gradient norm                           :    1.06849\n",
      "Mean episodic reward                    :  612.66000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 370 / 1000   \n",
      "Mean training time per iter (ms)        :     131.55\n",
      "Mean steps per sec (training time)      :   76015.16\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    0.04268\n",
      "Policy loss                             :    0.23911\n",
      "Value function loss                     :    4.27183\n",
      "Mean rewards                            :   -0.03005\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.29984\n",
      "Mean advantages                         :    0.05027\n",
      "Mean (norm.) advantages                 :    0.05027\n",
      "Mean (discounted) returns               :   -1.24957\n",
      "Mean normalized returns                 :   -1.24957\n",
      "Mean entropy                            :    4.78297\n",
      "Variance explained by the value function:    0.18250\n",
      "Std. of action_0 over agents            :    3.21299\n",
      "Std. of action_0 over envs              :    3.20958\n",
      "Std. of action_0 over time              :    3.21325\n",
      "Std. of action_1 over agents            :    3.20177\n",
      "Std. of action_1 over envs              :    3.19858\n",
      "Std. of action_1 over time              :    3.20289\n",
      "Current timestep                        : 3800000.00000\n",
      "Gradient norm                           :    0.02021\n",
      "Mean episodic reward                    : -602.57800\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    1.56292\n",
      "Policy loss                             :    0.49697\n",
      "Value function loss                     :  120.66508\n",
      "Mean rewards                            :    0.61880\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   23.59735\n",
      "Mean advantages                         :    0.23922\n",
      "Mean (norm.) advantages                 :    0.23922\n",
      "Mean (discounted) returns               :   23.83657\n",
      "Mean normalized returns                 :   23.83657\n",
      "Mean entropy                            :    2.81394\n",
      "Variance explained by the value function:    0.37444\n",
      "Std. of action_0 over agents            :    1.64359\n",
      "Std. of action_0 over envs              :    1.74559\n",
      "Std. of action_0 over time              :    1.80098\n",
      "Std. of action_1 over agents            :    1.76290\n",
      "Std. of action_1 over envs              :    2.01482\n",
      "Std. of action_1 over time              :    2.09230\n",
      "Current timestep                        : 3800000.00000\n",
      "Gradient norm                           :    0.79187\n",
      "Mean episodic reward                    :  619.90000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 380 / 1000   \n",
      "Mean training time per iter (ms)        :     131.54\n",
      "Mean steps per sec (training time)      :   76021.43\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.17471\n",
      "Policy loss                             :    0.02226\n",
      "Value function loss                     :    4.23532\n",
      "Mean rewards                            :   -0.03032\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.26071\n",
      "Mean advantages                         :    0.00465\n",
      "Mean (norm.) advantages                 :    0.00465\n",
      "Mean (discounted) returns               :   -1.25606\n",
      "Mean normalized returns                 :   -1.25606\n",
      "Mean entropy                            :    4.78651\n",
      "Variance explained by the value function:    0.19371\n",
      "Std. of action_0 over agents            :    3.18377\n",
      "Std. of action_0 over envs              :    3.18081\n",
      "Std. of action_0 over time              :    3.18555\n",
      "Std. of action_1 over agents            :    3.15707\n",
      "Std. of action_1 over envs              :    3.15388\n",
      "Std. of action_1 over time              :    3.15852\n",
      "Current timestep                        : 3900000.00000\n",
      "Gradient norm                           :    0.01720\n",
      "Mean episodic reward                    : -603.35600\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    1.81672\n",
      "Policy loss                             :    0.63851\n",
      "Value function loss                     :  130.94235\n",
      "Mean rewards                            :    0.62260\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   23.75616\n",
      "Mean advantages                         :    0.25667\n",
      "Mean (norm.) advantages                 :    0.25667\n",
      "Mean (discounted) returns               :   24.01283\n",
      "Mean normalized returns                 :   24.01283\n",
      "Mean entropy                            :    2.62433\n",
      "Variance explained by the value function:    0.35954\n",
      "Std. of action_0 over agents            :    2.16298\n",
      "Std. of action_0 over envs              :    2.26112\n",
      "Std. of action_0 over time              :    2.30542\n",
      "Std. of action_1 over agents            :    1.24336\n",
      "Std. of action_1 over envs              :    1.49583\n",
      "Std. of action_1 over time              :    1.63456\n",
      "Current timestep                        : 3900000.00000\n",
      "Gradient norm                           :    0.92593\n",
      "Mean episodic reward                    :  621.42000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 390 / 1000   \n",
      "Mean training time per iter (ms)        :     131.53\n",
      "Mean steps per sec (training time)      :   76030.31\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.48521\n",
      "Policy loss                             :   -0.28761\n",
      "Value function loss                     :    4.12874\n",
      "Mean rewards                            :   -0.02863\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.11529\n",
      "Mean advantages                         :   -0.05942\n",
      "Mean (norm.) advantages                 :   -0.05942\n",
      "Mean (discounted) returns               :   -1.17470\n",
      "Mean normalized returns                 :   -1.17470\n",
      "Mean entropy                            :    4.77769\n",
      "Variance explained by the value function:    0.18224\n",
      "Std. of action_0 over agents            :    3.16015\n",
      "Std. of action_0 over envs              :    3.15622\n",
      "Std. of action_0 over time              :    3.16059\n",
      "Std. of action_1 over agents            :    3.17417\n",
      "Std. of action_1 over envs              :    3.17037\n",
      "Std. of action_1 over time              :    3.17463\n",
      "Current timestep                        : 4000000.00000\n",
      "Gradient norm                           :    0.01883\n",
      "Mean episodic reward                    : -584.42000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    3.04338\n",
      "Policy loss                             :    1.89450\n",
      "Value function loss                     :  129.88705\n",
      "Mean rewards                            :    0.59020\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   21.50470\n",
      "Mean advantages                         :    0.79050\n",
      "Mean (norm.) advantages                 :    0.79050\n",
      "Mean (discounted) returns               :   22.29520\n",
      "Mean normalized returns                 :   22.29520\n",
      "Mean entropy                            :    2.99987\n",
      "Variance explained by the value function:    0.36567\n",
      "Std. of action_0 over agents            :    1.66942\n",
      "Std. of action_0 over envs              :    1.79491\n",
      "Std. of action_0 over time              :    1.82833\n",
      "Std. of action_1 over agents            :    1.43861\n",
      "Std. of action_1 over envs              :    1.64330\n",
      "Std. of action_1 over time              :    1.70277\n",
      "Current timestep                        : 4000000.00000\n",
      "Gradient norm                           :    1.00001\n",
      "Mean episodic reward                    :  603.58000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 400 / 1000   \n",
      "Mean training time per iter (ms)        :     131.55\n",
      "Mean steps per sec (training time)      :   76018.11\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.20110\n",
      "Policy loss                             :   -0.00258\n",
      "Value function loss                     :    4.09311\n",
      "Mean rewards                            :   -0.02699\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.16695\n",
      "Mean advantages                         :   -0.00034\n",
      "Mean (norm.) advantages                 :   -0.00034\n",
      "Mean (discounted) returns               :   -1.16729\n",
      "Mean normalized returns                 :   -1.16729\n",
      "Mean entropy                            :    4.78913\n",
      "Variance explained by the value function:    0.16174\n",
      "Std. of action_0 over agents            :    3.16714\n",
      "Std. of action_0 over envs              :    3.16367\n",
      "Std. of action_0 over time              :    3.16835\n",
      "Std. of action_1 over agents            :    3.14600\n",
      "Std. of action_1 over envs              :    3.14261\n",
      "Std. of action_1 over time              :    3.14728\n",
      "Current timestep                        : 4100000.00000\n",
      "Gradient norm                           :    0.01767\n",
      "Mean episodic reward                    : -573.87600\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.69126\n",
      "Policy loss                             :   -1.79528\n",
      "Value function loss                     :  125.64751\n",
      "Mean rewards                            :    0.56740\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   23.28480\n",
      "Mean advantages                         :   -0.87379\n",
      "Mean (norm.) advantages                 :   -0.87379\n",
      "Mean (discounted) returns               :   22.41101\n",
      "Mean normalized returns                 :   22.41101\n",
      "Mean entropy                            :    3.04909\n",
      "Variance explained by the value function:    0.26955\n",
      "Std. of action_0 over agents            :    1.63592\n",
      "Std. of action_0 over envs              :    1.74235\n",
      "Std. of action_0 over time              :    1.77057\n",
      "Std. of action_1 over agents            :    2.47124\n",
      "Std. of action_1 over envs              :    2.56823\n",
      "Std. of action_1 over time              :    2.58002\n",
      "Current timestep                        : 4100000.00000\n",
      "Gradient norm                           :    1.12786\n",
      "Mean episodic reward                    :  595.22000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 410 / 1000   \n",
      "Mean training time per iter (ms)        :     131.57\n",
      "Mean steps per sec (training time)      :   76004.67\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    0.06058\n",
      "Policy loss                             :    0.25825\n",
      "Value function loss                     :    4.10964\n",
      "Mean rewards                            :   -0.02905\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.27422\n",
      "Mean advantages                         :    0.05514\n",
      "Mean (norm.) advantages                 :    0.05514\n",
      "Mean (discounted) returns               :   -1.21909\n",
      "Mean normalized returns                 :   -1.21909\n",
      "Mean entropy                            :    4.77531\n",
      "Variance explained by the value function:    0.19852\n",
      "Std. of action_0 over agents            :    3.18710\n",
      "Std. of action_0 over envs              :    3.18347\n",
      "Std. of action_0 over time              :    3.18824\n",
      "Std. of action_1 over agents            :    3.21761\n",
      "Std. of action_1 over envs              :    3.21327\n",
      "Std. of action_1 over time              :    3.21761\n",
      "Current timestep                        : 4200000.00000\n",
      "Gradient norm                           :    0.03322\n",
      "Mean episodic reward                    : -553.77200\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -2.21798\n",
      "Policy loss                             :   -3.26730\n",
      "Value function loss                     :  119.04290\n",
      "Mean rewards                            :    0.60380\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   24.84565\n",
      "Mean advantages                         :   -1.31369\n",
      "Mean (norm.) advantages                 :   -1.31369\n",
      "Mean (discounted) returns               :   23.53195\n",
      "Mean normalized returns                 :   23.53195\n",
      "Mean entropy                            :    2.82212\n",
      "Variance explained by the value function:    0.36619\n",
      "Std. of action_0 over agents            :    1.38333\n",
      "Std. of action_0 over envs              :    1.53752\n",
      "Std. of action_0 over time              :    1.59793\n",
      "Std. of action_1 over agents            :    1.75281\n",
      "Std. of action_1 over envs              :    1.93683\n",
      "Std. of action_1 over time              :    1.97255\n",
      "Current timestep                        : 4200000.00000\n",
      "Gradient norm                           :    0.90726\n",
      "Mean episodic reward                    :  577.66000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 420 / 1000   \n",
      "Mean training time per iter (ms)        :     131.56\n",
      "Mean steps per sec (training time)      :   76009.21\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.18474\n",
      "Policy loss                             :    0.01189\n",
      "Value function loss                     :    4.26499\n",
      "Mean rewards                            :   -0.02940\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.24892\n",
      "Mean advantages                         :    0.00309\n",
      "Mean (norm.) advantages                 :    0.00309\n",
      "Mean (discounted) returns               :   -1.24584\n",
      "Mean normalized returns                 :   -1.24584\n",
      "Mean entropy                            :    4.78547\n",
      "Variance explained by the value function:    0.17893\n",
      "Std. of action_0 over agents            :    3.18786\n",
      "Std. of action_0 over envs              :    3.18400\n",
      "Std. of action_0 over time              :    3.18841\n",
      "Std. of action_1 over agents            :    3.16584\n",
      "Std. of action_1 over envs              :    3.16185\n",
      "Std. of action_1 over time              :    3.16704\n",
      "Current timestep                        : 4300000.00000\n",
      "Gradient norm                           :    0.02370\n",
      "Mean episodic reward                    : -574.09200\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    2.79387\n",
      "Policy loss                             :    1.66883\n",
      "Value function loss                     :  129.00156\n",
      "Mean rewards                            :    0.60700\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   23.32399\n",
      "Mean advantages                         :    0.48242\n",
      "Mean (norm.) advantages                 :    0.48242\n",
      "Mean (discounted) returns               :   23.80641\n",
      "Mean normalized returns                 :   23.80641\n",
      "Mean entropy                            :    3.29962\n",
      "Variance explained by the value function:    0.33807\n",
      "Std. of action_0 over agents            :    1.97080\n",
      "Std. of action_0 over envs              :    2.13823\n",
      "Std. of action_0 over time              :    2.19951\n",
      "Std. of action_1 over agents            :    2.51205\n",
      "Std. of action_1 over envs              :    2.61524\n",
      "Std. of action_1 over time              :    2.63861\n",
      "Current timestep                        : 4300000.00000\n",
      "Gradient norm                           :    1.07733\n",
      "Mean episodic reward                    :  595.68000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 430 / 1000   \n",
      "Mean training time per iter (ms)        :     131.55\n",
      "Mean steps per sec (training time)      :   76015.14\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.35016\n",
      "Policy loss                             :   -0.15345\n",
      "Value function loss                     :    4.22506\n",
      "Mean rewards                            :   -0.03015\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.20514\n",
      "Mean advantages                         :   -0.03215\n",
      "Mean (norm.) advantages                 :   -0.03215\n",
      "Mean (discounted) returns               :   -1.23729\n",
      "Mean normalized returns                 :   -1.23729\n",
      "Mean entropy                            :    4.77925\n",
      "Variance explained by the value function:    0.19107\n",
      "Std. of action_0 over agents            :    3.15297\n",
      "Std. of action_0 over envs              :    3.14875\n",
      "Std. of action_0 over time              :    3.15251\n",
      "Std. of action_1 over agents            :    3.16852\n",
      "Std. of action_1 over envs              :    3.16440\n",
      "Std. of action_1 over time              :    3.16943\n",
      "Current timestep                        : 4400000.00000\n",
      "Gradient norm                           :    0.01779\n",
      "Mean episodic reward                    : -593.65600\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    4.81203\n",
      "Policy loss                             :    3.68636\n",
      "Value function loss                     :  127.10030\n",
      "Mean rewards                            :    0.61940\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   22.12769\n",
      "Mean advantages                         :    1.42967\n",
      "Mean (norm.) advantages                 :    1.42967\n",
      "Mean (discounted) returns               :   23.55736\n",
      "Mean normalized returns                 :   23.55736\n",
      "Mean entropy                            :    2.90670\n",
      "Variance explained by the value function:    0.36861\n",
      "Std. of action_0 over agents            :    2.65032\n",
      "Std. of action_0 over envs              :    2.97717\n",
      "Std. of action_0 over time              :    3.14853\n",
      "Std. of action_1 over agents            :    1.35858\n",
      "Std. of action_1 over envs              :    1.61464\n",
      "Std. of action_1 over time              :    1.71680\n",
      "Current timestep                        : 4400000.00000\n",
      "Gradient norm                           :    1.16344\n",
      "Mean episodic reward                    :  612.14000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 440 / 1000   \n",
      "Mean training time per iter (ms)        :     131.52\n",
      "Mean steps per sec (training time)      :   76035.64\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.19392\n",
      "Policy loss                             :    0.00368\n",
      "Value function loss                     :    4.15307\n",
      "Mean rewards                            :   -0.02844\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.18998\n",
      "Mean advantages                         :    0.00096\n",
      "Mean (norm.) advantages                 :    0.00096\n",
      "Mean (discounted) returns               :   -1.18902\n",
      "Mean normalized returns                 :   -1.18902\n",
      "Mean entropy                            :    4.78275\n",
      "Variance explained by the value function:    0.17421\n",
      "Std. of action_0 over agents            :    3.14666\n",
      "Std. of action_0 over envs              :    3.14257\n",
      "Std. of action_0 over time              :    3.14725\n",
      "Std. of action_1 over agents            :    3.13846\n",
      "Std. of action_1 over envs              :    3.13396\n",
      "Std. of action_1 over time              :    3.13949\n",
      "Current timestep                        : 4500000.00000\n",
      "Gradient norm                           :    0.02253\n",
      "Mean episodic reward                    : -590.33800\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    2.83398\n",
      "Policy loss                             :    1.78551\n",
      "Value function loss                     :  118.63917\n",
      "Mean rewards                            :    0.58880\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   21.62008\n",
      "Mean advantages                         :    0.78197\n",
      "Mean (norm.) advantages                 :    0.78197\n",
      "Mean (discounted) returns               :   22.40204\n",
      "Mean normalized returns                 :   22.40204\n",
      "Mean entropy                            :    2.75842\n",
      "Variance explained by the value function:    0.38188\n",
      "Std. of action_0 over agents            :    1.63994\n",
      "Std. of action_0 over envs              :    1.79117\n",
      "Std. of action_0 over time              :    1.84512\n",
      "Std. of action_1 over agents            :    1.06583\n",
      "Std. of action_1 over envs              :    1.25039\n",
      "Std. of action_1 over time              :    1.37107\n",
      "Current timestep                        : 4500000.00000\n",
      "Gradient norm                           :    1.16807\n",
      "Mean episodic reward                    :  609.46000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 450 / 1000   \n",
      "Mean training time per iter (ms)        :     131.51\n",
      "Mean steps per sec (training time)      :   76041.84\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.28916\n",
      "Policy loss                             :   -0.09137\n",
      "Value function loss                     :    4.15352\n",
      "Mean rewards                            :   -0.02930\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.19019\n",
      "Mean advantages                         :   -0.01906\n",
      "Mean (norm.) advantages                 :   -0.01906\n",
      "Mean (discounted) returns               :   -1.20925\n",
      "Mean normalized returns                 :   -1.20925\n",
      "Mean entropy                            :    4.78651\n",
      "Variance explained by the value function:    0.18866\n",
      "Std. of action_0 over agents            :    3.17974\n",
      "Std. of action_0 over envs              :    3.17585\n",
      "Std. of action_0 over time              :    3.18113\n",
      "Std. of action_1 over agents            :    3.15586\n",
      "Std. of action_1 over envs              :    3.15264\n",
      "Std. of action_1 over time              :    3.15776\n",
      "Current timestep                        : 4600000.00000\n",
      "Gradient norm                           :    0.01948\n",
      "Mean episodic reward                    : -586.58000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    4.54603\n",
      "Policy loss                             :    3.42859\n",
      "Value function loss                     :  127.13714\n",
      "Mean rewards                            :    0.60380\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   21.68981\n",
      "Mean advantages                         :    1.11002\n",
      "Mean (norm.) advantages                 :    1.11002\n",
      "Mean (discounted) returns               :   22.79983\n",
      "Mean normalized returns                 :   22.79983\n",
      "Mean entropy                            :    3.07853\n",
      "Variance explained by the value function:    0.36333\n",
      "Std. of action_0 over agents            :    1.48807\n",
      "Std. of action_0 over envs              :    1.63037\n",
      "Std. of action_0 over time              :    1.73430\n",
      "Std. of action_1 over agents            :    1.26435\n",
      "Std. of action_1 over envs              :    1.41115\n",
      "Std. of action_1 over time              :    1.50269\n",
      "Current timestep                        : 4600000.00000\n",
      "Gradient norm                           :    0.83134\n",
      "Mean episodic reward                    :  606.76000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 460 / 1000   \n",
      "Mean training time per iter (ms)        :     131.46\n",
      "Mean steps per sec (training time)      :   76066.93\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.65366\n",
      "Policy loss                             :   -0.45661\n",
      "Value function loss                     :    4.18933\n",
      "Mean rewards                            :   -0.02874\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.10567\n",
      "Mean advantages                         :   -0.09550\n",
      "Mean (norm.) advantages                 :   -0.09550\n",
      "Mean (discounted) returns               :   -1.20117\n",
      "Mean normalized returns                 :   -1.20117\n",
      "Mean entropy                            :    4.77879\n",
      "Variance explained by the value function:    0.18222\n",
      "Std. of action_0 over agents            :    3.18494\n",
      "Std. of action_0 over envs              :    3.18091\n",
      "Std. of action_0 over time              :    3.18582\n",
      "Std. of action_1 over agents            :    3.17401\n",
      "Std. of action_1 over envs              :    3.16946\n",
      "Std. of action_1 over time              :    3.17463\n",
      "Current timestep                        : 4700000.00000\n",
      "Gradient norm                           :    0.02612\n",
      "Mean episodic reward                    : -575.90400\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    7.23932\n",
      "Policy loss                             :    6.03731\n",
      "Value function loss                     :  135.60860\n",
      "Mean rewards                            :    0.59320\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   20.80790\n",
      "Mean advantages                         :    2.10014\n",
      "Mean (norm.) advantages                 :    2.10014\n",
      "Mean (discounted) returns               :   22.90804\n",
      "Mean normalized returns                 :   22.90804\n",
      "Mean entropy                            :    3.08149\n",
      "Variance explained by the value function:    0.34040\n",
      "Std. of action_0 over agents            :    1.32211\n",
      "Std. of action_0 over envs              :    1.51414\n",
      "Std. of action_0 over time              :    1.61080\n",
      "Std. of action_1 over agents            :    1.69204\n",
      "Std. of action_1 over envs              :    1.85639\n",
      "Std. of action_1 over time              :    1.89515\n",
      "Current timestep                        : 4700000.00000\n",
      "Gradient norm                           :    1.17552\n",
      "Mean episodic reward                    :  596.60000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 470 / 1000   \n",
      "Mean training time per iter (ms)        :     131.44\n",
      "Mean steps per sec (training time)      :   76082.82\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.46113\n",
      "Policy loss                             :   -0.26462\n",
      "Value function loss                     :    4.28217\n",
      "Mean rewards                            :   -0.03009\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.20165\n",
      "Mean advantages                         :   -0.05508\n",
      "Mean (norm.) advantages                 :   -0.05508\n",
      "Mean (discounted) returns               :   -1.25673\n",
      "Mean normalized returns                 :   -1.25673\n",
      "Mean entropy                            :    4.78655\n",
      "Variance explained by the value function:    0.18939\n",
      "Std. of action_0 over agents            :    3.17332\n",
      "Std. of action_0 over envs              :    3.16987\n",
      "Std. of action_0 over time              :    3.17413\n",
      "Std. of action_1 over agents            :    3.14360\n",
      "Std. of action_1 over envs              :    3.13937\n",
      "Std. of action_1 over time              :    3.14530\n",
      "Current timestep                        : 4800000.00000\n",
      "Gradient norm                           :    0.01813\n",
      "Mean episodic reward                    : -587.58400\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    2.19960\n",
      "Policy loss                             :    1.14623\n",
      "Value function loss                     :  120.46364\n",
      "Mean rewards                            :    0.62260\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   24.07528\n",
      "Mean advantages                         :    0.49286\n",
      "Mean (norm.) advantages                 :    0.49286\n",
      "Mean (discounted) returns               :   24.56815\n",
      "Mean normalized returns                 :   24.56815\n",
      "Mean entropy                            :    3.02521\n",
      "Variance explained by the value function:    0.35859\n",
      "Std. of action_0 over agents            :    1.62654\n",
      "Std. of action_0 over envs              :    1.77381\n",
      "Std. of action_0 over time              :    1.81114\n",
      "Std. of action_1 over agents            :    2.07103\n",
      "Std. of action_1 over envs              :    2.23856\n",
      "Std. of action_1 over time              :    2.26556\n",
      "Current timestep                        : 4800000.00000\n",
      "Gradient norm                           :    1.04710\n",
      "Mean episodic reward                    :  607.02000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 480 / 1000   \n",
      "Mean training time per iter (ms)        :     131.48\n",
      "Mean steps per sec (training time)      :   76059.64\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.15208\n",
      "Policy loss                             :    0.04440\n",
      "Value function loss                     :    4.23780\n",
      "Mean rewards                            :   -0.02952\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.25399\n",
      "Mean advantages                         :    0.00979\n",
      "Mean (norm.) advantages                 :    0.00979\n",
      "Mean (discounted) returns               :   -1.24420\n",
      "Mean normalized returns                 :   -1.24420\n",
      "Mean entropy                            :    4.77725\n",
      "Variance explained by the value function:    0.18121\n",
      "Std. of action_0 over agents            :    3.14573\n",
      "Std. of action_0 over envs              :    3.14131\n",
      "Std. of action_0 over time              :    3.14561\n",
      "Std. of action_1 over agents            :    3.16076\n",
      "Std. of action_1 over envs              :    3.15667\n",
      "Std. of action_1 over time              :    3.16215\n",
      "Current timestep                        : 4900000.00000\n",
      "Gradient norm                           :    0.01975\n",
      "Mean episodic reward                    : -586.44200\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    1.42122\n",
      "Policy loss                             :    0.38212\n",
      "Value function loss                     :  118.11294\n",
      "Mean rewards                            :    0.61200\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   23.95471\n",
      "Mean advantages                         :    0.04253\n",
      "Mean (norm.) advantages                 :    0.04253\n",
      "Mean (discounted) returns               :   23.99724\n",
      "Mean normalized returns                 :   23.99724\n",
      "Mean entropy                            :    2.84069\n",
      "Variance explained by the value function:    0.32629\n",
      "Std. of action_0 over agents            :    1.95773\n",
      "Std. of action_0 over envs              :    2.19158\n",
      "Std. of action_0 over time              :    2.35496\n",
      "Std. of action_1 over agents            :    2.15653\n",
      "Std. of action_1 over envs              :    2.30828\n",
      "Std. of action_1 over time              :    2.34185\n",
      "Current timestep                        : 4900000.00000\n",
      "Gradient norm                           :    0.99588\n",
      "Mean episodic reward                    :  605.08000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 490 / 1000   \n",
      "Mean training time per iter (ms)        :     131.47\n",
      "Mean steps per sec (training time)      :   76061.29\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.17971\n",
      "Policy loss                             :    0.01790\n",
      "Value function loss                     :    4.12042\n",
      "Mean rewards                            :   -0.02852\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.20077\n",
      "Mean advantages                         :    0.00373\n",
      "Mean (norm.) advantages                 :    0.00373\n",
      "Mean (discounted) returns               :   -1.19703\n",
      "Mean normalized returns                 :   -1.19703\n",
      "Mean entropy                            :    4.77635\n",
      "Variance explained by the value function:    0.18098\n",
      "Std. of action_0 over agents            :    3.17309\n",
      "Std. of action_0 over envs              :    3.16883\n",
      "Std. of action_0 over time              :    3.17305\n",
      "Std. of action_1 over agents            :    3.18067\n",
      "Std. of action_1 over envs              :    3.17674\n",
      "Std. of action_1 over time              :    3.18183\n",
      "Current timestep                        : 5000000.00000\n",
      "Gradient norm                           :    0.02287\n",
      "Mean episodic reward                    : -588.65000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    0.95276\n",
      "Policy loss                             :   -0.21302\n",
      "Value function loss                     :  131.84268\n",
      "Mean rewards                            :    0.59280\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   23.20027\n",
      "Mean advantages                         :   -0.19128\n",
      "Mean (norm.) advantages                 :   -0.19128\n",
      "Mean (discounted) returns               :   23.00900\n",
      "Mean normalized returns                 :   23.00900\n",
      "Mean entropy                            :    3.05294\n",
      "Variance explained by the value function:    0.30400\n",
      "Std. of action_0 over agents            :    3.36361\n",
      "Std. of action_0 over envs              :    3.57013\n",
      "Std. of action_0 over time              :    3.65972\n",
      "Std. of action_1 over agents            :    2.04779\n",
      "Std. of action_1 over envs              :    2.18085\n",
      "Std. of action_1 over time              :    2.21191\n",
      "Current timestep                        : 5000000.00000\n",
      "Gradient norm                           :    0.94232\n",
      "Mean episodic reward                    :  608.54000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 500 / 1000   \n",
      "Mean training time per iter (ms)        :     131.45\n",
      "Mean steps per sec (training time)      :   76073.00\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.49737\n",
      "Policy loss                             :   -0.30150\n",
      "Value function loss                     :    4.28888\n",
      "Mean rewards                            :   -0.02987\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.17837\n",
      "Mean advantages                         :   -0.06192\n",
      "Mean (norm.) advantages                 :   -0.06192\n",
      "Mean (discounted) returns               :   -1.24030\n",
      "Mean normalized returns                 :   -1.24030\n",
      "Mean entropy                            :    4.77517\n",
      "Variance explained by the value function:    0.17681\n",
      "Std. of action_0 over agents            :    3.18182\n",
      "Std. of action_0 over envs              :    3.17764\n",
      "Std. of action_0 over time              :    3.18207\n",
      "Std. of action_1 over agents            :    3.18461\n",
      "Std. of action_1 over envs              :    3.18019\n",
      "Std. of action_1 over time              :    3.18581\n",
      "Current timestep                        : 5100000.00000\n",
      "Gradient norm                           :    0.02515\n",
      "Mean episodic reward                    : -577.27000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    0.43010\n",
      "Policy loss                             :   -0.74591\n",
      "Value function loss                     :  130.29968\n",
      "Mean rewards                            :    0.61540\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   24.74081\n",
      "Mean advantages                         :   -0.46125\n",
      "Mean (norm.) advantages                 :   -0.46125\n",
      "Mean (discounted) returns               :   24.27957\n",
      "Mean normalized returns                 :   24.27957\n",
      "Mean entropy                            :    2.53975\n",
      "Variance explained by the value function:    0.28724\n",
      "Std. of action_0 over agents            :    3.75676\n",
      "Std. of action_0 over envs              :    3.96237\n",
      "Std. of action_0 over time              :    4.03317\n",
      "Std. of action_1 over agents            :    2.25058\n",
      "Std. of action_1 over envs              :    2.37366\n",
      "Std. of action_1 over time              :    2.40704\n",
      "Current timestep                        : 5100000.00000\n",
      "Gradient norm                           :    0.93709\n",
      "Mean episodic reward                    :  598.44000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 510 / 1000   \n",
      "Mean training time per iter (ms)        :     131.46\n",
      "Mean steps per sec (training time)      :   76065.93\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.20546\n",
      "Policy loss                             :   -0.00907\n",
      "Value function loss                     :    4.22638\n",
      "Mean rewards                            :   -0.02972\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.24474\n",
      "Mean advantages                         :   -0.00145\n",
      "Mean (norm.) advantages                 :   -0.00145\n",
      "Mean (discounted) returns               :   -1.24619\n",
      "Mean normalized returns                 :   -1.24619\n",
      "Mean entropy                            :    4.77288\n",
      "Variance explained by the value function:    0.18467\n",
      "Std. of action_0 over agents            :    3.17872\n",
      "Std. of action_0 over envs              :    3.17436\n",
      "Std. of action_0 over time              :    3.17863\n",
      "Std. of action_1 over agents            :    3.14629\n",
      "Std. of action_1 over envs              :    3.14274\n",
      "Std. of action_1 over time              :    3.14751\n",
      "Current timestep                        : 5200000.00000\n",
      "Gradient norm                           :    0.01992\n",
      "Mean episodic reward                    : -578.99600\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    2.52694\n",
      "Policy loss                             :    1.41140\n",
      "Value function loss                     :  125.54841\n",
      "Mean rewards                            :    0.61180\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   23.70210\n",
      "Mean advantages                         :    0.27958\n",
      "Mean (norm.) advantages                 :    0.27958\n",
      "Mean (discounted) returns               :   23.98168\n",
      "Mean normalized returns                 :   23.98168\n",
      "Mean entropy                            :    2.79895\n",
      "Variance explained by the value function:    0.31139\n",
      "Std. of action_0 over agents            :    3.32953\n",
      "Std. of action_0 over envs              :    3.55246\n",
      "Std. of action_0 over time              :    3.64770\n",
      "Std. of action_1 over agents            :    1.49516\n",
      "Std. of action_1 over envs              :    1.73328\n",
      "Std. of action_1 over time              :    1.81478\n",
      "Current timestep                        : 5200000.00000\n",
      "Gradient norm                           :    1.01528\n",
      "Mean episodic reward                    :  600.02000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 520 / 1000   \n",
      "Mean training time per iter (ms)        :     131.45\n",
      "Mean steps per sec (training time)      :   76077.36\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.23724\n",
      "Policy loss                             :   -0.03543\n",
      "Value function loss                     :    3.73188\n",
      "Mean rewards                            :   -0.02359\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.01206\n",
      "Mean advantages                         :   -0.00738\n",
      "Mean (norm.) advantages                 :   -0.00738\n",
      "Mean (discounted) returns               :   -1.01945\n",
      "Mean normalized returns                 :   -1.01945\n",
      "Mean entropy                            :    4.78256\n",
      "Variance explained by the value function:    0.14703\n",
      "Std. of action_0 over agents            :    3.16255\n",
      "Std. of action_0 over envs              :    3.15823\n",
      "Std. of action_0 over time              :    3.16335\n",
      "Std. of action_1 over agents            :    3.15058\n",
      "Std. of action_1 over envs              :    3.14680\n",
      "Std. of action_1 over time              :    3.15191\n",
      "Current timestep                        : 5300000.00000\n",
      "Gradient norm                           :    0.02115\n",
      "Mean episodic reward                    : -529.06200\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    7.90190\n",
      "Policy loss                             :    6.72238\n",
      "Value function loss                     :  136.52116\n",
      "Mean rewards                            :    0.50360\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   16.90433\n",
      "Mean advantages                         :    1.87322\n",
      "Mean (norm.) advantages                 :    1.87322\n",
      "Mean (discounted) returns               :   18.77756\n",
      "Mean normalized returns                 :   18.77756\n",
      "Mean entropy                            :    3.71373\n",
      "Variance explained by the value function:    0.28331\n",
      "Std. of action_0 over agents            :    2.32177\n",
      "Std. of action_0 over envs              :    2.52142\n",
      "Std. of action_0 over time              :    2.56877\n",
      "Std. of action_1 over agents            :    1.98013\n",
      "Std. of action_1 over envs              :    2.10089\n",
      "Std. of action_1 over time              :    2.12323\n",
      "Current timestep                        : 5300000.00000\n",
      "Gradient norm                           :    1.12699\n",
      "Mean episodic reward                    :  555.30000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 530 / 1000   \n",
      "Mean training time per iter (ms)        :     131.43\n",
      "Mean steps per sec (training time)      :   76085.21\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.17573\n",
      "Policy loss                             :    0.02127\n",
      "Value function loss                     :    4.15137\n",
      "Mean rewards                            :   -0.02704\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.21360\n",
      "Mean advantages                         :    0.00617\n",
      "Mean (norm.) advantages                 :    0.00617\n",
      "Mean (discounted) returns               :   -1.20743\n",
      "Mean normalized returns                 :   -1.20743\n",
      "Mean entropy                            :    4.77022\n",
      "Variance explained by the value function:    0.15318\n",
      "Std. of action_0 over agents            :    3.17482\n",
      "Std. of action_0 over envs              :    3.17058\n",
      "Std. of action_0 over time              :    3.17549\n",
      "Std. of action_1 over agents            :    3.20741\n",
      "Std. of action_1 over envs              :    3.20365\n",
      "Std. of action_1 over time              :    3.20852\n",
      "Current timestep                        : 5400000.00000\n",
      "Gradient norm                           :    0.03017\n",
      "Mean episodic reward                    : -532.63800\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -3.30575\n",
      "Policy loss                             :   -4.49441\n",
      "Value function loss                     :  136.40163\n",
      "Mean rewards                            :    0.56540\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   25.17593\n",
      "Mean advantages                         :   -1.35405\n",
      "Mean (norm.) advantages                 :   -1.35405\n",
      "Mean (discounted) returns               :   23.82188\n",
      "Mean normalized returns                 :   23.82188\n",
      "Mean entropy                            :    3.50715\n",
      "Variance explained by the value function:    0.15117\n",
      "Std. of action_0 over agents            :    2.40030\n",
      "Std. of action_0 over envs              :    2.63203\n",
      "Std. of action_0 over time              :    2.67567\n",
      "Std. of action_1 over agents            :    1.73678\n",
      "Std. of action_1 over envs              :    1.86570\n",
      "Std. of action_1 over time              :    1.89477\n",
      "Current timestep                        : 5400000.00000\n",
      "Gradient norm                           :    1.09371\n",
      "Mean episodic reward                    :  557.24000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 540 / 1000   \n",
      "Mean training time per iter (ms)        :     131.45\n",
      "Mean steps per sec (training time)      :   76073.65\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.08114\n",
      "Policy loss                             :    0.11695\n",
      "Value function loss                     :    4.07153\n",
      "Mean rewards                            :   -0.02625\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.20011\n",
      "Mean advantages                         :    0.02562\n",
      "Mean (norm.) advantages                 :    0.02562\n",
      "Mean (discounted) returns               :   -1.17449\n",
      "Mean normalized returns                 :   -1.17449\n",
      "Mean entropy                            :    4.77622\n",
      "Variance explained by the value function:    0.15503\n",
      "Std. of action_0 over agents            :    3.15382\n",
      "Std. of action_0 over envs              :    3.15006\n",
      "Std. of action_0 over time              :    3.15490\n",
      "Std. of action_1 over agents            :    3.13384\n",
      "Std. of action_1 over envs              :    3.13046\n",
      "Std. of action_1 over time              :    3.13516\n",
      "Current timestep                        : 5500000.00000\n",
      "Gradient norm                           :    0.03553\n",
      "Mean episodic reward                    : -513.48600\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    3.18594\n",
      "Policy loss                             :    1.75481\n",
      "Value function loss                     :  161.62814\n",
      "Mean rewards                            :    0.55020\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   22.06698\n",
      "Mean advantages                         :    0.48769\n",
      "Mean (norm.) advantages                 :    0.48769\n",
      "Mean (discounted) returns               :   22.55467\n",
      "Mean normalized returns                 :   22.55467\n",
      "Mean entropy                            :    3.70296\n",
      "Variance explained by the value function:    0.20894\n",
      "Std. of action_0 over agents            :    2.45892\n",
      "Std. of action_0 over envs              :    2.68662\n",
      "Std. of action_0 over time              :    2.72122\n",
      "Std. of action_1 over agents            :    1.90494\n",
      "Std. of action_1 over envs              :    2.00838\n",
      "Std. of action_1 over time              :    2.01924\n",
      "Current timestep                        : 5500000.00000\n",
      "Gradient norm                           :    0.91830\n",
      "Mean episodic reward                    :  540.78000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 550 / 1000   \n",
      "Mean training time per iter (ms)        :     131.43\n",
      "Mean steps per sec (training time)      :   76086.58\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.19464\n",
      "Policy loss                             :    0.00367\n",
      "Value function loss                     :    4.04854\n",
      "Mean rewards                            :   -0.02771\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.19101\n",
      "Mean advantages                         :    0.00057\n",
      "Mean (norm.) advantages                 :    0.00057\n",
      "Mean (discounted) returns               :   -1.19045\n",
      "Mean normalized returns                 :   -1.19045\n",
      "Mean entropy                            :    4.77594\n",
      "Variance explained by the value function:    0.18235\n",
      "Std. of action_0 over agents            :    3.18406\n",
      "Std. of action_0 over envs              :    3.17920\n",
      "Std. of action_0 over time              :    3.18416\n",
      "Std. of action_1 over agents            :    3.16415\n",
      "Std. of action_1 over envs              :    3.16005\n",
      "Std. of action_1 over time              :    3.16502\n",
      "Current timestep                        : 5600000.00000\n",
      "Gradient norm                           :    0.02373\n",
      "Mean episodic reward                    : -551.89200\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    4.25344\n",
      "Policy loss                             :    3.14105\n",
      "Value function loss                     :  127.98193\n",
      "Mean rewards                            :    0.57700\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   21.67739\n",
      "Mean advantages                         :    1.05651\n",
      "Mean (norm.) advantages                 :    1.05651\n",
      "Mean (discounted) returns               :   22.73390\n",
      "Mean normalized returns                 :   22.73390\n",
      "Mean entropy                            :    3.34863\n",
      "Variance explained by the value function:    0.29933\n",
      "Std. of action_0 over agents            :    2.32948\n",
      "Std. of action_0 over envs              :    2.51772\n",
      "Std. of action_0 over time              :    2.56449\n",
      "Std. of action_1 over agents            :    1.50135\n",
      "Std. of action_1 over envs              :    1.70499\n",
      "Std. of action_1 over time              :    1.74539\n",
      "Current timestep                        : 5600000.00000\n",
      "Gradient norm                           :    0.91653\n",
      "Mean episodic reward                    :  575.04000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 560 / 1000   \n",
      "Mean training time per iter (ms)        :     131.41\n",
      "Mean steps per sec (training time)      :   76096.32\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.46283\n",
      "Policy loss                             :   -0.26692\n",
      "Value function loss                     :    4.21191\n",
      "Mean rewards                            :   -0.02879\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.15552\n",
      "Mean advantages                         :   -0.05532\n",
      "Mean (norm.) advantages                 :   -0.05532\n",
      "Mean (discounted) returns               :   -1.21085\n",
      "Mean normalized returns                 :   -1.21085\n",
      "Mean entropy                            :    4.76057\n",
      "Variance explained by the value function:    0.16959\n",
      "Std. of action_0 over agents            :    3.20372\n",
      "Std. of action_0 over envs              :    3.19806\n",
      "Std. of action_0 over time              :    3.20266\n",
      "Std. of action_1 over agents            :    3.14751\n",
      "Std. of action_1 over envs              :    3.14285\n",
      "Std. of action_1 over time              :    3.14869\n",
      "Current timestep                        : 5700000.00000\n",
      "Gradient norm                           :    0.02271\n",
      "Mean episodic reward                    : -570.32200\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    1.98510\n",
      "Policy loss                             :    0.91608\n",
      "Value function loss                     :  121.61179\n",
      "Mean rewards                            :    0.60160\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   23.22964\n",
      "Mean advantages                         :    0.32673\n",
      "Mean (norm.) advantages                 :    0.32673\n",
      "Mean (discounted) returns               :   23.55637\n",
      "Mean normalized returns                 :   23.55637\n",
      "Mean entropy                            :    2.94206\n",
      "Variance explained by the value function:    0.32381\n",
      "Std. of action_0 over agents            :    2.20196\n",
      "Std. of action_0 over envs              :    2.38378\n",
      "Std. of action_0 over time              :    2.46216\n",
      "Std. of action_1 over agents            :    1.78699\n",
      "Std. of action_1 over envs              :    2.02661\n",
      "Std. of action_1 over time              :    2.11268\n",
      "Current timestep                        : 5700000.00000\n",
      "Gradient norm                           :    0.96420\n",
      "Mean episodic reward                    :  593.58000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 570 / 1000   \n",
      "Mean training time per iter (ms)        :     131.40\n",
      "Mean steps per sec (training time)      :   76104.39\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.33392\n",
      "Policy loss                             :   -0.13712\n",
      "Value function loss                     :    4.18797\n",
      "Mean rewards                            :   -0.02823\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.17718\n",
      "Mean advantages                         :   -0.02808\n",
      "Mean (norm.) advantages                 :   -0.02808\n",
      "Mean (discounted) returns               :   -1.20526\n",
      "Mean normalized returns                 :   -1.20526\n",
      "Mean entropy                            :    4.77350\n",
      "Variance explained by the value function:    0.16641\n",
      "Std. of action_0 over agents            :    3.19262\n",
      "Std. of action_0 over envs              :    3.18818\n",
      "Std. of action_0 over time              :    3.19217\n",
      "Std. of action_1 over agents            :    3.18164\n",
      "Std. of action_1 over envs              :    3.17761\n",
      "Std. of action_1 over time              :    3.18330\n",
      "Current timestep                        : 5800000.00000\n",
      "Gradient norm                           :    0.01604\n",
      "Mean episodic reward                    : -568.16600\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    2.92832\n",
      "Policy loss                             :    1.86666\n",
      "Value function loss                     :  122.10934\n",
      "Mean rewards                            :    0.58640\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   22.52959\n",
      "Mean advantages                         :    0.61113\n",
      "Mean (norm.) advantages                 :    0.61113\n",
      "Mean (discounted) returns               :   23.14072\n",
      "Mean normalized returns                 :   23.14072\n",
      "Mean entropy                            :    3.18853\n",
      "Variance explained by the value function:    0.30759\n",
      "Std. of action_0 over agents            :    2.44358\n",
      "Std. of action_0 over envs              :    2.66187\n",
      "Std. of action_0 over time              :    2.77153\n",
      "Std. of action_1 over agents            :    2.05370\n",
      "Std. of action_1 over envs              :    2.21585\n",
      "Std. of action_1 over time              :    2.26112\n",
      "Current timestep                        : 5800000.00000\n",
      "Gradient norm                           :    0.94365\n",
      "Mean episodic reward                    :  590.36000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 580 / 1000   \n",
      "Mean training time per iter (ms)        :     131.40\n",
      "Mean steps per sec (training time)      :   76101.31\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.15272\n",
      "Policy loss                             :    0.04362\n",
      "Value function loss                     :    4.20945\n",
      "Mean rewards                            :   -0.02781\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.22614\n",
      "Mean advantages                         :    0.00973\n",
      "Mean (norm.) advantages                 :    0.00973\n",
      "Mean (discounted) returns               :   -1.21641\n",
      "Mean normalized returns                 :   -1.21641\n",
      "Mean entropy                            :    4.76871\n",
      "Variance explained by the value function:    0.15643\n",
      "Std. of action_0 over agents            :    3.17519\n",
      "Std. of action_0 over envs              :    3.17035\n",
      "Std. of action_0 over time              :    3.17581\n",
      "Std. of action_1 over agents            :    3.14275\n",
      "Std. of action_1 over envs              :    3.13791\n",
      "Std. of action_1 over time              :    3.14412\n",
      "Current timestep                        : 5900000.00000\n",
      "Gradient norm                           :    0.01892\n",
      "Mean episodic reward                    : -565.07400\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    0.98290\n",
      "Policy loss                             :    0.00967\n",
      "Value function loss                     :  111.87753\n",
      "Mean rewards                            :    0.58160\n",
      "Max. rewards                            :   30.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   23.63635\n",
      "Mean advantages                         :   -0.05174\n",
      "Mean (norm.) advantages                 :   -0.05174\n",
      "Mean (discounted) returns               :   23.58461\n",
      "Mean normalized returns                 :   23.58461\n",
      "Mean entropy                            :    2.91079\n",
      "Variance explained by the value function:    0.27158\n",
      "Std. of action_0 over agents            :    3.37658\n",
      "Std. of action_0 over envs              :    3.48958\n",
      "Std. of action_0 over time              :    3.52277\n",
      "Std. of action_1 over agents            :    1.59840\n",
      "Std. of action_1 over envs              :    1.84466\n",
      "Std. of action_1 over time              :    1.91278\n",
      "Current timestep                        : 5900000.00000\n",
      "Gradient norm                           :    0.87798\n",
      "Mean episodic reward                    :  588.56000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 590 / 1000   \n",
      "Mean training time per iter (ms)        :     131.39\n",
      "Mean steps per sec (training time)      :   76110.99\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    0.03581\n",
      "Policy loss                             :    0.23299\n",
      "Value function loss                     :    4.11153\n",
      "Mean rewards                            :   -0.02778\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.26051\n",
      "Mean advantages                         :    0.04970\n",
      "Mean (norm.) advantages                 :    0.04970\n",
      "Mean (discounted) returns               :   -1.21081\n",
      "Mean normalized returns                 :   -1.21081\n",
      "Mean entropy                            :    4.76589\n",
      "Variance explained by the value function:    0.17432\n",
      "Std. of action_0 over agents            :    3.20113\n",
      "Std. of action_0 over envs              :    3.19792\n",
      "Std. of action_0 over time              :    3.20065\n",
      "Std. of action_1 over agents            :    3.15068\n",
      "Std. of action_1 over envs              :    3.14644\n",
      "Std. of action_1 over time              :    3.15218\n",
      "Current timestep                        : 6000000.00000\n",
      "Gradient norm                           :    0.02276\n",
      "Mean episodic reward                    : -563.92800\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.69090\n",
      "Policy loss                             :   -1.77003\n",
      "Value function loss                     :  123.61098\n",
      "Mean rewards                            :    0.57840\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   23.90685\n",
      "Mean advantages                         :   -0.56174\n",
      "Mean (norm.) advantages                 :   -0.56174\n",
      "Mean (discounted) returns               :   23.34510\n",
      "Mean normalized returns                 :   23.34510\n",
      "Mean entropy                            :    3.13955\n",
      "Variance explained by the value function:    0.28989\n",
      "Std. of action_0 over agents            :    2.08701\n",
      "Std. of action_0 over envs              :    2.29075\n",
      "Std. of action_0 over time              :    2.33911\n",
      "Std. of action_1 over agents            :    1.29443\n",
      "Std. of action_1 over envs              :    1.44860\n",
      "Std. of action_1 over time              :    1.51060\n",
      "Current timestep                        : 6000000.00000\n",
      "Gradient norm                           :    1.08993\n",
      "Mean episodic reward                    :  587.28000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 600 / 1000   \n",
      "Mean training time per iter (ms)        :     131.42\n",
      "Mean steps per sec (training time)      :   76090.07\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    0.14761\n",
      "Policy loss                             :    0.34345\n",
      "Value function loss                     :    4.21633\n",
      "Mean rewards                            :   -0.02780\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.31222\n",
      "Mean advantages                         :    0.07579\n",
      "Mean (norm.) advantages                 :    0.07579\n",
      "Mean (discounted) returns               :   -1.23644\n",
      "Mean normalized returns                 :   -1.23644\n",
      "Mean entropy                            :    4.76005\n",
      "Variance explained by the value function:    0.15355\n",
      "Std. of action_0 over agents            :    3.17760\n",
      "Std. of action_0 over envs              :    3.17276\n",
      "Std. of action_0 over time              :    3.17647\n",
      "Std. of action_1 over agents            :    3.14696\n",
      "Std. of action_1 over envs              :    3.14291\n",
      "Std. of action_1 over time              :    3.14829\n",
      "Current timestep                        : 6100000.00000\n",
      "Gradient norm                           :    0.03321\n",
      "Mean episodic reward                    : -552.25200\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.73585\n",
      "Policy loss                             :   -1.80803\n",
      "Value function loss                     :  121.95171\n",
      "Mean rewards                            :    0.57760\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   24.66278\n",
      "Mean advantages                         :   -0.74601\n",
      "Mean (norm.) advantages                 :   -0.74601\n",
      "Mean (discounted) returns               :   23.91677\n",
      "Mean normalized returns                 :   23.91677\n",
      "Mean entropy                            :    2.94674\n",
      "Variance explained by the value function:    0.21478\n",
      "Std. of action_0 over agents            :    2.05188\n",
      "Std. of action_0 over envs              :    2.26733\n",
      "Std. of action_0 over time              :    2.36736\n",
      "Std. of action_1 over agents            :    1.46500\n",
      "Std. of action_1 over envs              :    1.67268\n",
      "Std. of action_1 over time              :    1.71890\n",
      "Current timestep                        : 6100000.00000\n",
      "Gradient norm                           :    1.01474\n",
      "Mean episodic reward                    :  577.78000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 610 / 1000   \n",
      "Mean training time per iter (ms)        :     131.43\n",
      "Mean steps per sec (training time)      :   76085.63\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.08280\n",
      "Policy loss                             :    0.11374\n",
      "Value function loss                     :    4.11694\n",
      "Mean rewards                            :   -0.02763\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.22817\n",
      "Mean advantages                         :    0.02428\n",
      "Mean (norm.) advantages                 :    0.02428\n",
      "Mean (discounted) returns               :   -1.20389\n",
      "Mean normalized returns                 :   -1.20389\n",
      "Mean entropy                            :    4.75429\n",
      "Variance explained by the value function:    0.16423\n",
      "Std. of action_0 over agents            :    3.14703\n",
      "Std. of action_0 over envs              :    3.14286\n",
      "Std. of action_0 over time              :    3.14541\n",
      "Std. of action_1 over agents            :    3.13414\n",
      "Std. of action_1 over envs              :    3.13102\n",
      "Std. of action_1 over time              :    3.13585\n",
      "Current timestep                        : 6200000.00000\n",
      "Gradient norm                           :    0.02878\n",
      "Mean episodic reward                    : -545.17800\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    3.54672\n",
      "Policy loss                             :    2.57837\n",
      "Value function loss                     :  112.35289\n",
      "Mean rewards                            :    0.58040\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   22.38323\n",
      "Mean advantages                         :    0.83881\n",
      "Mean (norm.) advantages                 :    0.83881\n",
      "Mean (discounted) returns               :   23.22205\n",
      "Mean normalized returns                 :   23.22205\n",
      "Mean entropy                            :    3.10364\n",
      "Variance explained by the value function:    0.27547\n",
      "Std. of action_0 over agents            :    2.27488\n",
      "Std. of action_0 over envs              :    2.41966\n",
      "Std. of action_0 over time              :    2.47452\n",
      "Std. of action_1 over agents            :    1.79500\n",
      "Std. of action_1 over envs              :    1.96269\n",
      "Std. of action_1 over time              :    1.98537\n",
      "Current timestep                        : 6200000.00000\n",
      "Gradient norm                           :    0.93361\n",
      "Mean episodic reward                    :  570.42000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 620 / 1000   \n",
      "Mean training time per iter (ms)        :     131.41\n",
      "Mean steps per sec (training time)      :   76097.81\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.40477\n",
      "Policy loss                             :   -0.20684\n",
      "Value function loss                     :    4.04879\n",
      "Mean rewards                            :   -0.02623\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.10154\n",
      "Mean advantages                         :   -0.04308\n",
      "Mean (norm.) advantages                 :   -0.04308\n",
      "Mean (discounted) returns               :   -1.14462\n",
      "Mean normalized returns                 :   -1.14462\n",
      "Mean entropy                            :    4.76828\n",
      "Variance explained by the value function:    0.15046\n",
      "Std. of action_0 over agents            :    3.14149\n",
      "Std. of action_0 over envs              :    3.13660\n",
      "Std. of action_0 over time              :    3.14045\n",
      "Std. of action_1 over agents            :    3.17385\n",
      "Std. of action_1 over envs              :    3.16958\n",
      "Std. of action_1 over time              :    3.17540\n",
      "Current timestep                        : 6300000.00000\n",
      "Gradient norm                           :    0.03669\n",
      "Mean episodic reward                    : -526.33000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    5.03141\n",
      "Policy loss                             :    4.02790\n",
      "Value function loss                     :  116.06092\n",
      "Mean rewards                            :    0.55300\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   20.37084\n",
      "Mean advantages                         :    1.37528\n",
      "Mean (norm.) advantages                 :    1.37528\n",
      "Mean (discounted) returns               :   21.74612\n",
      "Mean normalized returns                 :   21.74612\n",
      "Mean entropy                            :    3.14194\n",
      "Variance explained by the value function:    0.29223\n",
      "Std. of action_0 over agents            :    2.36502\n",
      "Std. of action_0 over envs              :    2.54840\n",
      "Std. of action_0 over time              :    2.60389\n",
      "Std. of action_1 over agents            :    2.02204\n",
      "Std. of action_1 over envs              :    2.19609\n",
      "Std. of action_1 over time              :    2.21513\n",
      "Current timestep                        : 6300000.00000\n",
      "Gradient norm                           :    1.18241\n",
      "Mean episodic reward                    :  554.18000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 630 / 1000   \n",
      "Mean training time per iter (ms)        :     131.39\n",
      "Mean steps per sec (training time)      :   76108.91\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.14864\n",
      "Policy loss                             :    0.04910\n",
      "Value function loss                     :    4.03307\n",
      "Mean rewards                            :   -0.02687\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.16444\n",
      "Mean advantages                         :    0.01123\n",
      "Mean (norm.) advantages                 :    0.01123\n",
      "Mean (discounted) returns               :   -1.15321\n",
      "Mean normalized returns                 :   -1.15321\n",
      "Mean entropy                            :    4.76137\n",
      "Variance explained by the value function:    0.16492\n",
      "Std. of action_0 over agents            :    3.14725\n",
      "Std. of action_0 over envs              :    3.14324\n",
      "Std. of action_0 over time              :    3.14561\n",
      "Std. of action_1 over agents            :    3.17265\n",
      "Std. of action_1 over envs              :    3.16767\n",
      "Std. of action_1 over time              :    3.17416\n",
      "Current timestep                        : 6400000.00000\n",
      "Gradient norm                           :    0.03134\n",
      "Mean episodic reward                    : -553.48800\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    1.77719\n",
      "Policy loss                             :    0.80037\n",
      "Value function loss                     :  112.58723\n",
      "Mean rewards                            :    0.56360\n",
      "Max. rewards                            :   30.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   21.42217\n",
      "Mean advantages                         :    0.48211\n",
      "Mean (norm.) advantages                 :    0.48211\n",
      "Mean (discounted) returns               :   21.90427\n",
      "Mean normalized returns                 :   21.90427\n",
      "Mean entropy                            :    2.98098\n",
      "Variance explained by the value function:    0.34493\n",
      "Std. of action_0 over agents            :    2.58535\n",
      "Std. of action_0 over envs              :    2.78940\n",
      "Std. of action_0 over time              :    2.85302\n",
      "Std. of action_1 over agents            :    1.46955\n",
      "Std. of action_1 over envs              :    1.72610\n",
      "Std. of action_1 over time              :    1.81765\n",
      "Current timestep                        : 6400000.00000\n",
      "Gradient norm                           :    1.04245\n",
      "Mean episodic reward                    :  577.16000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 640 / 1000   \n",
      "Mean training time per iter (ms)        :     131.42\n",
      "Mean steps per sec (training time)      :   76093.31\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.58180\n",
      "Policy loss                             :   -0.38580\n",
      "Value function loss                     :    4.17155\n",
      "Mean rewards                            :   -0.02745\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.08903\n",
      "Mean advantages                         :   -0.07911\n",
      "Mean (norm.) advantages                 :   -0.07911\n",
      "Mean (discounted) returns               :   -1.16814\n",
      "Mean normalized returns                 :   -1.16814\n",
      "Mean entropy                            :    4.75421\n",
      "Variance explained by the value function:    0.15157\n",
      "Std. of action_0 over agents            :    3.19460\n",
      "Std. of action_0 over envs              :    3.18915\n",
      "Std. of action_0 over time              :    3.19294\n",
      "Std. of action_1 over agents            :    3.13581\n",
      "Std. of action_1 over envs              :    3.13178\n",
      "Std. of action_1 over time              :    3.13717\n",
      "Current timestep                        : 6500000.00000\n",
      "Gradient norm                           :    0.02576\n",
      "Mean episodic reward                    : -533.82400\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    2.05177\n",
      "Policy loss                             :    0.94913\n",
      "Value function loss                     :  125.45804\n",
      "Mean rewards                            :    0.57420\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   22.12563\n",
      "Mean advantages                         :    0.29807\n",
      "Mean (norm.) advantages                 :    0.29807\n",
      "Mean (discounted) returns               :   22.42369\n",
      "Mean normalized returns                 :   22.42369\n",
      "Mean entropy                            :    3.03881\n",
      "Variance explained by the value function:    0.30923\n",
      "Std. of action_0 over agents            :    2.19948\n",
      "Std. of action_0 over envs              :    2.40877\n",
      "Std. of action_0 over time              :    2.53814\n",
      "Std. of action_1 over agents            :    2.32494\n",
      "Std. of action_1 over envs              :    2.41558\n",
      "Std. of action_1 over time              :    2.44702\n",
      "Current timestep                        : 6500000.00000\n",
      "Gradient norm                           :    0.96988\n",
      "Mean episodic reward                    :  560.52000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 650 / 1000   \n",
      "Mean training time per iter (ms)        :     131.40\n",
      "Mean steps per sec (training time)      :   76100.93\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.29131\n",
      "Policy loss                             :   -0.09424\n",
      "Value function loss                     :    4.17410\n",
      "Mean rewards                            :   -0.02822\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.19050\n",
      "Mean advantages                         :   -0.01948\n",
      "Mean (norm.) advantages                 :   -0.01948\n",
      "Mean (discounted) returns               :   -1.20998\n",
      "Mean normalized returns                 :   -1.20998\n",
      "Mean entropy                            :    4.77623\n",
      "Variance explained by the value function:    0.16383\n",
      "Std. of action_0 over agents            :    3.18287\n",
      "Std. of action_0 over envs              :    3.17907\n",
      "Std. of action_0 over time              :    3.18387\n",
      "Std. of action_1 over agents            :    3.13487\n",
      "Std. of action_1 over envs              :    3.13069\n",
      "Std. of action_1 over time              :    3.13606\n",
      "Current timestep                        : 6600000.00000\n",
      "Gradient norm                           :    0.02440\n",
      "Mean episodic reward                    : -556.43200\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    2.26123\n",
      "Policy loss                             :    1.24621\n",
      "Value function loss                     :  116.68912\n",
      "Mean rewards                            :    0.58600\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   23.01954\n",
      "Mean advantages                         :    0.35317\n",
      "Mean (norm.) advantages                 :    0.35317\n",
      "Mean (discounted) returns               :   23.37271\n",
      "Mean normalized returns                 :   23.37271\n",
      "Mean entropy                            :    3.03736\n",
      "Variance explained by the value function:    0.25681\n",
      "Std. of action_0 over agents            :    2.46176\n",
      "Std. of action_0 over envs              :    2.67155\n",
      "Std. of action_0 over time              :    2.79139\n",
      "Std. of action_1 over agents            :    2.28172\n",
      "Std. of action_1 over envs              :    2.36076\n",
      "Std. of action_1 over time              :    2.38159\n",
      "Current timestep                        : 6600000.00000\n",
      "Gradient norm                           :    0.89056\n",
      "Mean episodic reward                    :  580.68000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 660 / 1000   \n",
      "Mean training time per iter (ms)        :     131.44\n",
      "Mean steps per sec (training time)      :   76082.24\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.39086\n",
      "Policy loss                             :   -0.19448\n",
      "Value function loss                     :    4.14688\n",
      "Mean rewards                            :   -0.02802\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.13990\n",
      "Mean advantages                         :   -0.03972\n",
      "Mean (norm.) advantages                 :   -0.03972\n",
      "Mean (discounted) returns               :   -1.17961\n",
      "Mean normalized returns                 :   -1.17961\n",
      "Mean entropy                            :    4.75702\n",
      "Variance explained by the value function:    0.16162\n",
      "Std. of action_0 over agents            :    3.19944\n",
      "Std. of action_0 over envs              :    3.19508\n",
      "Std. of action_0 over time              :    3.19645\n",
      "Std. of action_1 over agents            :    3.13580\n",
      "Std. of action_1 over envs              :    3.13226\n",
      "Std. of action_1 over time              :    3.13786\n",
      "Current timestep                        : 6700000.00000\n",
      "Gradient norm                           :    0.01965\n",
      "Mean episodic reward                    : -551.67200\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.60327\n",
      "Policy loss                             :   -1.72414\n",
      "Value function loss                     :  127.66254\n",
      "Mean rewards                            :    0.58420\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   23.66537\n",
      "Mean advantages                         :   -0.43469\n",
      "Mean (norm.) advantages                 :   -0.43469\n",
      "Mean (discounted) returns               :   23.23068\n",
      "Mean normalized returns                 :   23.23068\n",
      "Mean entropy                            :    3.11519\n",
      "Variance explained by the value function:    0.28490\n",
      "Std. of action_0 over agents            :    2.52292\n",
      "Std. of action_0 over envs              :    2.73544\n",
      "Std. of action_0 over time              :    2.82339\n",
      "Std. of action_1 over agents            :    1.70259\n",
      "Std. of action_1 over envs              :    1.91424\n",
      "Std. of action_1 over time              :    1.96555\n",
      "Current timestep                        : 6700000.00000\n",
      "Gradient norm                           :    0.96987\n",
      "Mean episodic reward                    :  576.56000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 670 / 1000   \n",
      "Mean training time per iter (ms)        :     131.42\n",
      "Mean steps per sec (training time)      :   76092.68\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    0.00764\n",
      "Policy loss                             :    0.20491\n",
      "Value function loss                     :    4.04347\n",
      "Mean rewards                            :   -0.02699\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.21515\n",
      "Mean advantages                         :    0.04446\n",
      "Mean (norm.) advantages                 :    0.04446\n",
      "Mean (discounted) returns               :   -1.17069\n",
      "Mean normalized returns                 :   -1.17069\n",
      "Mean entropy                            :    4.75394\n",
      "Variance explained by the value function:    0.16005\n",
      "Std. of action_0 over agents            :    3.18199\n",
      "Std. of action_0 over envs              :    3.17782\n",
      "Std. of action_0 over time              :    3.17947\n",
      "Std. of action_1 over agents            :    3.15115\n",
      "Std. of action_1 over envs              :    3.14654\n",
      "Std. of action_1 over time              :    3.15266\n",
      "Current timestep                        : 6800000.00000\n",
      "Gradient norm                           :    0.04202\n",
      "Mean episodic reward                    : -553.08600\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.71318\n",
      "Policy loss                             :   -1.68677\n",
      "Value function loss                     :  111.41720\n",
      "Mean rewards                            :    0.56480\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   23.17813\n",
      "Mean advantages                         :   -0.65383\n",
      "Mean (norm.) advantages                 :   -0.65383\n",
      "Mean (discounted) returns               :   22.52431\n",
      "Mean normalized returns                 :   22.52431\n",
      "Mean entropy                            :    2.81166\n",
      "Variance explained by the value function:    0.27513\n",
      "Std. of action_0 over agents            :    1.74926\n",
      "Std. of action_0 over envs              :    2.03433\n",
      "Std. of action_0 over time              :    2.19792\n",
      "Std. of action_1 over agents            :    1.28842\n",
      "Std. of action_1 over envs              :    1.45515\n",
      "Std. of action_1 over time              :    1.51473\n",
      "Current timestep                        : 6800000.00000\n",
      "Gradient norm                           :    1.10341\n",
      "Mean episodic reward                    :  576.80000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 680 / 1000   \n",
      "Mean training time per iter (ms)        :     131.41\n",
      "Mean steps per sec (training time)      :   76100.57\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.74266\n",
      "Policy loss                             :   -0.54684\n",
      "Value function loss                     :    4.24369\n",
      "Mean rewards                            :   -0.02948\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.12354\n",
      "Mean advantages                         :   -0.11521\n",
      "Mean (norm.) advantages                 :   -0.11521\n",
      "Mean (discounted) returns               :   -1.23875\n",
      "Mean normalized returns                 :   -1.23875\n",
      "Mean entropy                            :    4.76507\n",
      "Variance explained by the value function:    0.18181\n",
      "Std. of action_0 over agents            :    3.16529\n",
      "Std. of action_0 over envs              :    3.16118\n",
      "Std. of action_0 over time              :    3.16448\n",
      "Std. of action_1 over agents            :    3.14710\n",
      "Std. of action_1 over envs              :    3.14322\n",
      "Std. of action_1 over time              :    3.14840\n",
      "Current timestep                        : 6900000.00000\n",
      "Gradient norm                           :    0.03772\n",
      "Mean episodic reward                    : -561.00200\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    4.27629\n",
      "Policy loss                             :    3.15523\n",
      "Value function loss                     :  127.43267\n",
      "Mean rewards                            :    0.60880\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   23.02739\n",
      "Mean advantages                         :    1.15527\n",
      "Mean (norm.) advantages                 :    1.15527\n",
      "Mean (discounted) returns               :   24.18266\n",
      "Mean normalized returns                 :   24.18266\n",
      "Mean entropy                            :    3.06529\n",
      "Variance explained by the value function:    0.30697\n",
      "Std. of action_0 over agents            :    1.91152\n",
      "Std. of action_0 over envs              :    2.19590\n",
      "Std. of action_0 over time              :    2.34553\n",
      "Std. of action_1 over agents            :    1.78696\n",
      "Std. of action_1 over envs              :    1.96121\n",
      "Std. of action_1 over time              :    2.00706\n",
      "Current timestep                        : 6900000.00000\n",
      "Gradient norm                           :    1.04680\n",
      "Mean episodic reward                    :  584.86000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 690 / 1000   \n",
      "Mean training time per iter (ms)        :     131.42\n",
      "Mean steps per sec (training time)      :   76093.64\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.64072\n",
      "Policy loss                             :   -0.44373\n",
      "Value function loss                     :    4.12863\n",
      "Mean rewards                            :   -0.02815\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.10326\n",
      "Mean advantages                         :   -0.09225\n",
      "Mean (norm.) advantages                 :   -0.09225\n",
      "Mean (discounted) returns               :   -1.19551\n",
      "Mean normalized returns                 :   -1.19551\n",
      "Mean entropy                            :    4.76550\n",
      "Variance explained by the value function:    0.17491\n",
      "Std. of action_0 over agents            :    3.19077\n",
      "Std. of action_0 over envs              :    3.18680\n",
      "Std. of action_0 over time              :    3.18932\n",
      "Std. of action_1 over agents            :    3.15081\n",
      "Std. of action_1 over envs              :    3.14729\n",
      "Std. of action_1 over time              :    3.15248\n",
      "Current timestep                        : 7000000.00000\n",
      "Gradient norm                           :    0.03479\n",
      "Mean episodic reward                    : -573.15400\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    5.54484\n",
      "Policy loss                             :    4.50058\n",
      "Value function loss                     :  121.07406\n",
      "Mean rewards                            :    0.58600\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   21.50246\n",
      "Mean advantages                         :    1.38172\n",
      "Mean (norm.) advantages                 :    1.38172\n",
      "Mean (discounted) returns               :   22.88417\n",
      "Mean normalized returns                 :   22.88417\n",
      "Mean entropy                            :    3.32958\n",
      "Variance explained by the value function:    0.30807\n",
      "Std. of action_0 over agents            :    2.30922\n",
      "Std. of action_0 over envs              :    2.58125\n",
      "Std. of action_0 over time              :    2.67993\n",
      "Std. of action_1 over agents            :    2.14134\n",
      "Std. of action_1 over envs              :    2.21932\n",
      "Std. of action_1 over time              :    2.24002\n",
      "Current timestep                        : 7000000.00000\n",
      "Gradient norm                           :    1.14815\n",
      "Mean episodic reward                    :  594.32000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 700 / 1000   \n",
      "Mean training time per iter (ms)        :     131.42\n",
      "Mean steps per sec (training time)      :   76091.61\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -1.18202\n",
      "Policy loss                             :   -0.98700\n",
      "Value function loss                     :    4.33388\n",
      "Mean rewards                            :   -0.02840\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -0.99263\n",
      "Mean advantages                         :   -0.20896\n",
      "Mean (norm.) advantages                 :   -0.20896\n",
      "Mean (discounted) returns               :   -1.20159\n",
      "Mean normalized returns                 :   -1.20159\n",
      "Mean entropy                            :    4.76709\n",
      "Variance explained by the value function:    0.15117\n",
      "Std. of action_0 over agents            :    3.15687\n",
      "Std. of action_0 over envs              :    3.15258\n",
      "Std. of action_0 over time              :    3.15823\n",
      "Std. of action_1 over agents            :    3.18768\n",
      "Std. of action_1 over envs              :    3.18350\n",
      "Std. of action_1 over time              :    3.18890\n",
      "Current timestep                        : 7100000.00000\n",
      "Gradient norm                           :    0.03177\n",
      "Mean episodic reward                    : -542.66800\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    7.05401\n",
      "Policy loss                             :    5.93539\n",
      "Value function loss                     :  127.55536\n",
      "Mean rewards                            :    0.59100\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   21.64320\n",
      "Mean advantages                         :    1.88968\n",
      "Mean (norm.) advantages                 :    1.88968\n",
      "Mean (discounted) returns               :   23.53288\n",
      "Mean normalized returns                 :   23.53288\n",
      "Mean entropy                            :    3.13867\n",
      "Variance explained by the value function:    0.26558\n",
      "Std. of action_0 over agents            :    3.19659\n",
      "Std. of action_0 over envs              :    3.39083\n",
      "Std. of action_0 over time              :    3.43133\n",
      "Std. of action_1 over agents            :    2.17190\n",
      "Std. of action_1 over envs              :    2.33291\n",
      "Std. of action_1 over time              :    2.39964\n",
      "Current timestep                        : 7100000.00000\n",
      "Gradient norm                           :    1.13644\n",
      "Mean episodic reward                    :  568.54000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 710 / 1000   \n",
      "Mean training time per iter (ms)        :     131.42\n",
      "Mean steps per sec (training time)      :   76092.99\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    0.12173\n",
      "Policy loss                             :    0.32024\n",
      "Value function loss                     :    3.97932\n",
      "Mean rewards                            :   -0.02645\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.24298\n",
      "Mean advantages                         :    0.06853\n",
      "Mean (norm.) advantages                 :    0.06853\n",
      "Mean (discounted) returns               :   -1.17445\n",
      "Mean normalized returns                 :   -1.17445\n",
      "Mean entropy                            :    4.76600\n",
      "Variance explained by the value function:    0.17313\n",
      "Std. of action_0 over agents            :    3.17387\n",
      "Std. of action_0 over envs              :    3.16990\n",
      "Std. of action_0 over time              :    3.17308\n",
      "Std. of action_1 over agents            :    3.14205\n",
      "Std. of action_1 over envs              :    3.13698\n",
      "Std. of action_1 over time              :    3.14322\n",
      "Current timestep                        : 7200000.00000\n",
      "Gradient norm                           :    0.03182\n",
      "Mean episodic reward                    : -568.01400\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -9.84317\n",
      "Policy loss                             :  -10.87781\n",
      "Value function loss                     :  120.48790\n",
      "Mean rewards                            :    0.55800\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   26.32091\n",
      "Mean advantages                         :   -3.23924\n",
      "Mean (norm.) advantages                 :   -3.23924\n",
      "Mean (discounted) returns               :   23.08167\n",
      "Mean normalized returns                 :   23.08167\n",
      "Mean entropy                            :    3.40477\n",
      "Variance explained by the value function:    0.26949\n",
      "Std. of action_0 over agents            :    3.00225\n",
      "Std. of action_0 over envs              :    3.19219\n",
      "Std. of action_0 over time              :    3.22891\n",
      "Std. of action_1 over agents            :    2.06082\n",
      "Std. of action_1 over envs              :    2.15521\n",
      "Std. of action_1 over time              :    2.17320\n",
      "Current timestep                        : 7200000.00000\n",
      "Gradient norm                           :    1.17321\n",
      "Mean episodic reward                    :  591.24000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 720 / 1000   \n",
      "Mean training time per iter (ms)        :     131.47\n",
      "Mean steps per sec (training time)      :   76064.20\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.45367\n",
      "Policy loss                             :   -0.25851\n",
      "Value function loss                     :    4.21425\n",
      "Mean rewards                            :   -0.02754\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.16541\n",
      "Mean advantages                         :   -0.05316\n",
      "Mean (norm.) advantages                 :   -0.05316\n",
      "Mean (discounted) returns               :   -1.21857\n",
      "Mean normalized returns                 :   -1.21857\n",
      "Mean entropy                            :    4.74592\n",
      "Variance explained by the value function:    0.15314\n",
      "Std. of action_0 over agents            :    3.21593\n",
      "Std. of action_0 over envs              :    3.21080\n",
      "Std. of action_0 over time              :    3.21304\n",
      "Std. of action_1 over agents            :    3.13688\n",
      "Std. of action_1 over envs              :    3.13147\n",
      "Std. of action_1 over time              :    3.13815\n",
      "Current timestep                        : 7300000.00000\n",
      "Gradient norm                           :    0.03804\n",
      "Mean episodic reward                    : -537.87800\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    1.23690\n",
      "Policy loss                             :    0.18524\n",
      "Value function loss                     :  121.67068\n",
      "Mean rewards                            :    0.57540\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   23.67240\n",
      "Mean advantages                         :    0.05390\n",
      "Mean (norm.) advantages                 :    0.05390\n",
      "Mean (discounted) returns               :   23.72630\n",
      "Mean normalized returns                 :   23.72630\n",
      "Mean entropy                            :    3.30091\n",
      "Variance explained by the value function:    0.24785\n",
      "Std. of action_0 over agents            :    2.23300\n",
      "Std. of action_0 over envs              :    2.36538\n",
      "Std. of action_0 over time              :    2.39389\n",
      "Std. of action_1 over agents            :    2.31113\n",
      "Std. of action_1 over envs              :    2.41149\n",
      "Std. of action_1 over time              :    2.42972\n",
      "Current timestep                        : 7300000.00000\n",
      "Gradient norm                           :    1.18424\n",
      "Mean episodic reward                    :  563.80000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 730 / 1000   \n",
      "Mean training time per iter (ms)        :     131.46\n",
      "Mean steps per sec (training time)      :   76070.91\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.40600\n",
      "Policy loss                             :   -0.20858\n",
      "Value function loss                     :    4.09301\n",
      "Mean rewards                            :   -0.02705\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.11920\n",
      "Mean advantages                         :   -0.04473\n",
      "Mean (norm.) advantages                 :   -0.04473\n",
      "Mean (discounted) returns               :   -1.16393\n",
      "Mean normalized returns                 :   -1.16393\n",
      "Mean entropy                            :    4.76709\n",
      "Variance explained by the value function:    0.15559\n",
      "Std. of action_0 over agents            :    3.20028\n",
      "Std. of action_0 over envs              :    3.19613\n",
      "Std. of action_0 over time              :    3.20053\n",
      "Std. of action_1 over agents            :    3.15291\n",
      "Std. of action_1 over envs              :    3.14953\n",
      "Std. of action_1 over time              :    3.15431\n",
      "Current timestep                        : 7400000.00000\n",
      "Gradient norm                           :    0.02327\n",
      "Mean episodic reward                    : -541.72600\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    8.94342\n",
      "Policy loss                             :    7.82372\n",
      "Value function loss                     :  128.49229\n",
      "Mean rewards                            :    0.56620\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   19.54300\n",
      "Mean advantages                         :    2.46796\n",
      "Mean (norm.) advantages                 :    2.46796\n",
      "Mean (discounted) returns               :   22.01096\n",
      "Mean normalized returns                 :   22.01096\n",
      "Mean entropy                            :    3.30451\n",
      "Variance explained by the value function:    0.27110\n",
      "Std. of action_0 over agents            :    2.15864\n",
      "Std. of action_0 over envs              :    2.29840\n",
      "Std. of action_0 over time              :    2.33618\n",
      "Std. of action_1 over agents            :    2.10426\n",
      "Std. of action_1 over envs              :    2.24872\n",
      "Std. of action_1 over time              :    2.26983\n",
      "Current timestep                        : 7400000.00000\n",
      "Gradient norm                           :    1.19526\n",
      "Mean episodic reward                    :  567.74000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 740 / 1000   \n",
      "Mean training time per iter (ms)        :     131.46\n",
      "Mean steps per sec (training time)      :   76066.34\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.08838\n",
      "Policy loss                             :    0.10906\n",
      "Value function loss                     :    4.02433\n",
      "Mean rewards                            :   -0.02716\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.19793\n",
      "Mean advantages                         :    0.02257\n",
      "Mean (norm.) advantages                 :    0.02257\n",
      "Mean (discounted) returns               :   -1.17536\n",
      "Mean normalized returns                 :   -1.17536\n",
      "Mean entropy                            :    4.75362\n",
      "Variance explained by the value function:    0.17242\n",
      "Std. of action_0 over agents            :    3.22729\n",
      "Std. of action_0 over envs              :    3.22363\n",
      "Std. of action_0 over time              :    3.22619\n",
      "Std. of action_1 over agents            :    3.12576\n",
      "Std. of action_1 over envs              :    3.12240\n",
      "Std. of action_1 over time              :    3.12763\n",
      "Current timestep                        : 7500000.00000\n",
      "Gradient norm                           :    0.02426\n",
      "Mean episodic reward                    : -554.52800\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -2.23446\n",
      "Policy loss                             :   -3.19990\n",
      "Value function loss                     :  112.95174\n",
      "Mean rewards                            :    0.56820\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   23.66832\n",
      "Mean advantages                         :   -0.97887\n",
      "Mean (norm.) advantages                 :   -0.97887\n",
      "Mean (discounted) returns               :   22.68945\n",
      "Mean normalized returns                 :   22.68945\n",
      "Mean entropy                            :    3.28147\n",
      "Variance explained by the value function:    0.30452\n",
      "Std. of action_0 over agents            :    2.36275\n",
      "Std. of action_0 over envs              :    2.51470\n",
      "Std. of action_0 over time              :    2.57744\n",
      "Std. of action_1 over agents            :    1.75850\n",
      "Std. of action_1 over envs              :    1.89775\n",
      "Std. of action_1 over time              :    1.92031\n",
      "Current timestep                        : 7500000.00000\n",
      "Gradient norm                           :    1.14334\n",
      "Mean episodic reward                    :  577.88000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 750 / 1000   \n",
      "Mean training time per iter (ms)        :     131.51\n",
      "Mean steps per sec (training time)      :   76039.39\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.43181\n",
      "Policy loss                             :   -0.23549\n",
      "Value function loss                     :    4.12539\n",
      "Mean rewards                            :   -0.02811\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.14847\n",
      "Mean advantages                         :   -0.04847\n",
      "Mean (norm.) advantages                 :   -0.04847\n",
      "Mean (discounted) returns               :   -1.19694\n",
      "Mean normalized returns                 :   -1.19694\n",
      "Mean entropy                            :    4.75150\n",
      "Variance explained by the value function:    0.17034\n",
      "Std. of action_0 over agents            :    3.18893\n",
      "Std. of action_0 over envs              :    3.18408\n",
      "Std. of action_0 over time              :    3.18777\n",
      "Std. of action_1 over agents            :    3.12208\n",
      "Std. of action_1 over envs              :    3.11801\n",
      "Std. of action_1 over time              :    3.12358\n",
      "Current timestep                        : 7600000.00000\n",
      "Gradient norm                           :    0.02734\n",
      "Mean episodic reward                    : -535.45400\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.49228\n",
      "Policy loss                             :   -1.60367\n",
      "Value function loss                     :  127.20393\n",
      "Mean rewards                            :    0.58680\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   23.82341\n",
      "Mean advantages                         :   -0.46620\n",
      "Mean (norm.) advantages                 :   -0.46620\n",
      "Mean (discounted) returns               :   23.35721\n",
      "Mean normalized returns                 :   23.35721\n",
      "Mean entropy                            :    3.21292\n",
      "Variance explained by the value function:    0.28449\n",
      "Std. of action_0 over agents            :    2.70679\n",
      "Std. of action_0 over envs              :    2.89904\n",
      "Std. of action_0 over time              :    2.94759\n",
      "Std. of action_1 over agents            :    1.79010\n",
      "Std. of action_1 over envs              :    1.92182\n",
      "Std. of action_1 over time              :    1.94822\n",
      "Current timestep                        : 7600000.00000\n",
      "Gradient norm                           :    0.91335\n",
      "Mean episodic reward                    :  561.52000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 760 / 1000   \n",
      "Mean training time per iter (ms)        :     131.50\n",
      "Mean steps per sec (training time)      :   76045.84\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.48779\n",
      "Policy loss                             :   -0.29094\n",
      "Value function loss                     :    4.14791\n",
      "Mean rewards                            :   -0.02790\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.15647\n",
      "Mean advantages                         :   -0.06015\n",
      "Mean (norm.) advantages                 :   -0.06015\n",
      "Mean (discounted) returns               :   -1.21662\n",
      "Mean normalized returns                 :   -1.21662\n",
      "Mean entropy                            :    4.76647\n",
      "Variance explained by the value function:    0.16544\n",
      "Std. of action_0 over agents            :    3.17417\n",
      "Std. of action_0 over envs              :    3.16971\n",
      "Std. of action_0 over time              :    3.17336\n",
      "Std. of action_1 over agents            :    3.14666\n",
      "Std. of action_1 over envs              :    3.14271\n",
      "Std. of action_1 over time              :    3.14805\n",
      "Current timestep                        : 7700000.00000\n",
      "Gradient norm                           :    0.02122\n",
      "Mean episodic reward                    : -539.06000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    7.92600\n",
      "Policy loss                             :    6.82271\n",
      "Value function loss                     :  127.35061\n",
      "Mean rewards                            :    0.58140\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   21.23464\n",
      "Mean advantages                         :    2.02077\n",
      "Mean (norm.) advantages                 :    2.02077\n",
      "Mean (discounted) returns               :   23.25541\n",
      "Mean normalized returns                 :   23.25541\n",
      "Mean entropy                            :    3.40440\n",
      "Variance explained by the value function:    0.23544\n",
      "Std. of action_0 over agents            :    2.59771\n",
      "Std. of action_0 over envs              :    2.80697\n",
      "Std. of action_0 over time              :    2.85103\n",
      "Std. of action_1 over agents            :    2.06664\n",
      "Std. of action_1 over envs              :    2.21571\n",
      "Std. of action_1 over time              :    2.23328\n",
      "Current timestep                        : 7700000.00000\n",
      "Gradient norm                           :    1.12072\n",
      "Mean episodic reward                    :  565.10000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 770 / 1000   \n",
      "Mean training time per iter (ms)        :     131.51\n",
      "Mean steps per sec (training time)      :   76041.69\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.45866\n",
      "Policy loss                             :   -0.26265\n",
      "Value function loss                     :    4.19262\n",
      "Mean rewards                            :   -0.02856\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.17547\n",
      "Mean advantages                         :   -0.05442\n",
      "Mean (norm.) advantages                 :   -0.05442\n",
      "Mean (discounted) returns               :   -1.22989\n",
      "Mean normalized returns                 :   -1.22989\n",
      "Mean entropy                            :    4.75884\n",
      "Variance explained by the value function:    0.17298\n",
      "Std. of action_0 over agents            :    3.17663\n",
      "Std. of action_0 over envs              :    3.17194\n",
      "Std. of action_0 over time              :    3.17501\n",
      "Std. of action_1 over agents            :    3.13906\n",
      "Std. of action_1 over envs              :    3.13573\n",
      "Std. of action_1 over time              :    3.14113\n",
      "Current timestep                        : 7800000.00000\n",
      "Gradient norm                           :    0.02224\n",
      "Mean episodic reward                    : -559.48400\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    3.26426\n",
      "Policy loss                             :    2.20624\n",
      "Value function loss                     :  122.67312\n",
      "Mean rewards                            :    0.59340\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   23.28918\n",
      "Mean advantages                         :    0.71340\n",
      "Mean (norm.) advantages                 :    0.71340\n",
      "Mean (discounted) returns               :   24.00257\n",
      "Mean normalized returns                 :   24.00257\n",
      "Mean entropy                            :    3.37422\n",
      "Variance explained by the value function:    0.27916\n",
      "Std. of action_0 over agents            :    3.19995\n",
      "Std. of action_0 over envs              :    3.38429\n",
      "Std. of action_0 over time              :    3.40444\n",
      "Std. of action_1 over agents            :    2.05925\n",
      "Std. of action_1 over envs              :    2.17518\n",
      "Std. of action_1 over time              :    2.18703\n",
      "Current timestep                        : 7800000.00000\n",
      "Gradient norm                           :    1.05449\n",
      "Mean episodic reward                    :  583.20000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 780 / 1000   \n",
      "Mean training time per iter (ms)        :     131.52\n",
      "Mean steps per sec (training time)      :   76035.50\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.41926\n",
      "Policy loss                             :   -0.22322\n",
      "Value function loss                     :    4.18300\n",
      "Mean rewards                            :   -0.02820\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.16410\n",
      "Mean advantages                         :   -0.04672\n",
      "Mean (norm.) advantages                 :   -0.04672\n",
      "Mean (discounted) returns               :   -1.21083\n",
      "Mean normalized returns                 :   -1.21083\n",
      "Mean entropy                            :    4.75737\n",
      "Variance explained by the value function:    0.16522\n",
      "Std. of action_0 over agents            :    3.17312\n",
      "Std. of action_0 over envs              :    3.16929\n",
      "Std. of action_0 over time              :    3.17282\n",
      "Std. of action_1 over agents            :    3.13632\n",
      "Std. of action_1 over envs              :    3.13291\n",
      "Std. of action_1 over time              :    3.13719\n",
      "Current timestep                        : 7900000.00000\n",
      "Gradient norm                           :    0.02719\n",
      "Mean episodic reward                    : -558.58000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    3.08698\n",
      "Policy loss                             :    2.03170\n",
      "Value function loss                     :  123.22449\n",
      "Mean rewards                            :    0.58680\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   23.02211\n",
      "Mean advantages                         :    0.57950\n",
      "Mean (norm.) advantages                 :    0.57950\n",
      "Mean (discounted) returns               :   23.60161\n",
      "Mean normalized returns                 :   23.60161\n",
      "Mean entropy                            :    3.53928\n",
      "Variance explained by the value function:    0.26366\n",
      "Std. of action_0 over agents            :    3.07204\n",
      "Std. of action_0 over envs              :    3.31728\n",
      "Std. of action_0 over time              :    3.37429\n",
      "Std. of action_1 over agents            :    1.98237\n",
      "Std. of action_1 over envs              :    2.11455\n",
      "Std. of action_1 over time              :    2.13927\n",
      "Current timestep                        : 7900000.00000\n",
      "Gradient norm                           :    1.00784\n",
      "Mean episodic reward                    :  582.08000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 790 / 1000   \n",
      "Mean training time per iter (ms)        :     131.51\n",
      "Mean steps per sec (training time)      :   76040.35\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.30213\n",
      "Policy loss                             :   -0.10642\n",
      "Value function loss                     :    4.15044\n",
      "Mean rewards                            :   -0.02711\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.17433\n",
      "Mean advantages                         :   -0.02023\n",
      "Mean (norm.) advantages                 :   -0.02023\n",
      "Mean (discounted) returns               :   -1.19456\n",
      "Mean normalized returns                 :   -1.19456\n",
      "Mean entropy                            :    4.74421\n",
      "Variance explained by the value function:    0.15254\n",
      "Std. of action_0 over agents            :    3.20932\n",
      "Std. of action_0 over envs              :    3.20477\n",
      "Std. of action_0 over time              :    3.20619\n",
      "Std. of action_1 over agents            :    3.15346\n",
      "Std. of action_1 over envs              :    3.14944\n",
      "Std. of action_1 over time              :    3.15497\n",
      "Current timestep                        : 8000000.00000\n",
      "Gradient norm                           :    0.04136\n",
      "Mean episodic reward                    : -546.62600\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    0.69346\n",
      "Policy loss                             :   -0.29057\n",
      "Value function loss                     :  116.33506\n",
      "Mean rewards                            :    0.56920\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   23.33390\n",
      "Mean advantages                         :   -0.11530\n",
      "Mean (norm.) advantages                 :   -0.11530\n",
      "Mean (discounted) returns               :   23.21861\n",
      "Mean normalized returns                 :   23.21861\n",
      "Mean entropy                            :    3.58649\n",
      "Variance explained by the value function:    0.23829\n",
      "Std. of action_0 over agents            :    2.38557\n",
      "Std. of action_0 over envs              :    2.59163\n",
      "Std. of action_0 over time              :    2.66794\n",
      "Std. of action_1 over agents            :    2.56035\n",
      "Std. of action_1 over envs              :    2.66890\n",
      "Std. of action_1 over time              :    2.67845\n",
      "Current timestep                        : 8000000.00000\n",
      "Gradient norm                           :    1.11066\n",
      "Mean episodic reward                    :  571.22000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 800 / 1000   \n",
      "Mean training time per iter (ms)        :     131.50\n",
      "Mean steps per sec (training time)      :   76046.89\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    0.16496\n",
      "Policy loss                             :    0.36233\n",
      "Value function loss                     :    4.00833\n",
      "Mean rewards                            :   -0.02715\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.27469\n",
      "Mean advantages                         :    0.07587\n",
      "Mean (norm.) advantages                 :    0.07587\n",
      "Mean (discounted) returns               :   -1.19882\n",
      "Mean normalized returns                 :   -1.19882\n",
      "Mean entropy                            :    4.74901\n",
      "Variance explained by the value function:    0.17956\n",
      "Std. of action_0 over agents            :    3.19388\n",
      "Std. of action_0 over envs              :    3.18879\n",
      "Std. of action_0 over time              :    3.19184\n",
      "Std. of action_1 over agents            :    3.17218\n",
      "Std. of action_1 over envs              :    3.16777\n",
      "Std. of action_1 over time              :    3.17340\n",
      "Current timestep                        : 8100000.00000\n",
      "Gradient norm                           :    0.04113\n",
      "Mean episodic reward                    : -541.02800\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    0.09909\n",
      "Policy loss                             :   -0.92593\n",
      "Value function loss                     :  119.10222\n",
      "Mean rewards                            :    0.56820\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   23.41670\n",
      "Mean advantages                         :   -0.36695\n",
      "Mean (norm.) advantages                 :   -0.36695\n",
      "Mean (discounted) returns               :   23.04976\n",
      "Mean normalized returns                 :   23.04976\n",
      "Mean entropy                            :    3.32007\n",
      "Variance explained by the value function:    0.27340\n",
      "Std. of action_0 over agents            :    2.27286\n",
      "Std. of action_0 over envs              :    2.45923\n",
      "Std. of action_0 over time              :    2.54284\n",
      "Std. of action_1 over agents            :    1.76278\n",
      "Std. of action_1 over envs              :    1.89776\n",
      "Std. of action_1 over time              :    1.92484\n",
      "Current timestep                        : 8100000.00000\n",
      "Gradient norm                           :    1.09730\n",
      "Mean episodic reward                    :  566.34000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 810 / 1000   \n",
      "Mean training time per iter (ms)        :     131.49\n",
      "Mean steps per sec (training time)      :   76050.27\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.18113\n",
      "Policy loss                             :    0.01574\n",
      "Value function loss                     :    4.08524\n",
      "Mean rewards                            :   -0.02647\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.18495\n",
      "Mean advantages                         :    0.00469\n",
      "Mean (norm.) advantages                 :    0.00469\n",
      "Mean (discounted) returns               :   -1.18026\n",
      "Mean normalized returns                 :   -1.18026\n",
      "Mean entropy                            :    4.75449\n",
      "Variance explained by the value function:    0.14952\n",
      "Std. of action_0 over agents            :    3.16292\n",
      "Std. of action_0 over envs              :    3.15804\n",
      "Std. of action_0 over time              :    3.16273\n",
      "Std. of action_1 over agents            :    3.16270\n",
      "Std. of action_1 over envs              :    3.15975\n",
      "Std. of action_1 over time              :    3.16472\n",
      "Current timestep                        : 8200000.00000\n",
      "Gradient norm                           :    0.05318\n",
      "Mean episodic reward                    : -553.83400\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -3.44440\n",
      "Policy loss                             :   -4.39817\n",
      "Value function loss                     :  110.89462\n",
      "Mean rewards                            :    0.55520\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   24.62424\n",
      "Mean advantages                         :   -1.54790\n",
      "Mean (norm.) advantages                 :   -1.54790\n",
      "Mean (discounted) returns               :   23.07634\n",
      "Mean normalized returns                 :   23.07634\n",
      "Mean entropy                            :    3.10346\n",
      "Variance explained by the value function:    0.21372\n",
      "Std. of action_0 over agents            :    1.90018\n",
      "Std. of action_0 over envs              :    2.07247\n",
      "Std. of action_0 over time              :    2.11348\n",
      "Std. of action_1 over agents            :    2.92395\n",
      "Std. of action_1 over envs              :    3.06655\n",
      "Std. of action_1 over time              :    3.11144\n",
      "Current timestep                        : 8200000.00000\n",
      "Gradient norm                           :    1.11373\n",
      "Mean episodic reward                    :  577.52000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 820 / 1000   \n",
      "Mean training time per iter (ms)        :     131.50\n",
      "Mean steps per sec (training time)      :   76043.32\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.54713\n",
      "Policy loss                             :   -0.35098\n",
      "Value function loss                     :    4.16105\n",
      "Mean rewards                            :   -0.02791\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.13524\n",
      "Mean advantages                         :   -0.07293\n",
      "Mean (norm.) advantages                 :   -0.07293\n",
      "Mean (discounted) returns               :   -1.20816\n",
      "Mean normalized returns                 :   -1.20816\n",
      "Mean entropy                            :    4.75520\n",
      "Variance explained by the value function:    0.17070\n",
      "Std. of action_0 over agents            :    3.18017\n",
      "Std. of action_0 over envs              :    3.17518\n",
      "Std. of action_0 over time              :    3.17946\n",
      "Std. of action_1 over agents            :    3.16972\n",
      "Std. of action_1 over envs              :    3.16651\n",
      "Std. of action_1 over time              :    3.17093\n",
      "Current timestep                        : 8300000.00000\n",
      "Gradient norm                           :    0.02979\n",
      "Mean episodic reward                    : -531.13600\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    1.88991\n",
      "Policy loss                             :    0.69926\n",
      "Value function loss                     :  136.24913\n",
      "Mean rewards                            :    0.57500\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   22.94963\n",
      "Mean advantages                         :    0.26201\n",
      "Mean (norm.) advantages                 :    0.26201\n",
      "Mean (discounted) returns               :   23.21163\n",
      "Mean normalized returns                 :   23.21163\n",
      "Mean entropy                            :    3.43683\n",
      "Variance explained by the value function:    0.27667\n",
      "Std. of action_0 over agents            :    2.21936\n",
      "Std. of action_0 over envs              :    2.43651\n",
      "Std. of action_0 over time              :    2.47854\n",
      "Std. of action_1 over agents            :    3.44087\n",
      "Std. of action_1 over envs              :    3.56183\n",
      "Std. of action_1 over time              :    3.57987\n",
      "Current timestep                        : 8300000.00000\n",
      "Gradient norm                           :    1.15732\n",
      "Mean episodic reward                    :  557.46000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 830 / 1000   \n",
      "Mean training time per iter (ms)        :     131.51\n",
      "Mean steps per sec (training time)      :   76040.70\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.25760\n",
      "Policy loss                             :   -0.06048\n",
      "Value function loss                     :    4.00246\n",
      "Mean rewards                            :   -0.02635\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.16154\n",
      "Mean advantages                         :   -0.01203\n",
      "Mean (norm.) advantages                 :   -0.01203\n",
      "Mean (discounted) returns               :   -1.17357\n",
      "Mean normalized returns                 :   -1.17357\n",
      "Mean entropy                            :    4.74290\n",
      "Variance explained by the value function:    0.16384\n",
      "Std. of action_0 over agents            :    3.17982\n",
      "Std. of action_0 over envs              :    3.17352\n",
      "Std. of action_0 over time              :    3.17499\n",
      "Std. of action_1 over agents            :    3.14941\n",
      "Std. of action_1 over envs              :    3.14560\n",
      "Std. of action_1 over time              :    3.15093\n",
      "Current timestep                        : 8400000.00000\n",
      "Gradient norm                           :    0.02441\n",
      "Mean episodic reward                    : -526.12800\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    8.96353\n",
      "Policy loss                             :    7.83867\n",
      "Value function loss                     :  129.46356\n",
      "Mean rewards                            :    0.55260\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   19.66001\n",
      "Mean advantages                         :    2.29976\n",
      "Mean (norm.) advantages                 :    2.29976\n",
      "Mean (discounted) returns               :   21.95976\n",
      "Mean normalized returns                 :   21.95976\n",
      "Mean entropy                            :    3.39555\n",
      "Variance explained by the value function:    0.23797\n",
      "Std. of action_0 over agents            :    2.23432\n",
      "Std. of action_0 over envs              :    2.43526\n",
      "Std. of action_0 over time              :    2.49098\n",
      "Std. of action_1 over agents            :    2.92587\n",
      "Std. of action_1 over envs              :    3.10808\n",
      "Std. of action_1 over time              :    3.12148\n",
      "Current timestep                        : 8400000.00000\n",
      "Gradient norm                           :    1.17941\n",
      "Mean episodic reward                    :  552.88000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 840 / 1000   \n",
      "Mean training time per iter (ms)        :     131.51\n",
      "Mean steps per sec (training time)      :   76042.39\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.55394\n",
      "Policy loss                             :   -0.35504\n",
      "Value function loss                     :    3.90331\n",
      "Mean rewards                            :   -0.02618\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.06174\n",
      "Mean advantages                         :   -0.07353\n",
      "Mean (norm.) advantages                 :   -0.07353\n",
      "Mean (discounted) returns               :   -1.13527\n",
      "Mean normalized returns                 :   -1.13527\n",
      "Mean entropy                            :    4.75863\n",
      "Variance explained by the value function:    0.17935\n",
      "Std. of action_0 over agents            :    3.15988\n",
      "Std. of action_0 over envs              :    3.15444\n",
      "Std. of action_0 over time              :    3.15931\n",
      "Std. of action_1 over agents            :    3.14429\n",
      "Std. of action_1 over envs              :    3.14045\n",
      "Std. of action_1 over time              :    3.14564\n",
      "Current timestep                        : 8500000.00000\n",
      "Gradient norm                           :    0.03652\n",
      "Mean episodic reward                    : -532.17600\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -5.34770\n",
      "Policy loss                             :   -6.53455\n",
      "Value function loss                     :  136.23773\n",
      "Mean rewards                            :    0.54840\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   24.07002\n",
      "Mean advantages                         :   -1.92602\n",
      "Mean (norm.) advantages                 :   -1.92602\n",
      "Mean (discounted) returns               :   22.14400\n",
      "Mean normalized returns                 :   22.14400\n",
      "Mean entropy                            :    3.51055\n",
      "Variance explained by the value function:    0.22309\n",
      "Std. of action_0 over agents            :    2.53538\n",
      "Std. of action_0 over envs              :    2.72877\n",
      "Std. of action_0 over time              :    2.77942\n",
      "Std. of action_1 over agents            :    3.29677\n",
      "Std. of action_1 over envs              :    3.46886\n",
      "Std. of action_1 over time              :    3.47977\n",
      "Current timestep                        : 8500000.00000\n",
      "Gradient norm                           :    1.19676\n",
      "Mean episodic reward                    :  557.78000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 850 / 1000   \n",
      "Mean training time per iter (ms)        :     131.50\n",
      "Mean steps per sec (training time)      :   76045.86\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.14986\n",
      "Policy loss                             :    0.04689\n",
      "Value function loss                     :    4.17001\n",
      "Mean rewards                            :   -0.02834\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.23012\n",
      "Mean advantages                         :    0.01202\n",
      "Mean (norm.) advantages                 :    0.01202\n",
      "Mean (discounted) returns               :   -1.21811\n",
      "Mean normalized returns                 :   -1.21811\n",
      "Mean entropy                            :    4.76905\n",
      "Variance explained by the value function:    0.16626\n",
      "Std. of action_0 over agents            :    3.21452\n",
      "Std. of action_0 over envs              :    3.20996\n",
      "Std. of action_0 over time              :    3.21457\n",
      "Std. of action_1 over agents            :    3.15778\n",
      "Std. of action_1 over envs              :    3.15413\n",
      "Std. of action_1 over time              :    3.15927\n",
      "Current timestep                        : 8600000.00000\n",
      "Gradient norm                           :    0.03752\n",
      "Mean episodic reward                    : -569.90600\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -6.14388\n",
      "Policy loss                             :   -7.34946\n",
      "Value function loss                     :  136.72488\n",
      "Mean rewards                            :    0.59260\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   26.56596\n",
      "Mean advantages                         :   -2.38842\n",
      "Mean (norm.) advantages                 :   -2.38842\n",
      "Mean (discounted) returns               :   24.17754\n",
      "Mean normalized returns                 :   24.17754\n",
      "Mean entropy                            :    3.23335\n",
      "Variance explained by the value function:    0.22871\n",
      "Std. of action_0 over agents            :    2.42495\n",
      "Std. of action_0 over envs              :    2.63417\n",
      "Std. of action_0 over time              :    2.70486\n",
      "Std. of action_1 over agents            :    3.07826\n",
      "Std. of action_1 over envs              :    3.21062\n",
      "Std. of action_1 over time              :    3.23284\n",
      "Current timestep                        : 8600000.00000\n",
      "Gradient norm                           :    1.08104\n",
      "Mean episodic reward                    :  590.64000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 860 / 1000   \n",
      "Mean training time per iter (ms)        :     131.52\n",
      "Mean steps per sec (training time)      :   76031.23\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.50445\n",
      "Policy loss                             :   -0.30974\n",
      "Value function loss                     :    4.26361\n",
      "Mean rewards                            :   -0.02970\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.20826\n",
      "Mean advantages                         :   -0.06295\n",
      "Mean (norm.) advantages                 :   -0.06295\n",
      "Mean (discounted) returns               :   -1.27120\n",
      "Mean normalized returns                 :   -1.27120\n",
      "Mean entropy                            :    4.74693\n",
      "Variance explained by the value function:    0.18125\n",
      "Std. of action_0 over agents            :    3.21300\n",
      "Std. of action_0 over envs              :    3.20802\n",
      "Std. of action_0 over time              :    3.21187\n",
      "Std. of action_1 over agents            :    3.17699\n",
      "Std. of action_1 over envs              :    3.17196\n",
      "Std. of action_1 over time              :    3.17810\n",
      "Current timestep                        : 8700000.00000\n",
      "Gradient norm                           :    0.03724\n",
      "Mean episodic reward                    : -557.93800\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   11.62696\n",
      "Policy loss                             :   10.38947\n",
      "Value function loss                     :  140.95999\n",
      "Mean rewards                            :    0.61020\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   21.09140\n",
      "Mean advantages                         :    3.08616\n",
      "Mean (norm.) advantages                 :    3.08616\n",
      "Mean (discounted) returns               :   24.17756\n",
      "Mean normalized returns                 :   24.17756\n",
      "Mean entropy                            :    3.44212\n",
      "Variance explained by the value function:    0.27942\n",
      "Std. of action_0 over agents            :    2.64000\n",
      "Std. of action_0 over envs              :    2.85386\n",
      "Std. of action_0 over time              :    2.91016\n",
      "Std. of action_1 over agents            :    2.54975\n",
      "Std. of action_1 over envs              :    2.75341\n",
      "Std. of action_1 over time              :    2.77565\n",
      "Current timestep                        : 8700000.00000\n",
      "Gradient norm                           :    1.13154\n",
      "Mean episodic reward                    :  579.78000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 870 / 1000   \n",
      "Mean training time per iter (ms)        :     131.51\n",
      "Mean steps per sec (training time)      :   76037.99\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -1.18635\n",
      "Policy loss                             :   -0.99188\n",
      "Value function loss                     :    4.25193\n",
      "Mean rewards                            :   -0.02911\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -0.99437\n",
      "Mean advantages                         :   -0.21242\n",
      "Mean (norm.) advantages                 :   -0.21242\n",
      "Mean (discounted) returns               :   -1.20678\n",
      "Mean normalized returns                 :   -1.20678\n",
      "Mean entropy                            :    4.73981\n",
      "Variance explained by the value function:    0.18426\n",
      "Std. of action_0 over agents            :    3.16924\n",
      "Std. of action_0 over envs              :    3.16311\n",
      "Std. of action_0 over time              :    3.16743\n",
      "Std. of action_1 over agents            :    3.16646\n",
      "Std. of action_1 over envs              :    3.16223\n",
      "Std. of action_1 over time              :    3.16647\n",
      "Current timestep                        : 8800000.00000\n",
      "Gradient norm                           :    0.03852\n",
      "Mean episodic reward                    : -537.97600\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    8.07780\n",
      "Policy loss                             :    6.92204\n",
      "Value function loss                     :  131.69608\n",
      "Mean rewards                            :    0.60440\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   21.59939\n",
      "Mean advantages                         :    2.28520\n",
      "Mean (norm.) advantages                 :    2.28520\n",
      "Mean (discounted) returns               :   23.88459\n",
      "Mean normalized returns                 :   23.88459\n",
      "Mean entropy                            :    3.22398\n",
      "Variance explained by the value function:    0.31692\n",
      "Std. of action_0 over agents            :    2.79544\n",
      "Std. of action_0 over envs              :    3.00661\n",
      "Std. of action_0 over time              :    3.04387\n",
      "Std. of action_1 over agents            :    2.94543\n",
      "Std. of action_1 over envs              :    3.11991\n",
      "Std. of action_1 over time              :    3.14387\n",
      "Current timestep                        : 8800000.00000\n",
      "Gradient norm                           :    1.11211\n",
      "Mean episodic reward                    :  564.10000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 880 / 1000   \n",
      "Mean training time per iter (ms)        :     131.49\n",
      "Mean steps per sec (training time)      :   76052.14\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.48378\n",
      "Policy loss                             :   -0.28822\n",
      "Value function loss                     :    4.29793\n",
      "Mean rewards                            :   -0.02909\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.18003\n",
      "Mean advantages                         :   -0.05871\n",
      "Mean (norm.) advantages                 :   -0.05871\n",
      "Mean (discounted) returns               :   -1.23874\n",
      "Mean normalized returns                 :   -1.23874\n",
      "Mean entropy                            :    4.77072\n",
      "Variance explained by the value function:    0.16179\n",
      "Std. of action_0 over agents            :    3.14665\n",
      "Std. of action_0 over envs              :    3.14209\n",
      "Std. of action_0 over time              :    3.14798\n",
      "Std. of action_1 over agents            :    3.13769\n",
      "Std. of action_1 over envs              :    3.13413\n",
      "Std. of action_1 over time              :    3.13955\n",
      "Current timestep                        : 8900000.00000\n",
      "Gradient norm                           :    0.03870\n",
      "Mean episodic reward                    : -587.85400\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -3.38747\n",
      "Policy loss                             :   -4.39543\n",
      "Value function loss                     :  115.76301\n",
      "Mean rewards                            :    0.60300\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   26.36319\n",
      "Mean advantages                         :   -1.55360\n",
      "Mean (norm.) advantages                 :   -1.55360\n",
      "Mean (discounted) returns               :   24.80959\n",
      "Mean normalized returns                 :   24.80959\n",
      "Mean entropy                            :    2.99344\n",
      "Variance explained by the value function:    0.24313\n",
      "Std. of action_0 over agents            :    2.48637\n",
      "Std. of action_0 over envs              :    2.68999\n",
      "Std. of action_0 over time              :    2.75907\n",
      "Std. of action_1 over agents            :    2.60556\n",
      "Std. of action_1 over envs              :    2.81442\n",
      "Std. of action_1 over time              :    2.85503\n",
      "Current timestep                        : 8900000.00000\n",
      "Gradient norm                           :    1.17420\n",
      "Mean episodic reward                    :  606.92000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 890 / 1000   \n",
      "Mean training time per iter (ms)        :     131.47\n",
      "Mean steps per sec (training time)      :   76061.94\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.36912\n",
      "Policy loss                             :   -0.17359\n",
      "Value function loss                     :    4.14300\n",
      "Mean rewards                            :   -0.02975\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.22767\n",
      "Mean advantages                         :   -0.03739\n",
      "Mean (norm.) advantages                 :   -0.03739\n",
      "Mean (discounted) returns               :   -1.26506\n",
      "Mean normalized returns                 :   -1.26506\n",
      "Mean entropy                            :    4.73914\n",
      "Variance explained by the value function:    0.20622\n",
      "Std. of action_0 over agents            :    3.20277\n",
      "Std. of action_0 over envs              :    3.19774\n",
      "Std. of action_0 over time              :    3.19983\n",
      "Std. of action_1 over agents            :    3.17064\n",
      "Std. of action_1 over envs              :    3.16686\n",
      "Std. of action_1 over time              :    3.17190\n",
      "Current timestep                        : 9000000.00000\n",
      "Gradient norm                           :    0.02715\n",
      "Mean episodic reward                    : -584.72800\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.03105\n",
      "Policy loss                             :   -1.10422\n",
      "Value function loss                     :  122.57107\n",
      "Mean rewards                            :    0.61460\n",
      "Max. rewards                            :   30.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   25.14753\n",
      "Mean advantages                         :   -0.43192\n",
      "Mean (norm.) advantages                 :   -0.43192\n",
      "Mean (discounted) returns               :   24.71561\n",
      "Mean normalized returns                 :   24.71561\n",
      "Mean entropy                            :    3.05089\n",
      "Variance explained by the value function:    0.31449\n",
      "Std. of action_0 over agents            :    2.89194\n",
      "Std. of action_0 over envs              :    3.08357\n",
      "Std. of action_0 over time              :    3.13042\n",
      "Std. of action_1 over agents            :    3.30961\n",
      "Std. of action_1 over envs              :    3.47596\n",
      "Std. of action_1 over time              :    3.51229\n",
      "Current timestep                        : 9000000.00000\n",
      "Gradient norm                           :    1.06997\n",
      "Mean episodic reward                    :  604.72000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 900 / 1000   \n",
      "Mean training time per iter (ms)        :     131.46\n",
      "Mean steps per sec (training time)      :   76070.81\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.14291\n",
      "Policy loss                             :    0.05394\n",
      "Value function loss                     :    4.06345\n",
      "Mean rewards                            :   -0.02845\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.23909\n",
      "Mean advantages                         :    0.01223\n",
      "Mean (norm.) advantages                 :    0.01223\n",
      "Mean (discounted) returns               :   -1.22685\n",
      "Mean normalized returns                 :   -1.22685\n",
      "Mean entropy                            :    4.74955\n",
      "Variance explained by the value function:    0.19480\n",
      "Std. of action_0 over agents            :    3.17530\n",
      "Std. of action_0 over envs              :    3.16798\n",
      "Std. of action_0 over time              :    3.17234\n",
      "Std. of action_1 over agents            :    3.12031\n",
      "Std. of action_1 over envs              :    3.11724\n",
      "Std. of action_1 over time              :    3.12251\n",
      "Current timestep                        : 9100000.00000\n",
      "Gradient norm                           :    0.02538\n",
      "Mean episodic reward                    : -566.91800\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    3.05968\n",
      "Policy loss                             :    2.11430\n",
      "Value function loss                     :  110.66602\n",
      "Mean rewards                            :    0.59020\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   23.04869\n",
      "Mean advantages                         :    0.63539\n",
      "Mean (norm.) advantages                 :    0.63539\n",
      "Mean (discounted) returns               :   23.68408\n",
      "Mean normalized returns                 :   23.68408\n",
      "Mean entropy                            :    3.22544\n",
      "Variance explained by the value function:    0.30168\n",
      "Std. of action_0 over agents            :    3.18260\n",
      "Std. of action_0 over envs              :    3.37691\n",
      "Std. of action_0 over time              :    3.42207\n",
      "Std. of action_1 over agents            :    3.68669\n",
      "Std. of action_1 over envs              :    3.82059\n",
      "Std. of action_1 over time              :    3.84825\n",
      "Current timestep                        : 9100000.00000\n",
      "Gradient norm                           :    1.06569\n",
      "Mean episodic reward                    :  589.60000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 910 / 1000   \n",
      "Mean training time per iter (ms)        :     131.45\n",
      "Mean steps per sec (training time)      :   76072.72\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.05486\n",
      "Policy loss                             :    0.14089\n",
      "Value function loss                     :    4.18044\n",
      "Mean rewards                            :   -0.02834\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.24300\n",
      "Mean advantages                         :    0.03174\n",
      "Mean (norm.) advantages                 :    0.03174\n",
      "Mean (discounted) returns               :   -1.21126\n",
      "Mean normalized returns                 :   -1.21126\n",
      "Mean entropy                            :    4.75115\n",
      "Variance explained by the value function:    0.16665\n",
      "Std. of action_0 over agents            :    3.15765\n",
      "Std. of action_0 over envs              :    3.15171\n",
      "Std. of action_0 over time              :    3.15776\n",
      "Std. of action_1 over agents            :    3.10956\n",
      "Std. of action_1 over envs              :    3.10617\n",
      "Std. of action_1 over time              :    3.11089\n",
      "Current timestep                        : 9200000.00000\n",
      "Gradient norm                           :    0.03981\n",
      "Mean episodic reward                    : -572.90000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    1.20769\n",
      "Policy loss                             :    0.15536\n",
      "Value function loss                     :  121.40173\n",
      "Mean rewards                            :    0.58820\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   23.63122\n",
      "Mean advantages                         :    0.00830\n",
      "Mean (norm.) advantages                 :    0.00830\n",
      "Mean (discounted) returns               :   23.63952\n",
      "Mean normalized returns                 :   23.63952\n",
      "Mean entropy                            :    3.23377\n",
      "Variance explained by the value function:    0.23743\n",
      "Std. of action_0 over agents            :    2.58762\n",
      "Std. of action_0 over envs              :    2.77729\n",
      "Std. of action_0 over time              :    2.80728\n",
      "Std. of action_1 over agents            :    3.64288\n",
      "Std. of action_1 over envs              :    3.82928\n",
      "Std. of action_1 over time              :    3.86244\n",
      "Current timestep                        : 9200000.00000\n",
      "Gradient norm                           :    1.04532\n",
      "Mean episodic reward                    :  592.98000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 920 / 1000   \n",
      "Mean training time per iter (ms)        :     131.46\n",
      "Mean steps per sec (training time)      :   76069.51\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.27581\n",
      "Policy loss                             :   -0.07884\n",
      "Value function loss                     :    4.04371\n",
      "Mean rewards                            :   -0.02736\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.17853\n",
      "Mean advantages                         :   -0.01491\n",
      "Mean (norm.) advantages                 :   -0.01491\n",
      "Mean (discounted) returns               :   -1.19344\n",
      "Mean normalized returns                 :   -1.19344\n",
      "Mean entropy                            :    4.74810\n",
      "Variance explained by the value function:    0.17657\n",
      "Std. of action_0 over agents            :    3.16767\n",
      "Std. of action_0 over envs              :    3.16128\n",
      "Std. of action_0 over time              :    3.16500\n",
      "Std. of action_1 over agents            :    3.12557\n",
      "Std. of action_1 over envs              :    3.12226\n",
      "Std. of action_1 over time              :    3.12740\n",
      "Current timestep                        : 9300000.00000\n",
      "Gradient norm                           :    0.03736\n",
      "Mean episodic reward                    : -573.03400\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -1.60792\n",
      "Policy loss                             :   -2.63305\n",
      "Value function loss                     :  119.45473\n",
      "Mean rewards                            :    0.57280\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   24.32656\n",
      "Mean advantages                         :   -0.82561\n",
      "Mean (norm.) advantages                 :   -0.82561\n",
      "Mean (discounted) returns               :   23.50094\n",
      "Mean normalized returns                 :   23.50094\n",
      "Mean entropy                            :    3.38838\n",
      "Variance explained by the value function:    0.22539\n",
      "Std. of action_0 over agents            :    2.50192\n",
      "Std. of action_0 over envs              :    2.70398\n",
      "Std. of action_0 over time              :    2.72757\n",
      "Std. of action_1 over agents            :    3.30610\n",
      "Std. of action_1 over envs              :    3.43749\n",
      "Std. of action_1 over time              :    3.46206\n",
      "Current timestep                        : 9300000.00000\n",
      "Gradient norm                           :    1.13402\n",
      "Mean episodic reward                    :  594.66000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 930 / 1000   \n",
      "Mean training time per iter (ms)        :     131.43\n",
      "Mean steps per sec (training time)      :   76083.31\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.49547\n",
      "Policy loss                             :   -0.30134\n",
      "Value function loss                     :    4.25694\n",
      "Mean rewards                            :   -0.02857\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.19271\n",
      "Mean advantages                         :   -0.06326\n",
      "Mean (norm.) advantages                 :   -0.06326\n",
      "Mean (discounted) returns               :   -1.25597\n",
      "Mean normalized returns                 :   -1.25597\n",
      "Mean entropy                            :    4.73399\n",
      "Variance explained by the value function:    0.16531\n",
      "Std. of action_0 over agents            :    3.22218\n",
      "Std. of action_0 over envs              :    3.21744\n",
      "Std. of action_0 over time              :    3.22140\n",
      "Std. of action_1 over agents            :    3.19984\n",
      "Std. of action_1 over envs              :    3.19650\n",
      "Std. of action_1 over time              :    3.20160\n",
      "Current timestep                        : 9400000.00000\n",
      "Gradient norm                           :    0.03201\n",
      "Mean episodic reward                    : -554.25600\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -1.66757\n",
      "Policy loss                             :   -2.83409\n",
      "Value function loss                     :  133.35826\n",
      "Mean rewards                            :    0.59520\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   26.13985\n",
      "Mean advantages                         :   -0.88884\n",
      "Mean (norm.) advantages                 :   -0.88884\n",
      "Mean (discounted) returns               :   25.25101\n",
      "Mean normalized returns                 :   25.25101\n",
      "Mean entropy                            :    3.34109\n",
      "Variance explained by the value function:    0.17955\n",
      "Std. of action_0 over agents            :    2.55535\n",
      "Std. of action_0 over envs              :    2.76395\n",
      "Std. of action_0 over time              :    2.79454\n",
      "Std. of action_1 over agents            :    2.09338\n",
      "Std. of action_1 over envs              :    2.25748\n",
      "Std. of action_1 over time              :    2.27124\n",
      "Current timestep                        : 9400000.00000\n",
      "Gradient norm                           :    1.04642\n",
      "Mean episodic reward                    :  577.54000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 940 / 1000   \n",
      "Mean training time per iter (ms)        :     131.41\n",
      "Mean steps per sec (training time)      :   76098.14\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.20821\n",
      "Policy loss                             :   -0.01201\n",
      "Value function loss                     :    4.15328\n",
      "Mean rewards                            :   -0.02957\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.27709\n",
      "Mean advantages                         :   -0.00364\n",
      "Mean (norm.) advantages                 :   -0.00364\n",
      "Mean (discounted) returns               :   -1.28073\n",
      "Mean normalized returns                 :   -1.28073\n",
      "Mean entropy                            :    4.75468\n",
      "Variance explained by the value function:    0.20202\n",
      "Std. of action_0 over agents            :    3.19947\n",
      "Std. of action_0 over envs              :    3.19537\n",
      "Std. of action_0 over time              :    3.19812\n",
      "Std. of action_1 over agents            :    3.17769\n",
      "Std. of action_1 over envs              :    3.17489\n",
      "Std. of action_1 over time              :    3.17932\n",
      "Current timestep                        : 9500000.00000\n",
      "Gradient norm                           :    0.03554\n",
      "Mean episodic reward                    : -567.83200\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    5.07376\n",
      "Policy loss                             :    3.97449\n",
      "Value function loss                     :  126.70538\n",
      "Mean rewards                            :    0.61200\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   23.69209\n",
      "Mean advantages                         :    1.29387\n",
      "Mean (norm.) advantages                 :    1.29387\n",
      "Mean (discounted) returns               :   24.98596\n",
      "Mean normalized returns                 :   24.98596\n",
      "Mean entropy                            :    3.35566\n",
      "Variance explained by the value function:    0.27646\n",
      "Std. of action_0 over agents            :    2.34827\n",
      "Std. of action_0 over envs              :    2.56406\n",
      "Std. of action_0 over time              :    2.60320\n",
      "Std. of action_1 over agents            :    2.00955\n",
      "Std. of action_1 over envs              :    2.09320\n",
      "Std. of action_1 over time              :    2.10885\n",
      "Current timestep                        : 9500000.00000\n",
      "Gradient norm                           :    1.15305\n",
      "Mean episodic reward                    :  590.20000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 950 / 1000   \n",
      "Mean training time per iter (ms)        :     131.38\n",
      "Mean steps per sec (training time)      :   76114.85\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.34081\n",
      "Policy loss                             :   -0.14562\n",
      "Value function loss                     :    4.22721\n",
      "Mean rewards                            :   -0.02945\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.25034\n",
      "Mean advantages                         :   -0.02811\n",
      "Mean (norm.) advantages                 :   -0.02811\n",
      "Mean (discounted) returns               :   -1.27845\n",
      "Mean normalized returns                 :   -1.27845\n",
      "Mean entropy                            :    4.74924\n",
      "Variance explained by the value function:    0.18769\n",
      "Std. of action_0 over agents            :    3.18969\n",
      "Std. of action_0 over envs              :    3.18522\n",
      "Std. of action_0 over time              :    3.18722\n",
      "Std. of action_1 over agents            :    3.14180\n",
      "Std. of action_1 over envs              :    3.13875\n",
      "Std. of action_1 over time              :    3.14342\n",
      "Current timestep                        : 9600000.00000\n",
      "Gradient norm                           :    0.03068\n",
      "Mean episodic reward                    : -594.98200\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    4.25302\n",
      "Policy loss                             :    3.19632\n",
      "Value function loss                     :  119.53486\n",
      "Mean rewards                            :    0.61180\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   23.50979\n",
      "Mean advantages                         :    1.31273\n",
      "Mean (norm.) advantages                 :    1.31273\n",
      "Mean (discounted) returns               :   24.82252\n",
      "Mean normalized returns                 :   24.82252\n",
      "Mean entropy                            :    2.77310\n",
      "Variance explained by the value function:    0.30261\n",
      "Std. of action_0 over agents            :    2.26174\n",
      "Std. of action_0 over envs              :    2.46423\n",
      "Std. of action_0 over time              :    2.51926\n",
      "Std. of action_1 over agents            :    1.68436\n",
      "Std. of action_1 over envs              :    1.78463\n",
      "Std. of action_1 over time              :    1.80277\n",
      "Current timestep                        : 9600000.00000\n",
      "Gradient norm                           :    1.10563\n",
      "Mean episodic reward                    :  614.36000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 960 / 1000   \n",
      "Mean training time per iter (ms)        :     131.35\n",
      "Mean steps per sec (training time)      :   76131.35\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.00258\n",
      "Policy loss                             :    0.19290\n",
      "Value function loss                     :    4.13975\n",
      "Mean rewards                            :   -0.02909\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.29192\n",
      "Mean advantages                         :    0.04022\n",
      "Mean (norm.) advantages                 :    0.04022\n",
      "Mean (discounted) returns               :   -1.25170\n",
      "Mean normalized returns                 :   -1.25170\n",
      "Mean entropy                            :    4.73767\n",
      "Variance explained by the value function:    0.18618\n",
      "Std. of action_0 over agents            :    3.19830\n",
      "Std. of action_0 over envs              :    3.19339\n",
      "Std. of action_0 over time              :    3.19447\n",
      "Std. of action_1 over agents            :    3.12764\n",
      "Std. of action_1 over envs              :    3.12300\n",
      "Std. of action_1 over time              :    3.12900\n",
      "Current timestep                        : 9700000.00000\n",
      "Gradient norm                           :    0.02691\n",
      "Mean episodic reward                    : -588.79600\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.81261\n",
      "Policy loss                             :   -1.72055\n",
      "Value function loss                     :  105.96576\n",
      "Mean rewards                            :    0.60140\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   24.60137\n",
      "Mean advantages                         :   -0.64857\n",
      "Mean (norm.) advantages                 :   -0.64857\n",
      "Mean (discounted) returns               :   23.95280\n",
      "Mean normalized returns                 :   23.95280\n",
      "Mean entropy                            :    3.03429\n",
      "Variance explained by the value function:    0.32590\n",
      "Std. of action_0 over agents            :    2.56005\n",
      "Std. of action_0 over envs              :    2.81306\n",
      "Std. of action_0 over time              :    2.92726\n",
      "Std. of action_1 over agents            :    1.82189\n",
      "Std. of action_1 over envs              :    1.92473\n",
      "Std. of action_1 over time              :    1.95218\n",
      "Current timestep                        : 9700000.00000\n",
      "Gradient norm                           :    1.12529\n",
      "Mean episodic reward                    :  609.00000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 970 / 1000   \n",
      "Mean training time per iter (ms)        :     131.35\n",
      "Mean steps per sec (training time)      :   76132.67\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.09292\n",
      "Policy loss                             :    0.10299\n",
      "Value function loss                     :    4.16516\n",
      "Mean rewards                            :   -0.02936\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.29036\n",
      "Mean advantages                         :    0.02345\n",
      "Mean (norm.) advantages                 :    0.02345\n",
      "Mean (discounted) returns               :   -1.26691\n",
      "Mean normalized returns                 :   -1.26691\n",
      "Mean entropy                            :    4.75133\n",
      "Variance explained by the value function:    0.18638\n",
      "Std. of action_0 over agents            :    3.19080\n",
      "Std. of action_0 over envs              :    3.18545\n",
      "Std. of action_0 over time              :    3.18853\n",
      "Std. of action_1 over agents            :    3.11612\n",
      "Std. of action_1 over envs              :    3.11245\n",
      "Std. of action_1 over time              :    3.11794\n",
      "Current timestep                        : 9800000.00000\n",
      "Gradient norm                           :    0.02722\n",
      "Mean episodic reward                    : -592.84600\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -2.32977\n",
      "Policy loss                             :   -3.38606\n",
      "Value function loss                     :  121.66192\n",
      "Mean rewards                            :    0.61060\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   25.73337\n",
      "Mean advantages                         :   -1.20760\n",
      "Mean (norm.) advantages                 :   -1.20760\n",
      "Mean (discounted) returns               :   24.52577\n",
      "Mean normalized returns                 :   24.52577\n",
      "Mean entropy                            :    3.20664\n",
      "Variance explained by the value function:    0.28089\n",
      "Std. of action_0 over agents            :    2.48739\n",
      "Std. of action_0 over envs              :    2.74256\n",
      "Std. of action_0 over time              :    2.84721\n",
      "Std. of action_1 over agents            :    2.02462\n",
      "Std. of action_1 over envs              :    2.14198\n",
      "Std. of action_1 over time              :    2.17311\n",
      "Current timestep                        : 9800000.00000\n",
      "Gradient norm                           :    1.17561\n",
      "Mean episodic reward                    :  612.60000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 980 / 1000   \n",
      "Mean training time per iter (ms)        :     131.33\n",
      "Mean steps per sec (training time)      :   76145.46\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.44418\n",
      "Policy loss                             :   -0.24911\n",
      "Value function loss                     :    4.19554\n",
      "Mean rewards                            :   -0.02911\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.19526\n",
      "Mean advantages                         :   -0.05146\n",
      "Mean (norm.) advantages                 :   -0.05146\n",
      "Mean (discounted) returns               :   -1.24672\n",
      "Mean normalized returns                 :   -1.24672\n",
      "Mean entropy                            :    4.74056\n",
      "Variance explained by the value function:    0.18070\n",
      "Std. of action_0 over agents            :    3.17969\n",
      "Std. of action_0 over envs              :    3.17441\n",
      "Std. of action_0 over time              :    3.17680\n",
      "Std. of action_1 over agents            :    3.11503\n",
      "Std. of action_1 over envs              :    3.11131\n",
      "Std. of action_1 over time              :    3.11650\n",
      "Current timestep                        : 9900000.00000\n",
      "Gradient norm                           :    0.02853\n",
      "Mean episodic reward                    : -578.76400\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.73092\n",
      "Policy loss                             :   -1.67921\n",
      "Value function loss                     :  111.90366\n",
      "Mean rewards                            :    0.60440\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   24.65270\n",
      "Mean advantages                         :   -0.47630\n",
      "Mean (norm.) advantages                 :   -0.47630\n",
      "Mean (discounted) returns               :   24.17640\n",
      "Mean normalized returns                 :   24.17640\n",
      "Mean entropy                            :    3.41493\n",
      "Variance explained by the value function:    0.32981\n",
      "Std. of action_0 over agents            :    2.35162\n",
      "Std. of action_0 over envs              :    2.51744\n",
      "Std. of action_0 over time              :    2.60429\n",
      "Std. of action_1 over agents            :    2.05317\n",
      "Std. of action_1 over envs              :    2.16243\n",
      "Std. of action_1 over time              :    2.18005\n",
      "Current timestep                        : 9900000.00000\n",
      "Gradient norm                           :    1.11308\n",
      "Mean episodic reward                    :  600.52000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 990 / 1000   \n",
      "Mean training time per iter (ms)        :     131.33\n",
      "Mean steps per sec (training time)      :   76145.42\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.52432\n",
      "Policy loss                             :   -0.32923\n",
      "Value function loss                     :    4.19812\n",
      "Mean rewards                            :   -0.02944\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.17564\n",
      "Mean advantages                         :   -0.07007\n",
      "Mean (norm.) advantages                 :   -0.07007\n",
      "Mean (discounted) returns               :   -1.24571\n",
      "Mean normalized returns                 :   -1.24571\n",
      "Mean entropy                            :    4.74142\n",
      "Variance explained by the value function:    0.18246\n",
      "Std. of action_0 over agents            :    3.19042\n",
      "Std. of action_0 over envs              :    3.18554\n",
      "Std. of action_0 over time              :    3.18809\n",
      "Std. of action_1 over agents            :    3.12981\n",
      "Std. of action_1 over envs              :    3.12630\n",
      "Std. of action_1 over time              :    3.13154\n",
      "Current timestep                        : 9990000.00000\n",
      "Gradient norm                           :    0.02496\n",
      "Mean episodic reward                    : -584.57556\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "[Device 0]: Saving the 'runner' torch model to the file: '/tmp/continuous_tag/example/1650600997/runner_9990000.state_dict'. \n",
      "[Device 0]: Saving the 'tagger' torch model to the file: '/tmp/continuous_tag/example/1650600997/tagger_9980000.state_dict'. \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    3.98438\n",
      "Policy loss                             :    3.00639\n",
      "Value function loss                     :  114.82838\n",
      "Mean rewards                            :    0.60620\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   23.08975\n",
      "Mean advantages                         :    0.91406\n",
      "Mean (norm.) advantages                 :    0.91406\n",
      "Mean (discounted) returns               :   24.00381\n",
      "Mean normalized returns                 :   24.00381\n",
      "Mean entropy                            :    3.40591\n",
      "Variance explained by the value function:    0.32785\n",
      "Std. of action_0 over agents            :    2.45406\n",
      "Std. of action_0 over envs              :    2.63899\n",
      "Std. of action_0 over time              :    2.73929\n",
      "Std. of action_1 over agents            :    2.04365\n",
      "Std. of action_1 over envs              :    2.15399\n",
      "Std. of action_1 over time              :    2.17432\n",
      "Current timestep                        : 9990000.00000\n",
      "Gradient norm                           :    1.14537\n",
      "Mean episodic reward                    :  604.33333\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "[Device 0]: Saving the 'runner' torch model to the file: '/tmp/continuous_tag/example/1650600997/runner_9990000.state_dict'. \n",
      "[Device 0]: Saving the 'tagger' torch model to the file: '/tmp/continuous_tag/example/1650600997/tagger_9990000.state_dict'. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.53310\n",
      "Policy loss                             :   -0.33753\n",
      "Value function loss                     :    4.15815\n",
      "Mean rewards                            :   -0.02936\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.15203\n",
      "Mean advantages                         :   -0.07211\n",
      "Mean (norm.) advantages                 :   -0.07211\n",
      "Mean (discounted) returns               :   -1.22414\n",
      "Mean normalized returns                 :   -1.22414\n",
      "Mean entropy                            :    4.74295\n",
      "Variance explained by the value function:    0.18542\n",
      "Std. of action_0 over agents            :    3.18995\n",
      "Std. of action_0 over envs              :    3.18508\n",
      "Std. of action_0 over time              :    3.18737\n",
      "Std. of action_1 over agents            :    3.13307\n",
      "Std. of action_1 over envs              :    3.12895\n",
      "Std. of action_1 over time              :    3.13443\n",
      "Current timestep                        : 10000000.00000\n",
      "Gradient norm                           :    0.02939\n",
      "Mean episodic reward                    : -587.28000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    2.22805\n",
      "Policy loss                             :    1.22339\n",
      "Value function loss                     :  117.49338\n",
      "Mean rewards                            :    0.61080\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   23.44936\n",
      "Mean advantages                         :    0.43354\n",
      "Mean (norm.) advantages                 :    0.43354\n",
      "Mean (discounted) returns               :   23.88290\n",
      "Mean normalized returns                 :   23.88290\n",
      "Mean entropy                            :    3.40551\n",
      "Variance explained by the value function:    0.32965\n",
      "Std. of action_0 over agents            :    2.49148\n",
      "Std. of action_0 over envs              :    2.68992\n",
      "Std. of action_0 over time              :    2.78799\n",
      "Std. of action_1 over agents            :    2.06527\n",
      "Std. of action_1 over envs              :    2.17076\n",
      "Std. of action_1 over time              :    2.18669\n",
      "Current timestep                        : 10000000.00000\n",
      "Gradient norm                           :    1.13207\n",
      "Mean episodic reward                    :  610.80000\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1650600997/results.json' \n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Iteration                               : 1000 / 1000  \n",
      "Mean training time per iter (ms)        :     131.34\n",
      "Mean steps per sec (training time)      :   76137.41\n",
      "\n",
      "\n",
      "Training is complete!\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(wd_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a3fb18",
   "metadata": {
    "papermill": {
     "duration": 0.135664,
     "end_time": "2022-04-22T04:31:21.187036",
     "exception": false,
     "start_time": "2022-04-22T04:31:21.051372",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Visualize an episode-rollout after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f44ba511",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T04:31:21.462855Z",
     "iopub.status.busy": "2022-04-22T04:31:21.462354Z",
     "iopub.status.idle": "2022-04-22T04:31:21.463995Z",
     "shell.execute_reply": "2022-04-22T04:31:21.464383Z"
    },
    "papermill": {
     "duration": 0.141577,
     "end_time": "2022-04-22T04:31:21.464553",
     "exception": false,
     "start_time": "2022-04-22T04:31:21.322976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# anim = generate_tag_env_rollout_animation(wd_module)\n",
    "# HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f631286",
   "metadata": {
    "papermill": {
     "duration": 0.134053,
     "end_time": "2022-04-22T04:31:21.735724",
     "exception": false,
     "start_time": "2022-04-22T04:31:21.601671",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Note: In the configuration above, we have set the trainer to only train on $50000$ rollout episodes, but you can increase the `num_episodes` configuration parameter to train further. As more training happens, the runners learn to escape the taggers, and the taggers learn to chase after the runner. Sometimes, the taggers also collaborate to team-tag runners. A good number of episodes to train on (for the configuration we have used) is $2$M or higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c9c79e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T04:31:22.030836Z",
     "iopub.status.busy": "2022-04-22T04:31:22.030312Z",
     "iopub.status.idle": "2022-04-22T04:31:22.032369Z",
     "shell.execute_reply": "2022-04-22T04:31:22.032767Z"
    },
    "papermill": {
     "duration": 0.141919,
     "end_time": "2022-04-22T04:31:22.032945",
     "exception": false,
     "start_time": "2022-04-22T04:31:21.891026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Device 0]: Trainer exits gracefully \n"
     ]
    }
   ],
   "source": [
    "# Finally, close the WarpDrive module to clear up the CUDA memory heap\n",
    "wd_module.graceful_close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a88c042",
   "metadata": {
    "papermill": {
     "duration": 0.135193,
     "end_time": "2022-04-22T04:31:22.320712",
     "exception": false,
     "start_time": "2022-04-22T04:31:22.185519",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Congratulations - Time to Join the Community!\n",
    "\n",
    "Congratulations on completing this notebook tutorial! If you enjoyed this and would like to join the Lightning\n",
    "movement, you can do so in the following ways!\n",
    "\n",
    "### Star [Lightning](https://github.com/PyTorchLightning/pytorch-lightning) on GitHub\n",
    "The easiest way to help our community is just by starring the GitHub repos! This helps raise awareness of the cool\n",
    "tools we're building.\n",
    "\n",
    "### Join our [Slack](https://www.pytorchlightning.ai/community)!\n",
    "The best way to keep up to date on the latest advancements is to join our community! Make sure to introduce yourself\n",
    "and share your interests in `#general` channel\n",
    "\n",
    "\n",
    "### Contributions !\n",
    "The best way to contribute to our community is to become a code contributor! At any time you can go to\n",
    "[Lightning](https://github.com/PyTorchLightning/pytorch-lightning) or [Bolt](https://github.com/PyTorchLightning/lightning-bolts)\n",
    "GitHub Issues page and filter for \"good first issue\".\n",
    "\n",
    "* [Lightning good first issue](https://github.com/PyTorchLightning/pytorch-lightning/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n",
    "* [Bolt good first issue](https://github.com/PyTorchLightning/lightning-bolts/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n",
    "* You can also contribute your own notebooks with useful examples !\n",
    "\n",
    "### Great thanks from the entire Pytorch Lightning Team for your interest !\n",
    "\n",
    "[![Pytorch Lightning](data:image/png;base64,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){height=\"60px\" width=\"240px\"}](https://pytorchlightning.ai)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "19734075",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. customcarditem::\n",
    "   :header: Multi-agent Reinforcement Learning With WarpDrive\n",
    "   :card_description: This notebook introduces multi-agent reinforcement learning (MARL) with WarpDrive (Lan et al. https://arxiv.org/abs/2108.13976). WarpDrive is a flexible, lightweight, and...\n",
    "   :tags: Reinforcement-Learning,Multi-agent,GPU,GPU/TPU,Lightning-Examples"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "colab,id,colab_type,-all",
   "formats": "ipynb,py:percent",
   "main_language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 989.899234,
   "end_time": "2022-04-22T04:31:23.671559",
   "environment_variables": {},
   "exception": null,
   "input_path": "lightning_examples/warp-drive/multi_agent_rl.ipynb",
   "output_path": ".notebooks/lightning_examples/warp-drive.ipynb",
   "parameters": {},
   "start_time": "2022-04-22T04:14:53.772325",
   "version": "2.3.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "506cafd9063b4655b538097b800b5c7c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_55b1a93b963c49aab91141adbf5e8275",
       "placeholder": "​",
       "style": "IPY_MODEL_d03e0513e62c46108363287a2a0e77a8",
       "value": "Epoch 999: 100%"
      }
     },
     "55b1a93b963c49aab91141adbf5e8275": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "68eabba3edea405fb5cf18d0d4570adf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "7d9c33be4337451fa0f668005e6ceaf8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9a6ea0c5bee84047bd405c4559ac83f4",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_dc6bf3f0e6bb433ba62346801a81fc77",
       "value": 1.0
      }
     },
     "9a6ea0c5bee84047bd405c4559ac83f4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": "2",
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a904bd1f2550454bbe04bac868ab813c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ab358b7366a84a88bfd37f925668a9b6",
       "placeholder": "​",
       "style": "IPY_MODEL_68eabba3edea405fb5cf18d0d4570adf",
       "value": " 1/1 [14:35&lt;00:00, 875.19s/it, loss=0.68, v_num=0, loss_runner=-.533, loss_tagger=2.230]"
      }
     },
     "ab358b7366a84a88bfd37f925668a9b6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c3ebd76328474e80a5fc3214fdbc80d5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": "inline-flex",
       "flex": null,
       "flex_flow": "row wrap",
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "100%"
      }
     },
     "d03e0513e62c46108363287a2a0e77a8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "dc6bf3f0e6bb433ba62346801a81fc77": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ec750af199a94f8091c03c862ee4ce11": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_506cafd9063b4655b538097b800b5c7c",
        "IPY_MODEL_7d9c33be4337451fa0f668005e6ceaf8",
        "IPY_MODEL_a904bd1f2550454bbe04bac868ab813c"
       ],
       "layout": "IPY_MODEL_c3ebd76328474e80a5fc3214fdbc80d5"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
